{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex_03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/ex_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UI2OiaD4zjsU",
        "colab_type": "code",
        "outputId": "46963ba2-5204-4010-dbc1-89d4fa4b7182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\"\"\"\n",
        "unpickle function from CIFAR-10 website\n",
        "\"\"\"\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        d = pickle.load(fo, encoding='latin')\n",
        "    return d\n",
        "\n",
        "def create_dataset_from_files(files):\n",
        "\trawdata = []\n",
        "\tlabels = []\n",
        "\tfor f in files:\n",
        "\t\td = unpickle(f)\n",
        "\t\trawdata.extend(d[\"data\"])\n",
        "\t\tlabels.extend(d[\"labels\"])\n",
        "\n",
        "\trawdata = np.array(rawdata)\n",
        "\n",
        "\tred = rawdata[:,:1024].reshape((-1,32,32))\n",
        "\tgreen = rawdata[:,1024:2048].reshape((-1,32,32))\n",
        "\tblue = rawdata[:,2048:].reshape((-1,32,32))\n",
        "\n",
        "\tdata = np.stack((red,green,blue), axis=3)\n",
        "\tlabels = np.array(labels)\n",
        "\n",
        "\treturn data, labels\n",
        "\n",
        "cifar_training_files = ['data_batch_{:d}'.format(i) for i in range(1,6)]\n",
        "cifar_testing_files = ['test_batch']\n",
        "\n",
        "train_data, train_labels = create_dataset_from_files(cifar_training_files)\n",
        "test_data, test_labels = create_dataset_from_files(cifar_testing_files)\n",
        "\n",
        "print(train_data.shape, train_data.dtype) # (50000, 32, 32, 3) -- 50000  images, each 32 x 32 with 3 color channels (UINT8)\n",
        "print(train_labels.shape, train_labels.dtype) # 50000 labels, one for each image (INT64)\n",
        "\n",
        "print(test_data.shape, test_data.dtype) # 10000 images, each 32 x 32 with 3 color channels (UINT8)\n",
        "print(train_labels.shape, train_labels.dtype) # 10000 labels, one for each image (INT64)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) uint8\n",
            "(50000,) int64\n",
            "(10000, 32, 32, 3) uint8\n",
            "(50000,) int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww09z7MnzwRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#Training the Neural Network\n",
        "def train_network(train_data, train_label, batch_size, learning_rate):\n",
        "    imgs_placeholder = tf.placeholder(tf.uint8, shape=(None, 32, 32, 3))\n",
        "    imgs_label_placeholder = tf.placeholder(tf.int64, shape=(None,))\n",
        "    preprocessed_imgs = tf.image.convert_image_dtype(imgs_placeholder, tf.float32)\n",
        "    \n",
        "    ######################### first convolutional layer ################################\n",
        "    conv1_1out = tf.layers.conv2d(preprocessed_imgs, filters=32, kernel_size=(3,3), activation=tf.nn.relu, name='conv1_1')\n",
        "    conv1_2out = tf.layers.conv2d(conv1_1out, filters=32, kernel_size=(3,3), activation=tf.nn.relu, name='conv1_2')\n",
        "    pool1_out = tf.layers.max_pooling2d(conv1_2out, pool_size=(2,2), strides=2, name='pool1')\n",
        "    \n",
        "    ######################### Second convolutional layer ################################\n",
        "    conv2_1out = tf.layers.conv2d(pool1_out, filters=64, kernel_size=(3,3), activation=tf.nn.relu, name='conv2_1')\n",
        "    conv2_2out = tf.layers.conv2d(conv2_1out, filters=64, kernel_size=(3,3), activation=tf.nn.relu, name='conv2_2')\n",
        "    pool2_out = tf.layers.max_pooling2d(conv2_2out, pool_size=(2,2), strides=2, name='pool2')\n",
        "    \n",
        "    ######################### Fully connected layer ################################\n",
        "    conv2_1out = tf.layers.flatten(pool2_out, name='flatten')\n",
        "    logits = tf.layers.dense(conv2_1out, units=10, name='fc1')\n",
        "    \n",
        "    ######################### Define Loss function ################################\n",
        "    loss =  tf.losses.sparse_softmax_cross_entropy(imgs_label_placeholder, logits)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate= learning_rate)\n",
        "    minimizer_op = optimizer.minimize(loss)\n",
        "    \n",
        "    ################## Add ops to save and restore all the variables. ###############\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    ######################### Accuracy check ################################\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    correct_pred = tf.equal(imgs_label_placeholder, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred , tf.float32))\n",
        "    \n",
        "    ################## Adding summary for consoling through tensorboard ###############\n",
        "    loss_summary = tf.summary.scalar(name=\"loss\", tensor=loss) #writes loss summary\n",
        "    acc_summary = tf.summary.scalar(name=\"accuracy\", tensor=accuracy) #writes accuracy summary\n",
        "    \n",
        "    ######################### Start Training ################################\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver.restore(sess, \"model.ckpt\")\n",
        "        print('model restored')\n",
        "        for k in range(50):\n",
        "            j = 0\n",
        "            unseen_data = train_data[:]\n",
        "            while(len(unseen_data)):\n",
        "                b_size = batch_size if batch_size < len(unseen_data) else len(unseen_data)\n",
        "                idx = np.random.choice( unseen_data.shape[0], b_size, replace=False )\n",
        "                indices_to_keep = [i for i in range(len(unseen_data)) if i not in idx]\n",
        "                unseen_data = unseen_data[indices_to_keep]\n",
        "                j = j + 1\n",
        "                training_data_batch = train_data[idx]\n",
        "                train_label_batch = train_label[idx]\n",
        "                _, l, acc, l_summary, a_summary =sess.run([minimizer_op, loss, accuracy, loss_summary, acc_summary], {imgs_placeholder: training_data_batch, imgs_label_placeholder: train_label_batch})\n",
        "                writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
        "                writer.add_summary(l_summary, global_step= j) # writes loss summary\n",
        "                writer.add_summary(a_summary, global_step=j) # writes loss summary\n",
        "                #print('After iteration {} batch {} loss is {} and accuracy is {}%'.format(k, j, l, acc))\n",
        "            \n",
        "            print('*********************************************************************')\n",
        "            print('After iteration {} loss is {} and accuracy is {}%'.format(k, l, acc*100))\n",
        "            save_path = saver.save(sess, \"model.ckpt\".format(k))   \n",
        "        \n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPFmDaQZ4qHB",
        "colab_type": "code",
        "outputId": "71adb33b-4e82-4956-8954-f738b9f9af12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1074
        }
      },
      "source": [
        "train_network(train_data, train_labels, 32, 0.001)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-2-ff95c8a5b383>:11: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-2-ff95c8a5b383>:13: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-ff95c8a5b383>:21: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-2-ff95c8a5b383>:22: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from model.ckpt\n",
            "model restored\n",
            "*********************************************************************\n",
            "After iteration 0 loss is 0.24775226414203644 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 1 loss is 0.196519136428833 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 2 loss is 0.22173868119716644 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 3 loss is 0.21004879474639893 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 4 loss is 0.19100552797317505 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 5 loss is 0.18868929147720337 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 6 loss is 0.18190845847129822 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 7 loss is 0.1803601086139679 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 8 loss is 0.16652710735797882 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 9 loss is 0.149810791015625 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 10 loss is 0.14998367428779602 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 11 loss is 0.1629752516746521 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 12 loss is 0.13781669735908508 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 13 loss is 0.14102941751480103 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 14 loss is 0.13824281096458435 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 15 loss is 0.12699174880981445 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 16 loss is 0.12659882009029388 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 17 loss is 0.12868475914001465 and accuracy is 100.0%\n",
            "*********************************************************************\n",
            "After iteration 18 loss is 0.11273819953203201 and accuracy is 100.0%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}