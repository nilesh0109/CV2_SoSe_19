{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cv2_ex4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/Copy_of_cv2_ex4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqKlMK9Pw56Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import h5py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMfhbwBbqICj",
        "colab_type": "code",
        "outputId": "da4da951-d612-4b20-c7d4-dace7854b5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "\"\"\"\n",
        "unpickle function from CIFAR-10 website\n",
        "\"\"\"\n",
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        d = pickle.load(fo, encoding='latin')\n",
        "    return d\n",
        "\n",
        "def create_dataset_from_files(files):\n",
        "\trawdata = []\n",
        "\tlabels = []\n",
        "\tfor f in files:\n",
        "\t\td = unpickle(f)\n",
        "\t\trawdata.extend(d[\"data\"])\n",
        "\t\tlabels.extend(d[\"labels\"])\n",
        "\n",
        "\trawdata = np.array(rawdata)\n",
        "\n",
        "\tred = rawdata[:,:1024].reshape((-1,32,32))\n",
        "\tgreen = rawdata[:,1024:2048].reshape((-1,32,32))\n",
        "\tblue = rawdata[:,2048:].reshape((-1,32,32))\n",
        "\n",
        "\tdata = np.stack((red,green,blue), axis=3)\n",
        "\tlabels = np.array(labels)\n",
        "\n",
        "\treturn data, labels\n",
        "\n",
        "cifar_training_files = ['data_batch_{:d}'.format(i) for i in range(1,6)]\n",
        "cifar_testing_files = ['test_batch']\n",
        "\n",
        "train_data, train_labels = create_dataset_from_files(cifar_training_files)\n",
        "test_data, test_labels = create_dataset_from_files(cifar_testing_files)\n",
        "\n",
        "print(train_data.shape, train_data.dtype) # (50000, 32, 32, 3) -- 50000  images, each 32 x 32 with 3 color channels (UINT8)\n",
        "print(train_labels.shape, train_labels.dtype) # 50000 labels, one for each image (INT64)\n",
        "\n",
        "print(test_data.shape, test_data.dtype) # 10000 images, each 32 x 32 with 3 color channels (UINT8)\n",
        "print(test_labels.shape, test_labels.dtype) # 10000 labels, one for each image (INT64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3) uint8\n",
            "(50000,) int64\n",
            "(10000, 32, 32, 3) uint8\n",
            "(10000,) int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dke_WNNs0YDp",
        "colab_type": "code",
        "outputId": "b3e37b7d-76d1-4efd-d54d-a0d3b7c1c084",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "hf = h5py.File('vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n",
        "layer_groups = list(hf)\n",
        "print(layer_groups)\n",
        "#data = hf.get('/block1_conv1')\n",
        "#vgg_weights\n",
        "weights={}\n",
        "for layer_grp in layer_groups:\n",
        "  grp = hf.get(layer_grp)\n",
        "  for k in grp.keys():\n",
        "    print( k + ' shape: {}'.format(grp[k].shape))\n",
        "    weights.update({k: grp[k].value})\n",
        "print(weights.keys())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['layer_0', 'layer_1', 'layer_10', 'layer_11', 'layer_12', 'layer_13', 'layer_14', 'layer_15', 'layer_16', 'layer_17', 'layer_18', 'layer_19', 'layer_2', 'layer_20', 'layer_21', 'layer_22', 'layer_23', 'layer_24', 'layer_25', 'layer_26', 'layer_27', 'layer_28', 'layer_29', 'layer_3', 'layer_30', 'layer_31', 'layer_32', 'layer_33', 'layer_34', 'layer_35', 'layer_36', 'layer_4', 'layer_5', 'layer_6', 'layer_7', 'layer_8', 'layer_9']\n",
            "param_0 shape: (64, 3, 3, 3)\n",
            "param_1 shape: (64,)\n",
            "param_0 shape: (256, 128, 3, 3)\n",
            "param_1 shape: (256,)\n",
            "param_0 shape: (256, 256, 3, 3)\n",
            "param_1 shape: (256,)\n",
            "param_0 shape: (256, 256, 3, 3)\n",
            "param_1 shape: (256,)\n",
            "param_0 shape: (512, 256, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (512, 512, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (512, 512, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (512, 512, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (512, 512, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (512, 512, 3, 3)\n",
            "param_1 shape: (512,)\n",
            "param_0 shape: (64, 64, 3, 3)\n",
            "param_1 shape: (64,)\n",
            "param_0 shape: (25088, 4096)\n",
            "param_1 shape: (4096,)\n",
            "param_0 shape: (4096, 4096)\n",
            "param_1 shape: (4096,)\n",
            "param_0 shape: (4096, 1000)\n",
            "param_1 shape: (1000,)\n",
            "param_0 shape: (128, 64, 3, 3)\n",
            "param_1 shape: (128,)\n",
            "param_0 shape: (128, 128, 3, 3)\n",
            "param_1 shape: (128,)\n",
            "dict_keys(['layer_1_param_0', 'layer_1_param_1', 'layer_11_param_0', 'layer_11_param_1', 'layer_13_param_0', 'layer_13_param_1', 'layer_15_param_0', 'layer_15_param_1', 'layer_18_param_0', 'layer_18_param_1', 'layer_20_param_0', 'layer_20_param_1', 'layer_22_param_0', 'layer_22_param_1', 'layer_25_param_0', 'layer_25_param_1', 'layer_27_param_0', 'layer_27_param_1', 'layer_29_param_0', 'layer_29_param_1', 'layer_3_param_0', 'layer_3_param_1', 'layer_32_param_0', 'layer_32_param_1', 'layer_34_param_0', 'layer_34_param_1', 'layer_36_param_0', 'layer_36_param_1', 'layer_6_param_0', 'layer_6_param_1', 'layer_8_param_0', 'layer_8_param_1'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qiAp5oikX5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the Neural Network\n",
        "def train_network(train_data, train_label, batch_size= 32, learning_rate=0.01, prediction_mode= False):\n",
        "  imgs_placeholder = tf.placeholder(tf.uint8, shape=(None, 32, 32, 3))\n",
        "  imgs_label_placeholder = tf.placeholder(tf.int64, shape=(None,))\n",
        "  #preprocessed_imgs = tf.image.convert_image_dtype(imgs_placeholder, tf.float32)\n",
        "  \n",
        "  with tf.name_scope('preprocess') as scope:\n",
        "    imgs = tf.image.convert_image_dtype(imgs_placeholder, tf.float32) * 255.0\n",
        "    mean = tf.constant([123.68 , 116.779 , 103.939] ,dtype = tf.float32, shape =[1 , 1 , 1 , 3], name ='img_mean')\n",
        "    imgs_normalized = imgs - mean\n",
        "  \n",
        "  with tf.name_scope('conv1_1') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['layer0_conv1_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block1_conv1_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(imgs_normalized, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act11 = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "  with tf.name_scope('conv1_2') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['block1_conv2_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block1_conv2_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(act11, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act12 = tf.nn.relu(out, name=scope)\n",
        "  \n",
        "  with tf.name_scope('pool1') as scope:\n",
        "    #pooling_kernel = tf.Variable(initial_value = weights['pool_1'], trainable = False, name ='weights')\n",
        "    pool1 = tf.nn.max_pool(act12, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "  with tf.name_scope('conv2_1') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['block2_conv1_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block2_conv1_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(pool1, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act21 = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "  with tf.name_scope('conv2_2') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['block2_conv2_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block2_conv2_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(act21, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act22 = tf.nn.relu(out, name=scope)\n",
        "\n",
        "  with tf.name_scope('pool2') as scope:\n",
        "    #pooling_kernel = tf.Variable(initial_value = weights['pool_2'], trainable = False, name ='weights')\n",
        "    pool2 = tf.nn.max_pool(act22, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "  with tf.name_scope('conv3_1') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['block3_conv1_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block3_conv1_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(pool2, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act31 = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "  with tf.name_scope('conv3_2') as scope:\n",
        "    kernel = tf.Variable(initial_value = weights['block3_conv2_W_1:0'], trainable = False, name ='weights')\n",
        "    biases = tf.Variable(initial_value = weights['block3_conv2_b_1:0'], trainable = False, name ='biases')\n",
        "    conv = tf.nn.conv2d(act31, kernel, [1 , 1 , 1 , 1], padding ='SAME')\n",
        "    out = tf.nn.bias_add(conv, biases)\n",
        "    act32 = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "  with tf.name_scope('pool3') as scope:\n",
        "    #pooling_kernel = tf.Variable(initial_value = weights['pool_3'], trainable = False, name ='weights')\n",
        "    pool3 = tf.nn.max_pool(act32, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
        "    \n",
        "  with tf.variable_scope('fc') as scope:\n",
        "    flat = tf.layers.flatten(pool3)\n",
        "    logits = tf.layers.dense(flat, units=10, reuse=tf.AUTO_REUSE)\n",
        "    \n",
        "   ######################### Define Loss function ################################\n",
        "    loss =  tf.losses.sparse_softmax_cross_entropy(imgs_label_placeholder, logits)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate= learning_rate)\n",
        "    minimizer_op = optimizer.minimize(loss)\n",
        "    \n",
        "    ################## Add ops to save and restore all the variables. ###############\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    ######################### Accuracy check ################################\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    correct_pred = tf.equal(imgs_label_placeholder, predictions)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_pred , tf.float32))\n",
        "    \n",
        "    ################## Adding summary for consoling through tensorboard ###############\n",
        "    loss_summary = tf.summary.scalar(name=\"loss\", tensor=loss) #writes loss summary\n",
        "    acc_summary = tf.summary.scalar(name=\"accuracy\", tensor=accuracy) #writes accuracy summary\n",
        "    \n",
        "    \n",
        "    ######################### Start Training ################################\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        #saver.restore(sess, \"model.ckpt\")\n",
        "        print('model restored')\n",
        "        if(prediction_mode):\n",
        "          print('prediction mode on')\n",
        "          pred, a = sess.run([predictions, accuracy], {imgs_placeholder: train_data, imgs_label_placeholder: train_label})\n",
        "          print('accuracy is ', a)\n",
        "          print('pred is ', pred)\n",
        "        else:\n",
        "          print('Training mode on')\n",
        "          for k in range(50):\n",
        "              j = 0\n",
        "              unseen_data = train_data[:]\n",
        "              while(len(unseen_data)):\n",
        "                  b_size = batch_size if batch_size < len(unseen_data) else len(unseen_data)\n",
        "                  idx = np.random.choice( unseen_data.shape[0], b_size, replace=False )\n",
        "                  indices_to_keep = [i for i in range(len(unseen_data)) if i not in idx]\n",
        "                  unseen_data = unseen_data[indices_to_keep]\n",
        "                  j = j + 1\n",
        "                  training_data_batch = train_data[idx]\n",
        "                  train_label_batch = train_label[idx]\n",
        "                  _, l, acc, l_summary, a_summary =sess.run([minimizer_op, loss, accuracy, loss_summary, acc_summary], {imgs_placeholder: training_data_batch, imgs_label_placeholder: train_label_batch})\n",
        "                  writer = tf.summary.FileWriter('./graphs', sess.graph)\n",
        "                  writer.add_summary(l_summary, global_step= j) # writes loss summary\n",
        "                  writer.add_summary(a_summary, global_step=j) # writes loss summary\n",
        "                  #print('After iteration {} batch {} loss is {} and accuracy is {}%'.format(k, j, l, acc))\n",
        "\n",
        "              print('*********************************************************************')\n",
        "              print('After iteration {} loss is {} and accuracy is {}%'.format(k, l, acc*100))\n",
        "              save_path = saver.save(sess, \"model.ckpt\".format(k))\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tSj_349qbCh",
        "colab_type": "code",
        "outputId": "1891a38d-535b-4e1b-83c7-17b13ad957cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        }
      },
      "source": [
        "train_network(train_data, train_labels, 64, 1e-06)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(?, 32, 32, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-71fc2ab77f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-06\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-70e6e324c5dc>\u001b[0m in \u001b[0;36mtrain_network\u001b[0;34m(train_data, train_label, batch_size, learning_rate, prediction_mode)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mkernel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer_2_param_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layer_2_param_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'biases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mconv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'SAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'layer_2_param_0'"
          ]
        }
      ]
    }
  ]
}