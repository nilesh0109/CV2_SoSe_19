{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saliency_system.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/Saliency_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwVzzyrpD2R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "aab9b95c-874d-484d-b114-f3577d29fde0"
      },
      "source": [
        "import zipfile\n",
        "from google.colab import files, drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK13Xnq_pG2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/CV2 exercies/Archive.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EayiS3g6pJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "from __future__ import division\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBWzAZLlpLcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT= '/tmp'\n",
        "NUM_IMAGES = 1200\n",
        "def load_train_data():\n",
        "\ttraining_img_directory = ROOT+'/data/train/images'\n",
        "\ttraining_fixation_directory = ROOT+'/data/train/fixations'\n",
        "\n",
        "\ttrain_imgs = np.zeros((NUM_IMAGES, 180, 320, 3), dtype=np.uint8)\n",
        "\ttrain_fixations = np.zeros((NUM_IMAGES, 180, 320, 1), dtype=np.uint8)\n",
        "  \n",
        "\tfor i in range(1, NUM_IMAGES + 1):\n",
        "\t\timg_file = os.path.join(training_img_directory, '{:04d}.jpg'.format(i))\n",
        "\t\tfixation_file = os.path.join(training_fixation_directory, '{:04d}.jpg'.format(i))\n",
        "\t\ttrain_imgs[i-1] = imageio.imread(img_file)\n",
        "\t\tfixation = imageio.imread(fixation_file)\n",
        "\t\ttrain_fixations[i-1] = np.expand_dims(fixation, -1) # adds singleton dimension so fixation size is (180,320,1)\n",
        "\t\n",
        "\treturn train_imgs, train_fixations\n",
        "\n",
        "# Generator function will output one (image, target) tuple at a time,\n",
        "# and shuffle the data for each new epoch\n",
        "def data_generator(imgs, targets):\n",
        "\twhile True: # produce new epochs forever\n",
        "\t\t# Shuffle the data for this epoch\n",
        "\t\tidx = np.arange(imgs.shape[0])\n",
        "\t\tnp.random.shuffle(idx)\n",
        "\n",
        "\t\timgs = imgs[idx]\n",
        "\t\ttargets = targets[idx]\n",
        "\t\tfor i in range(imgs.shape[0]):\n",
        "\t\t\tyield imgs[i], targets[i]\n",
        "\n",
        "def get_batch_from_generator(gen, batchsize):\n",
        "\tbatch_imgs = []\n",
        "\tbatch_fixations = []\n",
        "\tfor i in range(batchsize):\n",
        "\t\timg, target = gen.__next__()\n",
        "\t\tbatch_imgs.append(img)\n",
        "\t\tbatch_fixations.append(target)\n",
        "\treturn np.array(batch_imgs), np.array(batch_fixations)\n",
        "\n",
        "def get_training_val_data(train_imgs, train_fixations):\n",
        "  kf = KFold(n_splits=2)\n",
        "  for train_index, test_index in kf.split(train_imgs):\n",
        "    \n",
        "    \n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFi73W_n0rEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "\n",
        "'''\n",
        "Model class\n",
        "'''\n",
        "\n",
        "class saliencyModel:\n",
        "  def __init__(self, model_weights=None, learning_rate=1e-4, batch_size=32, num_batches=100, prior_downsampling_factor=10):\n",
        "    self.dir_path = 'drive/My Drive/Colab Notebooks/CV2 exercies/'\n",
        "    self.model_path = self.dir_path+'model_cptk/my-model1'\n",
        "    self.lr_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = num_batches\n",
        "    self.prior_downsampling_factor = prior_downsampling_factor\n",
        "    self.input_h = 180\n",
        "    self.input_w = 320\n",
        "    self.prior_h = self.input_h /(2 * prior_downsampling_factor)\n",
        "    self.prior_w = self.input_w / (2 * prior_downsampling_factor)\n",
        "    self.reg_lambda = 1 / (self.prior_h * self.prior_w)\n",
        "    \n",
        "    if model_weights is not None:\n",
        "      self.load_weights(model_weights)\n",
        "    \n",
        "  def load_weights(self, model_weights):\n",
        "    vgg_weight_file = model_weights\n",
        "    self.weights = np.load(vgg_weight_file)\n",
        "  \n",
        "  def setup(self, mode= 'Train'):\n",
        "    self.input_images_placeholder = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 3])\n",
        "    self.target_images_placehodler = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 1])\n",
        "    \n",
        "    with tf.name_scope('preprocessing') as scope:\n",
        "      input_imgs = tf.image.convert_image_dtype(self.input_images_placeholder, tf.float32) * 255\n",
        "      fixations_normalized = tf.image.convert_image_dtype(self.target_images_placehodler, tf.float32)\n",
        "      mean = tf.constant([123.68 , 116.779 , 103.939], dtype = tf.float32, shape =[1,1,1,3], name ='img_mean')\n",
        "      imgs_normalized = input_imgs - mean\n",
        "\n",
        "    with tf.name_scope('conv1_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(imgs_normalized, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv1_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool1') as scope:\n",
        "      pool = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv2_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv2_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool2') as scope:\n",
        "      pool2 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv3_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool3') as scope:\n",
        "      pool3 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv4_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv4_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv4_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('pool4') as scope:\n",
        "      pool4 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "    \n",
        "    with tf.name_scope('conv5_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv5_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv5_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      conv5_3 = act\n",
        "      \n",
        "    with tf.name_scope('concat_featuremaps') as scope:\n",
        "      concatenated_feature_maps = tf.concat([pool2, pool3, pool4, conv5_3], axis=3)\n",
        "      print('IN_TRAINING_MODE is',mode =='Train')\n",
        "      regularized_feature_maps = tf.layers.dropout(concatenated_feature_maps, rate=0.5, training= mode =='Train')\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv1') as scope:\n",
        "      my_regularizer = tf.contrib.layers.l2_regularizer(1e-5)\n",
        "      act_featuremaps_conv1 = tf.layers.conv2d(regularized_feature_maps, filters=64, kernel_size=(3,3), padding='SAME', activation=tf.nn.relu, name='featuremaps_conv1', kernel_regularizer = my_regularizer, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv2') as scope:\n",
        "      act_featuremaps_conv2 = tf.layers.conv2d(act_featuremaps_conv1, filters=1, kernel_size=(1,1), activation=tf.nn.relu, name='featuremaps_conv2', kernel_regularizer = my_regularizer, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('Learned_prior') as scope:\n",
        "      prior_shape = (1, self.prior_h, self.prior_w,1)\n",
        "      prior = tf.Variable(tf.ones(prior_shape), name=\"prior\", trainable=True)\n",
        "      upsampled_prior = tf.image.resize_bilinear(prior, size=act_featuremaps_conv2.shape[1:3])\n",
        "      saliency_mixed = tf.multiply(act_featuremaps_conv2, upsampled_prior)\n",
        "      saliency_raw = tf.nn.relu(saliency_mixed)\n",
        "      \n",
        "    with tf.name_scope('loss') as scope:\n",
        "      upsampled_saliency = tf.image.resize_bilinear(saliency_raw, size=fixations_normalized.shape[1:3])\n",
        "      # normalize saliency\n",
        "      max_value_per_image = tf.reduce_max(upsampled_saliency, axis=[1,2,3], keepdims=True)\n",
        "      predicted_saliency = (upsampled_saliency / max_value_per_image)\n",
        "      \n",
        "      # Loss function from Cornia et al. (2016) [with higher weight for salient pixels]\n",
        "      alpha = 1.01\n",
        "      weight = 1.0 / (alpha - fixations_normalized)\n",
        "      loss = tf.losses.mean_squared_error(labels=fixations_normalized, \n",
        "                        predictions=predicted_saliency, \n",
        "                        weights=weight)\n",
        "      regularizer = tf.nn.l2_loss(1 - prior)\n",
        "      loss += tf.reduce_mean(self.reg_lambda * regularizer)\n",
        "      l2_loss = tf.losses.get_regularization_loss() \n",
        "      loss += l2_loss\n",
        "      \n",
        "    return predicted_saliency, loss\n",
        "    \n",
        "  def train(self, t_imgs, t_fixations):\n",
        "    \n",
        "    pred ,loss_op = self.setup(mode='Train')\n",
        "    # Optimizer settings from Cornia et al. (2016) [except for decay]\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr_rate, momentum=0.9, use_nesterov=True)\n",
        "    #optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_rate)\n",
        "    minimize_op = optimizer.minimize(loss_op)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver.restore(sess, \"model/latest1\")\n",
        "      gen = data_generator(t_imgs, t_fixations)\n",
        "      for b in range(self.num_batches):\n",
        "        batch_imgs, batch_fixations = get_batch_from_generator(gen, self.batch_size)\n",
        "        predication, batch_loss,_ = sess.run([pred, loss_op, minimize_op], feed_dict={self.input_images_placeholder: batch_imgs, self.target_images_placehodler: batch_fixations})\n",
        "\n",
        "        if b % 10 == 0:\n",
        "          #writer.add_summary(l_summary, global_step=b)\n",
        "          print('Batch {0} done: batch loss {1}'.format(b, batch_loss))\n",
        "        if b % 5000 == 0 and b != 0:\n",
        "          save_path = saver.save(sess, 'model/latest1', global_step=b)\n",
        "      save_path = saver.save(sess, 'model/latest1')\n",
        "      files.download('model/latest1.index')\n",
        "      files.download('model/latest1.meta')\n",
        "      files.download('model/latest1.data-00000-of-00001')\n",
        "      \n",
        "      \n",
        "  def test(self, test_imgs):\n",
        "    pred_saliency, l = self.setup(mode='Test')\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      saver.restore(sess, 'model/latest1')\n",
        "      saliency = sess.run(pred_saliency, feed_dict={self.input_images_placeholder: test_imgs})\n",
        "      for i in range(len(test_imgs)):\n",
        "        print('saving images')\n",
        "        saliency_img = sess.run(tf.image.convert_image_dtype(saliency[i], tf.uint8))\n",
        "        imageio.imwrite(str(i)+'.jpg', saliency_img)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhmQ0g-1-GKV",
        "colab_type": "code",
        "outputId": "2e42b4eb-c387-4ccc-bbf6-6128144d2f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32769
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=20001, batch_size=16)\n",
        "\n",
        "my_model.train(train_imgs[0:16], train_fixations[0:16])\n",
        "#my_model.train(train_imgs, train_fixations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is True\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "INFO:tensorflow:Restoring parameters from latest1-20000\n",
            "Batch 0 done: batch loss 0.0005391780869103968\n",
            "Batch 10 done: batch loss 0.0005365873803384602\n",
            "Batch 20 done: batch loss 0.0005455882055684924\n",
            "Batch 30 done: batch loss 0.0005397502682171762\n",
            "Batch 40 done: batch loss 0.0005856662173755467\n",
            "Batch 50 done: batch loss 0.0005746281240135431\n",
            "Batch 60 done: batch loss 0.0005595723632723093\n",
            "Batch 70 done: batch loss 0.0005589302163571119\n",
            "Batch 80 done: batch loss 0.0005914648063480854\n",
            "Batch 90 done: batch loss 0.0005977281834930182\n",
            "Batch 100 done: batch loss 0.0005530593916773796\n",
            "Batch 110 done: batch loss 0.0005483748391270638\n",
            "Batch 120 done: batch loss 0.0005966063472442329\n",
            "Batch 130 done: batch loss 0.0005314391455613077\n",
            "Batch 140 done: batch loss 0.0005315498565323651\n",
            "Batch 150 done: batch loss 0.0005615094560198486\n",
            "Batch 160 done: batch loss 0.0005419891676865518\n",
            "Batch 170 done: batch loss 0.0005854328046552837\n",
            "Batch 180 done: batch loss 0.000529751181602478\n",
            "Batch 190 done: batch loss 0.0005612940294668078\n",
            "Batch 200 done: batch loss 0.0005550780333578587\n",
            "Batch 210 done: batch loss 0.0005476106889545918\n",
            "Batch 220 done: batch loss 0.0005602193414233625\n",
            "Batch 230 done: batch loss 0.0005322616780176759\n",
            "Batch 240 done: batch loss 0.0005224553751759231\n",
            "Batch 250 done: batch loss 0.0005534404772333801\n",
            "Batch 260 done: batch loss 0.0005126141477376223\n",
            "Batch 270 done: batch loss 0.0005490275798365474\n",
            "Batch 280 done: batch loss 0.0005670258542522788\n",
            "Batch 290 done: batch loss 0.0005507245077751577\n",
            "Batch 300 done: batch loss 0.000537313986569643\n",
            "Batch 310 done: batch loss 0.0005724764778278768\n",
            "Batch 320 done: batch loss 0.0005570905632339418\n",
            "Batch 330 done: batch loss 0.000534652906935662\n",
            "Batch 340 done: batch loss 0.0006047773640602827\n",
            "Batch 350 done: batch loss 0.0005391381564550102\n",
            "Batch 360 done: batch loss 0.000549620483070612\n",
            "Batch 370 done: batch loss 0.0005275016301311553\n",
            "Batch 380 done: batch loss 0.0005248729721643031\n",
            "Batch 390 done: batch loss 0.0005223914631642401\n",
            "Batch 400 done: batch loss 0.0005616506678052247\n",
            "Batch 410 done: batch loss 0.0005486384616233408\n",
            "Batch 420 done: batch loss 0.000540206499863416\n",
            "Batch 430 done: batch loss 0.0005464832065626979\n",
            "Batch 440 done: batch loss 0.0005289779510349035\n",
            "Batch 450 done: batch loss 0.0005373215535655618\n",
            "Batch 460 done: batch loss 0.0005891556502319872\n",
            "Batch 470 done: batch loss 0.0005732764839194715\n",
            "Batch 480 done: batch loss 0.0005372659652493894\n",
            "Batch 490 done: batch loss 0.000555553357116878\n",
            "Batch 500 done: batch loss 0.0005462539847940207\n",
            "Batch 510 done: batch loss 0.0005367865087464452\n",
            "Batch 520 done: batch loss 0.0005339060444384813\n",
            "Batch 530 done: batch loss 0.0005389137659221888\n",
            "Batch 540 done: batch loss 0.000586247886531055\n",
            "Batch 550 done: batch loss 0.0005458560772240162\n",
            "Batch 560 done: batch loss 0.0005617272690869868\n",
            "Batch 570 done: batch loss 0.000576959690079093\n",
            "Batch 580 done: batch loss 0.0005225609056651592\n",
            "Batch 590 done: batch loss 0.0005461171385832131\n",
            "Batch 600 done: batch loss 0.0005347264814190567\n",
            "Batch 610 done: batch loss 0.0005480441614054143\n",
            "Batch 620 done: batch loss 0.0005373473977670074\n",
            "Batch 630 done: batch loss 0.0005574811366386712\n",
            "Batch 640 done: batch loss 0.0005882952245883644\n",
            "Batch 650 done: batch loss 0.0005748008261434734\n",
            "Batch 660 done: batch loss 0.0005012432229705155\n",
            "Batch 670 done: batch loss 0.0005176797276362777\n",
            "Batch 680 done: batch loss 0.0005291111883707345\n",
            "Batch 690 done: batch loss 0.0005191061645746231\n",
            "Batch 700 done: batch loss 0.0005690073012374341\n",
            "Batch 710 done: batch loss 0.0005494471406564116\n",
            "Batch 720 done: batch loss 0.0005239520105533302\n",
            "Batch 730 done: batch loss 0.0005366564728319645\n",
            "Batch 740 done: batch loss 0.0005660525639541447\n",
            "Batch 750 done: batch loss 0.0005212713149376214\n",
            "Batch 760 done: batch loss 0.0005461403634399176\n",
            "Batch 770 done: batch loss 0.0005542379803955555\n",
            "Batch 780 done: batch loss 0.0005747813847847283\n",
            "Batch 790 done: batch loss 0.0005738098989240825\n",
            "Batch 800 done: batch loss 0.000502847193274647\n",
            "Batch 810 done: batch loss 0.0005054554785601795\n",
            "Batch 820 done: batch loss 0.0005609793006442487\n",
            "Batch 830 done: batch loss 0.0005533481016755104\n",
            "Batch 840 done: batch loss 0.0005415817140601575\n",
            "Batch 850 done: batch loss 0.0005753982695750892\n",
            "Batch 860 done: batch loss 0.0005491733900271356\n",
            "Batch 870 done: batch loss 0.0005453081685118377\n",
            "Batch 880 done: batch loss 0.0005664537893608212\n",
            "Batch 890 done: batch loss 0.0005210365052334964\n",
            "Batch 900 done: batch loss 0.0005599890719167888\n",
            "Batch 910 done: batch loss 0.0005646883510053158\n",
            "Batch 920 done: batch loss 0.0005230671958997846\n",
            "Batch 930 done: batch loss 0.000512270606122911\n",
            "Batch 940 done: batch loss 0.0005617396091111004\n",
            "Batch 950 done: batch loss 0.0005112920771352947\n",
            "Batch 960 done: batch loss 0.0005426406860351562\n",
            "Batch 970 done: batch loss 0.0005507320747710764\n",
            "Batch 980 done: batch loss 0.0005904791760258377\n",
            "Batch 990 done: batch loss 0.0005408746073953807\n",
            "Batch 1000 done: batch loss 0.0005429130978882313\n",
            "Batch 1010 done: batch loss 0.0005295607261359692\n",
            "Batch 1020 done: batch loss 0.0006194571615196764\n",
            "Batch 1030 done: batch loss 0.000555500213522464\n",
            "Batch 1040 done: batch loss 0.0005386833800002933\n",
            "Batch 1050 done: batch loss 0.0005238833837211132\n",
            "Batch 1060 done: batch loss 0.0005541860009543598\n",
            "Batch 1070 done: batch loss 0.0005842838436365128\n",
            "Batch 1080 done: batch loss 0.0005507467431016266\n",
            "Batch 1090 done: batch loss 0.0005439224187284708\n",
            "Batch 1100 done: batch loss 0.0005500570987351239\n",
            "Batch 1110 done: batch loss 0.0005306171369738877\n",
            "Batch 1120 done: batch loss 0.000557000981643796\n",
            "Batch 1130 done: batch loss 0.0005509653710760176\n",
            "Batch 1140 done: batch loss 0.0005376345943659544\n",
            "Batch 1150 done: batch loss 0.0005673611303791404\n",
            "Batch 1160 done: batch loss 0.0005427696160040796\n",
            "Batch 1170 done: batch loss 0.0005516681121662259\n",
            "Batch 1180 done: batch loss 0.0005513665382750332\n",
            "Batch 1190 done: batch loss 0.0005268866661936045\n",
            "Batch 1200 done: batch loss 0.0005013898480683565\n",
            "Batch 1210 done: batch loss 0.0005318878102116287\n",
            "Batch 1220 done: batch loss 0.000556097540538758\n",
            "Batch 1230 done: batch loss 0.0005247414810582995\n",
            "Batch 1240 done: batch loss 0.0005526153254322708\n",
            "Batch 1250 done: batch loss 0.000503223214764148\n",
            "Batch 1260 done: batch loss 0.0005646056379191577\n",
            "Batch 1270 done: batch loss 0.0005283442442305386\n",
            "Batch 1280 done: batch loss 0.0005327053368091583\n",
            "Batch 1290 done: batch loss 0.0005471706972457469\n",
            "Batch 1300 done: batch loss 0.0005673510022461414\n",
            "Batch 1310 done: batch loss 0.0005634238477796316\n",
            "Batch 1320 done: batch loss 0.0005400325171649456\n",
            "Batch 1330 done: batch loss 0.0005164880421943963\n",
            "Batch 1340 done: batch loss 0.000572335091419518\n",
            "Batch 1350 done: batch loss 0.0005566944018937647\n",
            "Batch 1360 done: batch loss 0.0005644633783958852\n",
            "Batch 1370 done: batch loss 0.0005762223736383021\n",
            "Batch 1380 done: batch loss 0.0005871377070434391\n",
            "Batch 1390 done: batch loss 0.0005299185286276042\n",
            "Batch 1400 done: batch loss 0.0005854961345903575\n",
            "Batch 1410 done: batch loss 0.0006069217342883348\n",
            "Batch 1420 done: batch loss 0.0005524706211872399\n",
            "Batch 1430 done: batch loss 0.0005339810159057379\n",
            "Batch 1440 done: batch loss 0.0005318342009559274\n",
            "Batch 1450 done: batch loss 0.0005900429678149521\n",
            "Batch 1460 done: batch loss 0.0005622384487651289\n",
            "Batch 1470 done: batch loss 0.0005565956817008555\n",
            "Batch 1480 done: batch loss 0.000556675367988646\n",
            "Batch 1490 done: batch loss 0.0005669131060130894\n",
            "Batch 1500 done: batch loss 0.0005213799304328859\n",
            "Batch 1510 done: batch loss 0.0005520043196156621\n",
            "Batch 1520 done: batch loss 0.0005719973705708981\n",
            "Batch 1530 done: batch loss 0.0005453618359751999\n",
            "Batch 1540 done: batch loss 0.0005578553536906838\n",
            "Batch 1550 done: batch loss 0.0005045824218541384\n",
            "Batch 1560 done: batch loss 0.0005235430435277522\n",
            "Batch 1570 done: batch loss 0.0005181464948691428\n",
            "Batch 1580 done: batch loss 0.0005377358756959438\n",
            "Batch 1590 done: batch loss 0.0005577834672294557\n",
            "Batch 1600 done: batch loss 0.0005440425011329353\n",
            "Batch 1610 done: batch loss 0.0005370147991925478\n",
            "Batch 1620 done: batch loss 0.0005266420776024461\n",
            "Batch 1630 done: batch loss 0.0005439368542283773\n",
            "Batch 1640 done: batch loss 0.0005463300039991736\n",
            "Batch 1650 done: batch loss 0.0005891048931516707\n",
            "Batch 1660 done: batch loss 0.000552255951333791\n",
            "Batch 1670 done: batch loss 0.0005179090076126158\n",
            "Batch 1680 done: batch loss 0.0005105448653921485\n",
            "Batch 1690 done: batch loss 0.0005354610038921237\n",
            "Batch 1700 done: batch loss 0.0005834614275954664\n",
            "Batch 1710 done: batch loss 0.0005503118736669421\n",
            "Batch 1720 done: batch loss 0.0005733971484005451\n",
            "Batch 1730 done: batch loss 0.0005250281537882984\n",
            "Batch 1740 done: batch loss 0.0005122576258145273\n",
            "Batch 1750 done: batch loss 0.0005509156617335975\n",
            "Batch 1760 done: batch loss 0.0005399155197665095\n",
            "Batch 1770 done: batch loss 0.0005266100633889437\n",
            "Batch 1780 done: batch loss 0.0005129246856085956\n",
            "Batch 1790 done: batch loss 0.0005347903934307396\n",
            "Batch 1800 done: batch loss 0.0006183730438351631\n",
            "Batch 1810 done: batch loss 0.0005258528399281204\n",
            "Batch 1820 done: batch loss 0.0005621322779916227\n",
            "Batch 1830 done: batch loss 0.0005466128350235522\n",
            "Batch 1840 done: batch loss 0.0005344246164895594\n",
            "Batch 1850 done: batch loss 0.0005278497119434178\n",
            "Batch 1860 done: batch loss 0.0005820997757837176\n",
            "Batch 1870 done: batch loss 0.0005305667291395366\n",
            "Batch 1880 done: batch loss 0.0005400414229370654\n",
            "Batch 1890 done: batch loss 0.0005445556016638875\n",
            "Batch 1900 done: batch loss 0.0005347737460397184\n",
            "Batch 1910 done: batch loss 0.0005573477828875184\n",
            "Batch 1920 done: batch loss 0.0005224212654866278\n",
            "Batch 1930 done: batch loss 0.0005220220191404223\n",
            "Batch 1940 done: batch loss 0.000508045544847846\n",
            "Batch 1950 done: batch loss 0.0005334093584679067\n",
            "Batch 1960 done: batch loss 0.0005581903969869018\n",
            "Batch 1970 done: batch loss 0.0005293159047141671\n",
            "Batch 1980 done: batch loss 0.0005109385820105672\n",
            "Batch 1990 done: batch loss 0.0005366370314732194\n",
            "Batch 2000 done: batch loss 0.0005196467973291874\n",
            "Batch 2010 done: batch loss 0.000584938854444772\n",
            "Batch 2020 done: batch loss 0.0005468702875077724\n",
            "Batch 2030 done: batch loss 0.0005649433587677777\n",
            "Batch 2040 done: batch loss 0.000581697269808501\n",
            "Batch 2050 done: batch loss 0.0005488168681040406\n",
            "Batch 2060 done: batch loss 0.0005151338991709054\n",
            "Batch 2070 done: batch loss 0.0005276601295918226\n",
            "Batch 2080 done: batch loss 0.00053931720321998\n",
            "Batch 2090 done: batch loss 0.0006178629118949175\n",
            "Batch 2100 done: batch loss 0.0005253326380625367\n",
            "Batch 2110 done: batch loss 0.0005396516644395888\n",
            "Batch 2120 done: batch loss 0.0005316812894307077\n",
            "Batch 2130 done: batch loss 0.0005613985704258084\n",
            "Batch 2140 done: batch loss 0.0005094080115668476\n",
            "Batch 2150 done: batch loss 0.0005162429297342896\n",
            "Batch 2160 done: batch loss 0.0005780528881587088\n",
            "Batch 2170 done: batch loss 0.0005451433244161308\n",
            "Batch 2180 done: batch loss 0.0005652267718687654\n",
            "Batch 2190 done: batch loss 0.0005083189462311566\n",
            "Batch 2200 done: batch loss 0.000551328994333744\n",
            "Batch 2210 done: batch loss 0.000536909035872668\n",
            "Batch 2220 done: batch loss 0.0005391070153564215\n",
            "Batch 2230 done: batch loss 0.0005435461062006652\n",
            "Batch 2240 done: batch loss 0.000533739454112947\n",
            "Batch 2250 done: batch loss 0.0005336604081094265\n",
            "Batch 2260 done: batch loss 0.0005242933984845877\n",
            "Batch 2270 done: batch loss 0.0004898250917904079\n",
            "Batch 2280 done: batch loss 0.0005618165014311671\n",
            "Batch 2290 done: batch loss 0.000540197070222348\n",
            "Batch 2300 done: batch loss 0.0005028207087889314\n",
            "Batch 2310 done: batch loss 0.0004999034572392702\n",
            "Batch 2320 done: batch loss 0.000576626742258668\n",
            "Batch 2330 done: batch loss 0.0005083062569610775\n",
            "Batch 2340 done: batch loss 0.0005119866691529751\n",
            "Batch 2350 done: batch loss 0.0005074313376098871\n",
            "Batch 2360 done: batch loss 0.0005446101422421634\n",
            "Batch 2370 done: batch loss 0.0005314720328897238\n",
            "Batch 2380 done: batch loss 0.0005473041092045605\n",
            "Batch 2390 done: batch loss 0.0005445986753329635\n",
            "Batch 2400 done: batch loss 0.0005390001460909843\n",
            "Batch 2410 done: batch loss 0.0005575264804065228\n",
            "Batch 2420 done: batch loss 0.0005543286097235978\n",
            "Batch 2430 done: batch loss 0.0005110289203003049\n",
            "Batch 2440 done: batch loss 0.0005896394141018391\n",
            "Batch 2450 done: batch loss 0.0005411567399278283\n",
            "Batch 2460 done: batch loss 0.0005316230817697942\n",
            "Batch 2470 done: batch loss 0.0005199410952627659\n",
            "Batch 2480 done: batch loss 0.0005507335299625993\n",
            "Batch 2490 done: batch loss 0.0005495357327163219\n",
            "Batch 2500 done: batch loss 0.000531782628968358\n",
            "Batch 2510 done: batch loss 0.0005477842059917748\n",
            "Batch 2520 done: batch loss 0.0005409433506429195\n",
            "Batch 2530 done: batch loss 0.0005357105983421206\n",
            "Batch 2540 done: batch loss 0.0005391269805841148\n",
            "Batch 2550 done: batch loss 0.0005145201575942338\n",
            "Batch 2560 done: batch loss 0.0005182603490538895\n",
            "Batch 2570 done: batch loss 0.0005242316983640194\n",
            "Batch 2580 done: batch loss 0.0005254617426544428\n",
            "Batch 2590 done: batch loss 0.000515215506311506\n",
            "Batch 2600 done: batch loss 0.0005058433744125068\n",
            "Batch 2610 done: batch loss 0.0005539595731534064\n",
            "Batch 2620 done: batch loss 0.0005268390523269773\n",
            "Batch 2630 done: batch loss 0.0004867335082963109\n",
            "Batch 2640 done: batch loss 0.0005176170379854739\n",
            "Batch 2650 done: batch loss 0.0005316500901244581\n",
            "Batch 2660 done: batch loss 0.0005044419667683542\n",
            "Batch 2670 done: batch loss 0.0005205196794122458\n",
            "Batch 2680 done: batch loss 0.0005544541636481881\n",
            "Batch 2690 done: batch loss 0.0005627505015581846\n",
            "Batch 2700 done: batch loss 0.0005587642663158476\n",
            "Batch 2710 done: batch loss 0.0005329885170795023\n",
            "Batch 2720 done: batch loss 0.0005544240120798349\n",
            "Batch 2730 done: batch loss 0.000540969951543957\n",
            "Batch 2740 done: batch loss 0.0005239210440777242\n",
            "Batch 2750 done: batch loss 0.0005254822899587452\n",
            "Batch 2760 done: batch loss 0.000528163043782115\n",
            "Batch 2770 done: batch loss 0.0005408952129073441\n",
            "Batch 2780 done: batch loss 0.0005090076010674238\n",
            "Batch 2790 done: batch loss 0.0005241231410764158\n",
            "Batch 2800 done: batch loss 0.0005423763650469482\n",
            "Batch 2810 done: batch loss 0.0005317081231623888\n",
            "Batch 2820 done: batch loss 0.0005190146621316671\n",
            "Batch 2830 done: batch loss 0.0005179666914045811\n",
            "Batch 2840 done: batch loss 0.0005528517649509013\n",
            "Batch 2850 done: batch loss 0.0005194709519855678\n",
            "Batch 2860 done: batch loss 0.000533906277269125\n",
            "Batch 2870 done: batch loss 0.0005349835264496505\n",
            "Batch 2880 done: batch loss 0.0005392525345087051\n",
            "Batch 2890 done: batch loss 0.0005008823936805129\n",
            "Batch 2900 done: batch loss 0.0005676457658410072\n",
            "Batch 2910 done: batch loss 0.0005575887626037002\n",
            "Batch 2920 done: batch loss 0.0005362819065339863\n",
            "Batch 2930 done: batch loss 0.000560609856620431\n",
            "Batch 2940 done: batch loss 0.0005168679053895175\n",
            "Batch 2950 done: batch loss 0.0005318305920809507\n",
            "Batch 2960 done: batch loss 0.0005633338587358594\n",
            "Batch 2970 done: batch loss 0.000567572598811239\n",
            "Batch 2980 done: batch loss 0.0005383631796576083\n",
            "Batch 2990 done: batch loss 0.0005219404119998217\n",
            "Batch 3000 done: batch loss 0.0005598922725766897\n",
            "Batch 3010 done: batch loss 0.0005331659922376275\n",
            "Batch 3020 done: batch loss 0.0005223144544288516\n",
            "Batch 3030 done: batch loss 0.0005133613012731075\n",
            "Batch 3040 done: batch loss 0.0005072894273325801\n",
            "Batch 3050 done: batch loss 0.0005657552974298596\n",
            "Batch 3060 done: batch loss 0.0005421245004981756\n",
            "Batch 3070 done: batch loss 0.0005688778473995626\n",
            "Batch 3080 done: batch loss 0.0005737887695431709\n",
            "Batch 3090 done: batch loss 0.0005093771032989025\n",
            "Batch 3100 done: batch loss 0.0005310017731972039\n",
            "Batch 3110 done: batch loss 0.0005291840061545372\n",
            "Batch 3120 done: batch loss 0.0005368909332901239\n",
            "Batch 3130 done: batch loss 0.0004988068831153214\n",
            "Batch 3140 done: batch loss 0.0005249923560768366\n",
            "Batch 3150 done: batch loss 0.0005239917081780732\n",
            "Batch 3160 done: batch loss 0.0005120074492879212\n",
            "Batch 3170 done: batch loss 0.0005246564978733659\n",
            "Batch 3180 done: batch loss 0.0005296621820889413\n",
            "Batch 3190 done: batch loss 0.0005141409928910434\n",
            "Batch 3200 done: batch loss 0.0005416347994469106\n",
            "Batch 3210 done: batch loss 0.0005383098614402115\n",
            "Batch 3220 done: batch loss 0.0005577451665885746\n",
            "Batch 3230 done: batch loss 0.0005045352154411376\n",
            "Batch 3240 done: batch loss 0.0005616743001155555\n",
            "Batch 3250 done: batch loss 0.0005683664348907769\n",
            "Batch 3260 done: batch loss 0.0005407873541116714\n",
            "Batch 3270 done: batch loss 0.0005317115574143827\n",
            "Batch 3280 done: batch loss 0.0005322731449268758\n",
            "Batch 3290 done: batch loss 0.0005035527283325791\n",
            "Batch 3300 done: batch loss 0.0005047142622061074\n",
            "Batch 3310 done: batch loss 0.0005173670360818505\n",
            "Batch 3320 done: batch loss 0.0005372197483666241\n",
            "Batch 3330 done: batch loss 0.0005062250420451164\n",
            "Batch 3340 done: batch loss 0.0005085713346488774\n",
            "Batch 3350 done: batch loss 0.0004997958312742412\n",
            "Batch 3360 done: batch loss 0.0005203474429436028\n",
            "Batch 3370 done: batch loss 0.0005934377550147474\n",
            "Batch 3380 done: batch loss 0.000544377020560205\n",
            "Batch 3390 done: batch loss 0.0005456922808662057\n",
            "Batch 3400 done: batch loss 0.00054711609845981\n",
            "Batch 3410 done: batch loss 0.000521360372658819\n",
            "Batch 3420 done: batch loss 0.0005206806818023324\n",
            "Batch 3430 done: batch loss 0.0005596085102297366\n",
            "Batch 3440 done: batch loss 0.0005497145466506481\n",
            "Batch 3450 done: batch loss 0.0005218269652687013\n",
            "Batch 3460 done: batch loss 0.0005138775450177491\n",
            "Batch 3470 done: batch loss 0.0005619946750812232\n",
            "Batch 3480 done: batch loss 0.0005567842163145542\n",
            "Batch 3490 done: batch loss 0.0005499004037119448\n",
            "Batch 3500 done: batch loss 0.0005251489346846938\n",
            "Batch 3510 done: batch loss 0.0005444332491606474\n",
            "Batch 3520 done: batch loss 0.0005095089436508715\n",
            "Batch 3530 done: batch loss 0.0005067865713499486\n",
            "Batch 3540 done: batch loss 0.0005649953964166343\n",
            "Batch 3550 done: batch loss 0.0005893689813092351\n",
            "Batch 3560 done: batch loss 0.000545946997590363\n",
            "Batch 3570 done: batch loss 0.0005191676318645477\n",
            "Batch 3580 done: batch loss 0.0005158651038073003\n",
            "Batch 3590 done: batch loss 0.0005499990074895322\n",
            "Batch 3600 done: batch loss 0.0005152823869138956\n",
            "Batch 3610 done: batch loss 0.0005636359564960003\n",
            "Batch 3620 done: batch loss 0.0005486496374942362\n",
            "Batch 3630 done: batch loss 0.0005050222971476614\n",
            "Batch 3640 done: batch loss 0.0005128584452904761\n",
            "Batch 3650 done: batch loss 0.0005197097780182958\n",
            "Batch 3660 done: batch loss 0.0005232961848378181\n",
            "Batch 3670 done: batch loss 0.0005291373818181455\n",
            "Batch 3680 done: batch loss 0.000539480010047555\n",
            "Batch 3690 done: batch loss 0.0005267240339890122\n",
            "Batch 3700 done: batch loss 0.0005528924521058798\n",
            "Batch 3710 done: batch loss 0.0004984108963981271\n",
            "Batch 3720 done: batch loss 0.0005367898847907782\n",
            "Batch 3730 done: batch loss 0.0005480095860548317\n",
            "Batch 3740 done: batch loss 0.0004984288243576884\n",
            "Batch 3750 done: batch loss 0.0005671944818459451\n",
            "Batch 3760 done: batch loss 0.0005978727713227272\n",
            "Batch 3770 done: batch loss 0.0005318267503753304\n",
            "Batch 3780 done: batch loss 0.0004975192714482546\n",
            "Batch 3790 done: batch loss 0.000512234284542501\n",
            "Batch 3800 done: batch loss 0.0005287140375003219\n",
            "Batch 3810 done: batch loss 0.0005191322998143733\n",
            "Batch 3820 done: batch loss 0.0005460596294142306\n",
            "Batch 3830 done: batch loss 0.0005289726541377604\n",
            "Batch 3840 done: batch loss 0.0005304358201101422\n",
            "Batch 3850 done: batch loss 0.0005537328543141484\n",
            "Batch 3860 done: batch loss 0.0005169730284251273\n",
            "Batch 3870 done: batch loss 0.0005489317118190229\n",
            "Batch 3880 done: batch loss 0.000547133618965745\n",
            "Batch 3890 done: batch loss 0.0005123420851305127\n",
            "Batch 3900 done: batch loss 0.0005480421241372824\n",
            "Batch 3910 done: batch loss 0.000558755302336067\n",
            "Batch 3920 done: batch loss 0.0005229342496022582\n",
            "Batch 3930 done: batch loss 0.0005296613089740276\n",
            "Batch 3940 done: batch loss 0.0005631488747894764\n",
            "Batch 3950 done: batch loss 0.0005189995281398296\n",
            "Batch 3960 done: batch loss 0.0005543771549127996\n",
            "Batch 3970 done: batch loss 0.0005233103292994201\n",
            "Batch 3980 done: batch loss 0.0005446469294838607\n",
            "Batch 3990 done: batch loss 0.0005143900634720922\n",
            "Batch 4000 done: batch loss 0.0005184438778087497\n",
            "Batch 4010 done: batch loss 0.0005130160134285688\n",
            "Batch 4020 done: batch loss 0.0005251542315818369\n",
            "Batch 4030 done: batch loss 0.000510526355355978\n",
            "Batch 4040 done: batch loss 0.0005169010837562382\n",
            "Batch 4050 done: batch loss 0.000548795738723129\n",
            "Batch 4060 done: batch loss 0.0005250215181149542\n",
            "Batch 4070 done: batch loss 0.0004907076945528388\n",
            "Batch 4080 done: batch loss 0.00048581702867522836\n",
            "Batch 4090 done: batch loss 0.000551971432287246\n",
            "Batch 4100 done: batch loss 0.00048788246931508183\n",
            "Batch 4110 done: batch loss 0.000537441810593009\n",
            "Batch 4120 done: batch loss 0.000541821529623121\n",
            "Batch 4130 done: batch loss 0.0005558562115766108\n",
            "Batch 4140 done: batch loss 0.000500377849675715\n",
            "Batch 4150 done: batch loss 0.0005498153623193502\n",
            "Batch 4160 done: batch loss 0.0005466901930049062\n",
            "Batch 4170 done: batch loss 0.0005416585481725633\n",
            "Batch 4180 done: batch loss 0.0005286072264425457\n",
            "Batch 4190 done: batch loss 0.0004867742827627808\n",
            "Batch 4200 done: batch loss 0.0005261259502731264\n",
            "Batch 4210 done: batch loss 0.0004962846869602799\n",
            "Batch 4220 done: batch loss 0.0005230134120211005\n",
            "Batch 4230 done: batch loss 0.000565169146284461\n",
            "Batch 4240 done: batch loss 0.0005424377741292119\n",
            "Batch 4250 done: batch loss 0.0004971139132976532\n",
            "Batch 4260 done: batch loss 0.0005390450824052095\n",
            "Batch 4270 done: batch loss 0.0005606092745438218\n",
            "Batch 4280 done: batch loss 0.0005256164004094899\n",
            "Batch 4290 done: batch loss 0.0005424555856734514\n",
            "Batch 4300 done: batch loss 0.0005713103455491364\n",
            "Batch 4310 done: batch loss 0.0005103132571093738\n",
            "Batch 4320 done: batch loss 0.0005611403612419963\n",
            "Batch 4330 done: batch loss 0.0005254910793155432\n",
            "Batch 4340 done: batch loss 0.0005005025886930525\n",
            "Batch 4350 done: batch loss 0.0005563778686337173\n",
            "Batch 4360 done: batch loss 0.0005492184427566826\n",
            "Batch 4370 done: batch loss 0.000494857900775969\n",
            "Batch 4380 done: batch loss 0.0005632158135995269\n",
            "Batch 4390 done: batch loss 0.0005053781205788255\n",
            "Batch 4400 done: batch loss 0.0005366199766285717\n",
            "Batch 4410 done: batch loss 0.0005296878516674042\n",
            "Batch 4420 done: batch loss 0.0005429867305792868\n",
            "Batch 4430 done: batch loss 0.0005420628003776073\n",
            "Batch 4440 done: batch loss 0.0005350853898562491\n",
            "Batch 4450 done: batch loss 0.0005419160588644445\n",
            "Batch 4460 done: batch loss 0.0005703374627046287\n",
            "Batch 4470 done: batch loss 0.0004959653597325087\n",
            "Batch 4480 done: batch loss 0.0005593803361989558\n",
            "Batch 4490 done: batch loss 0.0005094403168186545\n",
            "Batch 4500 done: batch loss 0.0005389079451560974\n",
            "Batch 4510 done: batch loss 0.0005154995014891028\n",
            "Batch 4520 done: batch loss 0.0005341534852050245\n",
            "Batch 4530 done: batch loss 0.0005391174927353859\n",
            "Batch 4540 done: batch loss 0.0005249269306659698\n",
            "Batch 4550 done: batch loss 0.0004946536500938237\n",
            "Batch 4560 done: batch loss 0.0005264434730634093\n",
            "Batch 4570 done: batch loss 0.0005384751129895449\n",
            "Batch 4580 done: batch loss 0.0005032701301388443\n",
            "Batch 4590 done: batch loss 0.0005147394258528948\n",
            "Batch 4600 done: batch loss 0.0005305723170749843\n",
            "Batch 4610 done: batch loss 0.0005235671414993703\n",
            "Batch 4620 done: batch loss 0.0005637769354507327\n",
            "Batch 4630 done: batch loss 0.0005257282755337656\n",
            "Batch 4640 done: batch loss 0.0005447675939649343\n",
            "Batch 4650 done: batch loss 0.0005610522930510342\n",
            "Batch 4660 done: batch loss 0.0005608241190202534\n",
            "Batch 4670 done: batch loss 0.0005370806320570409\n",
            "Batch 4680 done: batch loss 0.0004993377369828522\n",
            "Batch 4690 done: batch loss 0.0005064923316240311\n",
            "Batch 4700 done: batch loss 0.0005185240879654884\n",
            "Batch 4710 done: batch loss 0.0005413646576926112\n",
            "Batch 4720 done: batch loss 0.0005594456451945007\n",
            "Batch 4730 done: batch loss 0.0005172427045181394\n",
            "Batch 4740 done: batch loss 0.0005341413198038936\n",
            "Batch 4750 done: batch loss 0.00048799169599078596\n",
            "Batch 4760 done: batch loss 0.0004973215400241315\n",
            "Batch 4770 done: batch loss 0.0005388600984588265\n",
            "Batch 4780 done: batch loss 0.0005703518399968743\n",
            "Batch 4790 done: batch loss 0.0005356741603463888\n",
            "Batch 4800 done: batch loss 0.000565206864848733\n",
            "Batch 4810 done: batch loss 0.0005090438644401729\n",
            "Batch 4820 done: batch loss 0.0005151236546225846\n",
            "Batch 4830 done: batch loss 0.0004945216351188719\n",
            "Batch 4840 done: batch loss 0.0005346554680727422\n",
            "Batch 4850 done: batch loss 0.0005110338679514825\n",
            "Batch 4860 done: batch loss 0.0005409825243987143\n",
            "Batch 4870 done: batch loss 0.0005120563437230885\n",
            "Batch 4880 done: batch loss 0.0005161408917047083\n",
            "Batch 4890 done: batch loss 0.0005590742803178728\n",
            "Batch 4900 done: batch loss 0.0005431062309071422\n",
            "Batch 4910 done: batch loss 0.0005243768682703376\n",
            "Batch 4920 done: batch loss 0.0005751702701672912\n",
            "Batch 4930 done: batch loss 0.0005414800252765417\n",
            "Batch 4940 done: batch loss 0.0005279643228277564\n",
            "Batch 4950 done: batch loss 0.0005067295278422534\n",
            "Batch 4960 done: batch loss 0.000524685310665518\n",
            "Batch 4970 done: batch loss 0.0005667295772582293\n",
            "Batch 4980 done: batch loss 0.0005351828876882792\n",
            "Batch 4990 done: batch loss 0.0005404867115430534\n",
            "Batch 5000 done: batch loss 0.0005245678476057947\n",
            "Batch 5010 done: batch loss 0.0005199069855734706\n",
            "Batch 5020 done: batch loss 0.0005063500138930976\n",
            "Batch 5030 done: batch loss 0.0005115105304867029\n",
            "Batch 5040 done: batch loss 0.0005460747051984072\n",
            "Batch 5050 done: batch loss 0.000511174148414284\n",
            "Batch 5060 done: batch loss 0.0005230577662587166\n",
            "Batch 5070 done: batch loss 0.0004979569930583239\n",
            "Batch 5080 done: batch loss 0.0005413017352111638\n",
            "Batch 5090 done: batch loss 0.0005070570623502135\n",
            "Batch 5100 done: batch loss 0.0005372153245843947\n",
            "Batch 5110 done: batch loss 0.0005050738691352308\n",
            "Batch 5120 done: batch loss 0.0005219113663770258\n",
            "Batch 5130 done: batch loss 0.0005057731177657843\n",
            "Batch 5140 done: batch loss 0.0005728729302063584\n",
            "Batch 5150 done: batch loss 0.0005215536220930517\n",
            "Batch 5160 done: batch loss 0.0005122377770021558\n",
            "Batch 5170 done: batch loss 0.0004943704698234797\n",
            "Batch 5180 done: batch loss 0.0005309682455845177\n",
            "Batch 5190 done: batch loss 0.000541538989637047\n",
            "Batch 5200 done: batch loss 0.0005275790463201702\n",
            "Batch 5210 done: batch loss 0.0005217087455093861\n",
            "Batch 5220 done: batch loss 0.000530523422639817\n",
            "Batch 5230 done: batch loss 0.000530025630723685\n",
            "Batch 5240 done: batch loss 0.0005147302290424705\n",
            "Batch 5250 done: batch loss 0.0005386295379139483\n",
            "Batch 5260 done: batch loss 0.0005266800872050226\n",
            "Batch 5270 done: batch loss 0.0005020671524107456\n",
            "Batch 5280 done: batch loss 0.0005817852797918022\n",
            "Batch 5290 done: batch loss 0.0004950155853293836\n",
            "Batch 5300 done: batch loss 0.0005254212883301079\n",
            "Batch 5310 done: batch loss 0.0004782508476637304\n",
            "Batch 5320 done: batch loss 0.0005174108082428575\n",
            "Batch 5330 done: batch loss 0.0005371001898311079\n",
            "Batch 5340 done: batch loss 0.0005250924732536077\n",
            "Batch 5350 done: batch loss 0.000516502361278981\n",
            "Batch 5360 done: batch loss 0.0005502154235728085\n",
            "Batch 5370 done: batch loss 0.0005226937355473638\n",
            "Batch 5380 done: batch loss 0.0005389970610849559\n",
            "Batch 5390 done: batch loss 0.0005711758276447654\n",
            "Batch 5400 done: batch loss 0.0005436165956780314\n",
            "Batch 5410 done: batch loss 0.0005076706293039024\n",
            "Batch 5420 done: batch loss 0.0005544114392250776\n",
            "Batch 5430 done: batch loss 0.0005237782024778426\n",
            "Batch 5440 done: batch loss 0.000542059016879648\n",
            "Batch 5450 done: batch loss 0.000499281391967088\n",
            "Batch 5460 done: batch loss 0.0005105329328216612\n",
            "Batch 5470 done: batch loss 0.000515541760250926\n",
            "Batch 5480 done: batch loss 0.0005228841328062117\n",
            "Batch 5490 done: batch loss 0.0005289128748700023\n",
            "Batch 5500 done: batch loss 0.0005399327492341399\n",
            "Batch 5510 done: batch loss 0.0005483318818733096\n",
            "Batch 5520 done: batch loss 0.000568305142223835\n",
            "Batch 5530 done: batch loss 0.000507025804836303\n",
            "Batch 5540 done: batch loss 0.0005122013972140849\n",
            "Batch 5550 done: batch loss 0.0005128026241436601\n",
            "Batch 5560 done: batch loss 0.0004988576984032989\n",
            "Batch 5570 done: batch loss 0.0005224464694038033\n",
            "Batch 5580 done: batch loss 0.0005644842167384923\n",
            "Batch 5590 done: batch loss 0.0005415299674496055\n",
            "Batch 5600 done: batch loss 0.0004929560818709433\n",
            "Batch 5610 done: batch loss 0.0005100858979858458\n",
            "Batch 5620 done: batch loss 0.0005111570353619754\n",
            "Batch 5630 done: batch loss 0.0005128593184053898\n",
            "Batch 5640 done: batch loss 0.0005328443367034197\n",
            "Batch 5650 done: batch loss 0.0004794441629201174\n",
            "Batch 5660 done: batch loss 0.0005473574856296182\n",
            "Batch 5670 done: batch loss 0.0005285634542815387\n",
            "Batch 5680 done: batch loss 0.0005051245680078864\n",
            "Batch 5690 done: batch loss 0.0004935655160807073\n",
            "Batch 5700 done: batch loss 0.000526559422723949\n",
            "Batch 5710 done: batch loss 0.0005596046103164554\n",
            "Batch 5720 done: batch loss 0.0004920399514958262\n",
            "Batch 5730 done: batch loss 0.0004802600888069719\n",
            "Batch 5740 done: batch loss 0.0005157051491551101\n",
            "Batch 5750 done: batch loss 0.0005058501847088337\n",
            "Batch 5760 done: batch loss 0.0005168998031876981\n",
            "Batch 5770 done: batch loss 0.0005098608671687543\n",
            "Batch 5780 done: batch loss 0.00054772017756477\n",
            "Batch 5790 done: batch loss 0.0005362746305763721\n",
            "Batch 5800 done: batch loss 0.0005032018525525928\n",
            "Batch 5810 done: batch loss 0.0005302319768816233\n",
            "Batch 5820 done: batch loss 0.0005054686916992068\n",
            "Batch 5830 done: batch loss 0.0005119300330989063\n",
            "Batch 5840 done: batch loss 0.0005070171318948269\n",
            "Batch 5850 done: batch loss 0.0005467843730002642\n",
            "Batch 5860 done: batch loss 0.0005089850164949894\n",
            "Batch 5870 done: batch loss 0.000520798028446734\n",
            "Batch 5880 done: batch loss 0.0005382720264606178\n",
            "Batch 5890 done: batch loss 0.0005208991351537406\n",
            "Batch 5900 done: batch loss 0.0004834224237129092\n",
            "Batch 5910 done: batch loss 0.0004930660361424088\n",
            "Batch 5920 done: batch loss 0.0005354795139282942\n",
            "Batch 5930 done: batch loss 0.0005441753310151398\n",
            "Batch 5940 done: batch loss 0.0005233242409303784\n",
            "Batch 5950 done: batch loss 0.0005396240740083158\n",
            "Batch 5960 done: batch loss 0.00047680892748758197\n",
            "Batch 5970 done: batch loss 0.0005386184202507138\n",
            "Batch 5980 done: batch loss 0.0005453976336866617\n",
            "Batch 5990 done: batch loss 0.0005331420688889921\n",
            "Batch 6000 done: batch loss 0.0005363654927350581\n",
            "Batch 6010 done: batch loss 0.0005714078433811665\n",
            "Batch 6020 done: batch loss 0.0004959690268151462\n",
            "Batch 6030 done: batch loss 0.0005600028089247644\n",
            "Batch 6040 done: batch loss 0.0004897914477623999\n",
            "Batch 6050 done: batch loss 0.0005256382864899933\n",
            "Batch 6060 done: batch loss 0.0005132954102009535\n",
            "Batch 6070 done: batch loss 0.0005166280316188931\n",
            "Batch 6080 done: batch loss 0.0005405751289799809\n",
            "Batch 6090 done: batch loss 0.0005169970681890845\n",
            "Batch 6100 done: batch loss 0.0005468523595482111\n",
            "Batch 6110 done: batch loss 0.0005352512234821916\n",
            "Batch 6120 done: batch loss 0.0005117395194247365\n",
            "Batch 6130 done: batch loss 0.0005465327412821352\n",
            "Batch 6140 done: batch loss 0.0005370229482650757\n",
            "Batch 6150 done: batch loss 0.0005143918097019196\n",
            "Batch 6160 done: batch loss 0.0005092931678518653\n",
            "Batch 6170 done: batch loss 0.0005028891609981656\n",
            "Batch 6180 done: batch loss 0.0005583436577580869\n",
            "Batch 6190 done: batch loss 0.0005081861163489521\n",
            "Batch 6200 done: batch loss 0.0005482162814587355\n",
            "Batch 6210 done: batch loss 0.0005704229697585106\n",
            "Batch 6220 done: batch loss 0.0005374373286031187\n",
            "Batch 6230 done: batch loss 0.0005652547115460038\n",
            "Batch 6240 done: batch loss 0.0005336819449439645\n",
            "Batch 6250 done: batch loss 0.0005086900782771409\n",
            "Batch 6260 done: batch loss 0.000538277963642031\n",
            "Batch 6270 done: batch loss 0.0004973938339389861\n",
            "Batch 6280 done: batch loss 0.0004989724257029593\n",
            "Batch 6290 done: batch loss 0.00047404953511431813\n",
            "Batch 6300 done: batch loss 0.0005239134770818055\n",
            "Batch 6310 done: batch loss 0.0005474380450323224\n",
            "Batch 6320 done: batch loss 0.000530859746504575\n",
            "Batch 6330 done: batch loss 0.0005433290498331189\n",
            "Batch 6340 done: batch loss 0.0005460571264848113\n",
            "Batch 6350 done: batch loss 0.0005302461795508862\n",
            "Batch 6360 done: batch loss 0.0005199906299822032\n",
            "Batch 6370 done: batch loss 0.0004905677051283419\n",
            "Batch 6380 done: batch loss 0.0005268722306936979\n",
            "Batch 6390 done: batch loss 0.00047313555842265487\n",
            "Batch 6400 done: batch loss 0.0005474922945722938\n",
            "Batch 6410 done: batch loss 0.0005195181583985686\n",
            "Batch 6420 done: batch loss 0.0005307706305757165\n",
            "Batch 6430 done: batch loss 0.0004977344651706517\n",
            "Batch 6440 done: batch loss 0.0005013143527321517\n",
            "Batch 6450 done: batch loss 0.00048572730156593025\n",
            "Batch 6460 done: batch loss 0.0005361901130527258\n",
            "Batch 6470 done: batch loss 0.0005622084718197584\n",
            "Batch 6480 done: batch loss 0.0005178783903829753\n",
            "Batch 6490 done: batch loss 0.0005027712322771549\n",
            "Batch 6500 done: batch loss 0.0005270526162348688\n",
            "Batch 6510 done: batch loss 0.000528808799572289\n",
            "Batch 6520 done: batch loss 0.0005055652582086623\n",
            "Batch 6530 done: batch loss 0.0004995624185539782\n",
            "Batch 6540 done: batch loss 0.0005378659698180854\n",
            "Batch 6550 done: batch loss 0.0005017772200517356\n",
            "Batch 6560 done: batch loss 0.0005119233974255621\n",
            "Batch 6570 done: batch loss 0.0005163062596693635\n",
            "Batch 6580 done: batch loss 0.0005059575196355581\n",
            "Batch 6590 done: batch loss 0.0005536556709557772\n",
            "Batch 6600 done: batch loss 0.0005116714164614677\n",
            "Batch 6610 done: batch loss 0.0005243743071332574\n",
            "Batch 6620 done: batch loss 0.0005666730576194823\n",
            "Batch 6630 done: batch loss 0.00051715417066589\n",
            "Batch 6640 done: batch loss 0.0005383215029723942\n",
            "Batch 6650 done: batch loss 0.0005282593774609268\n",
            "Batch 6660 done: batch loss 0.0005223952466621995\n",
            "Batch 6670 done: batch loss 0.0005591251538135111\n",
            "Batch 6680 done: batch loss 0.0005041605327278376\n",
            "Batch 6690 done: batch loss 0.0005480223335325718\n",
            "Batch 6700 done: batch loss 0.0005210929084569216\n",
            "Batch 6710 done: batch loss 0.0005067996680736542\n",
            "Batch 6720 done: batch loss 0.0004996545030735433\n",
            "Batch 6730 done: batch loss 0.0005166128976270556\n",
            "Batch 6740 done: batch loss 0.0004996416391804814\n",
            "Batch 6750 done: batch loss 0.0004982242244295776\n",
            "Batch 6760 done: batch loss 0.0005434402264654636\n",
            "Batch 6770 done: batch loss 0.0005215878481976688\n",
            "Batch 6780 done: batch loss 0.0005159213906154037\n",
            "Batch 6790 done: batch loss 0.0005127337062731385\n",
            "Batch 6800 done: batch loss 0.000541275308933109\n",
            "Batch 6810 done: batch loss 0.0005254597053863108\n",
            "Batch 6820 done: batch loss 0.0005365608958527446\n",
            "Batch 6830 done: batch loss 0.0004992707399651408\n",
            "Batch 6840 done: batch loss 0.000530403689481318\n",
            "Batch 6850 done: batch loss 0.00050815922440961\n",
            "Batch 6860 done: batch loss 0.0005146736511960626\n",
            "Batch 6870 done: batch loss 0.0004980872618034482\n",
            "Batch 6880 done: batch loss 0.0005095782107673585\n",
            "Batch 6890 done: batch loss 0.000505144998896867\n",
            "Batch 6900 done: batch loss 0.0005208153161220253\n",
            "Batch 6910 done: batch loss 0.0004997160285711288\n",
            "Batch 6920 done: batch loss 0.000554300204385072\n",
            "Batch 6930 done: batch loss 0.000550131662748754\n",
            "Batch 6940 done: batch loss 0.0005096084205433726\n",
            "Batch 6950 done: batch loss 0.0005015960778109729\n",
            "Batch 6960 done: batch loss 0.0005044381832703948\n",
            "Batch 6970 done: batch loss 0.000500256719533354\n",
            "Batch 6980 done: batch loss 0.0005184395122341812\n",
            "Batch 6990 done: batch loss 0.0005155208054929972\n",
            "Batch 7000 done: batch loss 0.0005137253901921213\n",
            "Batch 7010 done: batch loss 0.0004730766813736409\n",
            "Batch 7020 done: batch loss 0.0004937684861943126\n",
            "Batch 7030 done: batch loss 0.0004800720198545605\n",
            "Batch 7040 done: batch loss 0.0005554794333875179\n",
            "Batch 7050 done: batch loss 0.000510241836309433\n",
            "Batch 7060 done: batch loss 0.0005043856217525899\n",
            "Batch 7070 done: batch loss 0.00048438613885082304\n",
            "Batch 7080 done: batch loss 0.0004979300429113209\n",
            "Batch 7090 done: batch loss 0.0005297554889693856\n",
            "Batch 7100 done: batch loss 0.0005029537715017796\n",
            "Batch 7110 done: batch loss 0.000480820017401129\n",
            "Batch 7120 done: batch loss 0.0005115066887810826\n",
            "Batch 7130 done: batch loss 0.0005022522527724504\n",
            "Batch 7140 done: batch loss 0.0006053896504454315\n",
            "Batch 7150 done: batch loss 0.0005585543694905937\n",
            "Batch 7160 done: batch loss 0.0005434408085420728\n",
            "Batch 7170 done: batch loss 0.0005060487892478704\n",
            "Batch 7180 done: batch loss 0.000558316707611084\n",
            "Batch 7190 done: batch loss 0.0005451561301015317\n",
            "Batch 7200 done: batch loss 0.0005005288985557854\n",
            "Batch 7210 done: batch loss 0.0005171775701455772\n",
            "Batch 7220 done: batch loss 0.0005094847292639315\n",
            "Batch 7230 done: batch loss 0.0005228603258728981\n",
            "Batch 7240 done: batch loss 0.0005120743298903108\n",
            "Batch 7250 done: batch loss 0.0005236384458839893\n",
            "Batch 7260 done: batch loss 0.0005490684998221695\n",
            "Batch 7270 done: batch loss 0.0004959215293638408\n",
            "Batch 7280 done: batch loss 0.0004939415375702083\n",
            "Batch 7290 done: batch loss 0.0005429763114079833\n",
            "Batch 7300 done: batch loss 0.0005135216051712632\n",
            "Batch 7310 done: batch loss 0.0005061951815150678\n",
            "Batch 7320 done: batch loss 0.0004974553594365716\n",
            "Batch 7330 done: batch loss 0.0005078086978755891\n",
            "Batch 7340 done: batch loss 0.0005149957141838968\n",
            "Batch 7350 done: batch loss 0.0005247867666184902\n",
            "Batch 7360 done: batch loss 0.000534767284989357\n",
            "Batch 7370 done: batch loss 0.0004927431000396609\n",
            "Batch 7380 done: batch loss 0.0005621844320558012\n",
            "Batch 7390 done: batch loss 0.0005032968474552035\n",
            "Batch 7400 done: batch loss 0.0005053973873145878\n",
            "Batch 7410 done: batch loss 0.0005613604444079101\n",
            "Batch 7420 done: batch loss 0.0004996189381927252\n",
            "Batch 7430 done: batch loss 0.0004922992666251957\n",
            "Batch 7440 done: batch loss 0.000542877649422735\n",
            "Batch 7450 done: batch loss 0.00048019131645560265\n",
            "Batch 7460 done: batch loss 0.000543907459359616\n",
            "Batch 7470 done: batch loss 0.0004990403540432453\n",
            "Batch 7480 done: batch loss 0.0005091482307761908\n",
            "Batch 7490 done: batch loss 0.0005333169247023761\n",
            "Batch 7500 done: batch loss 0.0005088592879474163\n",
            "Batch 7510 done: batch loss 0.0005056062946096063\n",
            "Batch 7520 done: batch loss 0.0004924575332552195\n",
            "Batch 7530 done: batch loss 0.0005191810778342187\n",
            "Batch 7540 done: batch loss 0.0005136540275998414\n",
            "Batch 7550 done: batch loss 0.0005379836657084525\n",
            "Batch 7560 done: batch loss 0.0005310848937369883\n",
            "Batch 7570 done: batch loss 0.00048385519767180085\n",
            "Batch 7580 done: batch loss 0.0005412983009591699\n",
            "Batch 7590 done: batch loss 0.0006256799097172916\n",
            "Batch 7600 done: batch loss 0.0005027605802752078\n",
            "Batch 7610 done: batch loss 0.0005053397617302835\n",
            "Batch 7620 done: batch loss 0.0005098649417050183\n",
            "Batch 7630 done: batch loss 0.0005029387539252639\n",
            "Batch 7640 done: batch loss 0.0005276334704831243\n",
            "Batch 7650 done: batch loss 0.0004892065189778805\n",
            "Batch 7660 done: batch loss 0.0005230547394603491\n",
            "Batch 7670 done: batch loss 0.0005117959226481616\n",
            "Batch 7680 done: batch loss 0.0005198167054913938\n",
            "Batch 7690 done: batch loss 0.0005367232370190322\n",
            "Batch 7700 done: batch loss 0.0005396993365138769\n",
            "Batch 7710 done: batch loss 0.0005143288290128112\n",
            "Batch 7720 done: batch loss 0.0005330644198693335\n",
            "Batch 7730 done: batch loss 0.0005451133474707603\n",
            "Batch 7740 done: batch loss 0.0005770358839072287\n",
            "Batch 7750 done: batch loss 0.0005107407341711223\n",
            "Batch 7760 done: batch loss 0.0005333321751095355\n",
            "Batch 7770 done: batch loss 0.0005306220264174044\n",
            "Batch 7780 done: batch loss 0.0005116755492053926\n",
            "Batch 7790 done: batch loss 0.0005005311104469001\n",
            "Batch 7800 done: batch loss 0.0004978497745469213\n",
            "Batch 7810 done: batch loss 0.000515169114805758\n",
            "Batch 7820 done: batch loss 0.0005227730725891888\n",
            "Batch 7830 done: batch loss 0.0005559548153541982\n",
            "Batch 7840 done: batch loss 0.00052990997210145\n",
            "Batch 7850 done: batch loss 0.0005204504705034196\n",
            "Batch 7860 done: batch loss 0.0005306439707055688\n",
            "Batch 7870 done: batch loss 0.0004891087883152068\n",
            "Batch 7880 done: batch loss 0.0005375861655920744\n",
            "Batch 7890 done: batch loss 0.0005370959406718612\n",
            "Batch 7900 done: batch loss 0.0005024934653192759\n",
            "Batch 7910 done: batch loss 0.0005085023585706949\n",
            "Batch 7920 done: batch loss 0.0005014199414290488\n",
            "Batch 7930 done: batch loss 0.000519945693667978\n",
            "Batch 7940 done: batch loss 0.0005223575863055885\n",
            "Batch 7950 done: batch loss 0.0005037817172706127\n",
            "Batch 7960 done: batch loss 0.00048170684021897614\n",
            "Batch 7970 done: batch loss 0.000504985568113625\n",
            "Batch 7980 done: batch loss 0.0005108927725814283\n",
            "Batch 7990 done: batch loss 0.0004961928934790194\n",
            "Batch 8000 done: batch loss 0.0005047398153692484\n",
            "Batch 8010 done: batch loss 0.0005305635859258473\n",
            "Batch 8020 done: batch loss 0.0004853756108786911\n",
            "Batch 8030 done: batch loss 0.0005187821807339787\n",
            "Batch 8040 done: batch loss 0.000562920409720391\n",
            "Batch 8050 done: batch loss 0.00053460116032511\n",
            "Batch 8060 done: batch loss 0.0005472130142152309\n",
            "Batch 8070 done: batch loss 0.0005385841941460967\n",
            "Batch 8080 done: batch loss 0.0004913168959319592\n",
            "Batch 8090 done: batch loss 0.0005014906637370586\n",
            "Batch 8100 done: batch loss 0.0004731958615593612\n",
            "Batch 8110 done: batch loss 0.000513215665705502\n",
            "Batch 8120 done: batch loss 0.0005093757063150406\n",
            "Batch 8130 done: batch loss 0.00048274584696628153\n",
            "Batch 8140 done: batch loss 0.0005161711014807224\n",
            "Batch 8150 done: batch loss 0.0004839376488234848\n",
            "Batch 8160 done: batch loss 0.0005126146716065705\n",
            "Batch 8170 done: batch loss 0.000519216526299715\n",
            "Batch 8180 done: batch loss 0.0005365416291169822\n",
            "Batch 8190 done: batch loss 0.0004763778706546873\n",
            "Batch 8200 done: batch loss 0.000511069200001657\n",
            "Batch 8210 done: batch loss 0.00047625822480767965\n",
            "Batch 8220 done: batch loss 0.0004965101834386587\n",
            "Batch 8230 done: batch loss 0.0005466719157993793\n",
            "Batch 8240 done: batch loss 0.0005062684649601579\n",
            "Batch 8250 done: batch loss 0.0004883997607976198\n",
            "Batch 8260 done: batch loss 0.00048299983609467745\n",
            "Batch 8270 done: batch loss 0.0005028604064136744\n",
            "Batch 8280 done: batch loss 0.0005255180294625461\n",
            "Batch 8290 done: batch loss 0.0005535113159567118\n",
            "Batch 8300 done: batch loss 0.0004959396319463849\n",
            "Batch 8310 done: batch loss 0.0005104546435177326\n",
            "Batch 8320 done: batch loss 0.0005195099511183798\n",
            "Batch 8330 done: batch loss 0.0004941861261613667\n",
            "Batch 8340 done: batch loss 0.0004860218323301524\n",
            "Batch 8350 done: batch loss 0.000571640906855464\n",
            "Batch 8360 done: batch loss 0.00048558865091763437\n",
            "Batch 8370 done: batch loss 0.0004934815224260092\n",
            "Batch 8380 done: batch loss 0.0005338958580978215\n",
            "Batch 8390 done: batch loss 0.0005125772440806031\n",
            "Batch 8400 done: batch loss 0.0004884822992607951\n",
            "Batch 8410 done: batch loss 0.0005094455555081367\n",
            "Batch 8420 done: batch loss 0.0004903233493678272\n",
            "Batch 8430 done: batch loss 0.0005285325460135937\n",
            "Batch 8440 done: batch loss 0.0005095613305456936\n",
            "Batch 8450 done: batch loss 0.0004992655012756586\n",
            "Batch 8460 done: batch loss 0.0004896754981018603\n",
            "Batch 8470 done: batch loss 0.0005088082398287952\n",
            "Batch 8480 done: batch loss 0.0005330567364580929\n",
            "Batch 8490 done: batch loss 0.000525625713635236\n",
            "Batch 8500 done: batch loss 0.0005181093001738191\n",
            "Batch 8510 done: batch loss 0.0004910104908049107\n",
            "Batch 8520 done: batch loss 0.000532311387360096\n",
            "Batch 8530 done: batch loss 0.0005173585959710181\n",
            "Batch 8540 done: batch loss 0.000499362766277045\n",
            "Batch 8550 done: batch loss 0.00048402257380075753\n",
            "Batch 8560 done: batch loss 0.0004924932727590203\n",
            "Batch 8570 done: batch loss 0.0005399901419878006\n",
            "Batch 8580 done: batch loss 0.0004932322772219777\n",
            "Batch 8590 done: batch loss 0.0005251246620900929\n",
            "Batch 8600 done: batch loss 0.0005366719560697675\n",
            "Batch 8610 done: batch loss 0.0005259944591671228\n",
            "Batch 8620 done: batch loss 0.0005253113922663033\n",
            "Batch 8630 done: batch loss 0.0005318120238371193\n",
            "Batch 8640 done: batch loss 0.00048746541142463684\n",
            "Batch 8650 done: batch loss 0.0005043139099143445\n",
            "Batch 8660 done: batch loss 0.0005112753715366125\n",
            "Batch 8670 done: batch loss 0.0004914338933303952\n",
            "Batch 8680 done: batch loss 0.00048721383791416883\n",
            "Batch 8690 done: batch loss 0.0005320695345290005\n",
            "Batch 8700 done: batch loss 0.0005076040979474783\n",
            "Batch 8710 done: batch loss 0.0004850548575632274\n",
            "Batch 8720 done: batch loss 0.0005191059317439795\n",
            "Batch 8730 done: batch loss 0.0004995916970074177\n",
            "Batch 8740 done: batch loss 0.0004963245010003448\n",
            "Batch 8750 done: batch loss 0.0005002328543923795\n",
            "Batch 8760 done: batch loss 0.0005418836954049766\n",
            "Batch 8770 done: batch loss 0.00047499427455477417\n",
            "Batch 8780 done: batch loss 0.0004966803826391697\n",
            "Batch 8790 done: batch loss 0.000512327125761658\n",
            "Batch 8800 done: batch loss 0.0004998476943001151\n",
            "Batch 8810 done: batch loss 0.0005283208447508514\n",
            "Batch 8820 done: batch loss 0.0005223722546361387\n",
            "Batch 8830 done: batch loss 0.0005289954715408385\n",
            "Batch 8840 done: batch loss 0.0004981736419722438\n",
            "Batch 8850 done: batch loss 0.0005180641310289502\n",
            "Batch 8860 done: batch loss 0.0004921715008094907\n",
            "Batch 8870 done: batch loss 0.0005359401693567634\n",
            "Batch 8880 done: batch loss 0.000521212350577116\n",
            "Batch 8890 done: batch loss 0.0005129874916747212\n",
            "Batch 8900 done: batch loss 0.0005322406068444252\n",
            "Batch 8910 done: batch loss 0.0004944432293996215\n",
            "Batch 8920 done: batch loss 0.0005085362354293466\n",
            "Batch 8930 done: batch loss 0.000504824798554182\n",
            "Batch 8940 done: batch loss 0.0005460080574266613\n",
            "Batch 8950 done: batch loss 0.0005089893820695579\n",
            "Batch 8960 done: batch loss 0.0005066939629614353\n",
            "Batch 8970 done: batch loss 0.0004956346238031983\n",
            "Batch 8980 done: batch loss 0.0004694240924436599\n",
            "Batch 8990 done: batch loss 0.0005231898976489902\n",
            "Batch 9000 done: batch loss 0.000491168349981308\n",
            "Batch 9010 done: batch loss 0.0005150260403752327\n",
            "Batch 9020 done: batch loss 0.0005185659974813461\n",
            "Batch 9030 done: batch loss 0.0005061048432253301\n",
            "Batch 9040 done: batch loss 0.0004965835832990706\n",
            "Batch 9050 done: batch loss 0.0005139062996022403\n",
            "Batch 9060 done: batch loss 0.0005329682026058435\n",
            "Batch 9070 done: batch loss 0.0005138475680723786\n",
            "Batch 9080 done: batch loss 0.0004940945655107498\n",
            "Batch 9090 done: batch loss 0.0005104221636429429\n",
            "Batch 9100 done: batch loss 0.000525364768691361\n",
            "Batch 9110 done: batch loss 0.000477796042105183\n",
            "Batch 9120 done: batch loss 0.0005057859816588461\n",
            "Batch 9130 done: batch loss 0.0005165750626474619\n",
            "Batch 9140 done: batch loss 0.0005262104095891118\n",
            "Batch 9150 done: batch loss 0.00047979349619708955\n",
            "Batch 9160 done: batch loss 0.0004927480476908386\n",
            "Batch 9170 done: batch loss 0.000506711658090353\n",
            "Batch 9180 done: batch loss 0.0004954264732077718\n",
            "Batch 9190 done: batch loss 0.0004907947150059044\n",
            "Batch 9200 done: batch loss 0.0005195268895477057\n",
            "Batch 9210 done: batch loss 0.00048644724301993847\n",
            "Batch 9220 done: batch loss 0.0005162556772120297\n",
            "Batch 9230 done: batch loss 0.0005091552156955004\n",
            "Batch 9240 done: batch loss 0.00048315769527107477\n",
            "Batch 9250 done: batch loss 0.0004946767585352063\n",
            "Batch 9260 done: batch loss 0.0005202300380915403\n",
            "Batch 9270 done: batch loss 0.00048812429304234684\n",
            "Batch 9280 done: batch loss 0.00048435869393870234\n",
            "Batch 9290 done: batch loss 0.0005291299894452095\n",
            "Batch 9300 done: batch loss 0.000522859685588628\n",
            "Batch 9310 done: batch loss 0.0004974839393980801\n",
            "Batch 9320 done: batch loss 0.0004867404932156205\n",
            "Batch 9330 done: batch loss 0.00048733671428635716\n",
            "Batch 9340 done: batch loss 0.0005507838795892894\n",
            "Batch 9350 done: batch loss 0.0005407579592429101\n",
            "Batch 9360 done: batch loss 0.0004890756099484861\n",
            "Batch 9370 done: batch loss 0.0005363908130675554\n",
            "Batch 9380 done: batch loss 0.000503748538903892\n",
            "Batch 9390 done: batch loss 0.0004951206501573324\n",
            "Batch 9400 done: batch loss 0.0004955233889631927\n",
            "Batch 9410 done: batch loss 0.0005051622283644974\n",
            "Batch 9420 done: batch loss 0.000483441079268232\n",
            "Batch 9430 done: batch loss 0.00048649494419805706\n",
            "Batch 9440 done: batch loss 0.0005018350784666836\n",
            "Batch 9450 done: batch loss 0.00048162805614992976\n",
            "Batch 9460 done: batch loss 0.0005071297637186944\n",
            "Batch 9470 done: batch loss 0.0004780034360010177\n",
            "Batch 9480 done: batch loss 0.0005117249675095081\n",
            "Batch 9490 done: batch loss 0.0004960051155649126\n",
            "Batch 9500 done: batch loss 0.0005070499028079212\n",
            "Batch 9510 done: batch loss 0.00047750663361512125\n",
            "Batch 9520 done: batch loss 0.0005142554291523993\n",
            "Batch 9530 done: batch loss 0.0004874076403211802\n",
            "Batch 9540 done: batch loss 0.0005225793574936688\n",
            "Batch 9550 done: batch loss 0.0005075386143289506\n",
            "Batch 9560 done: batch loss 0.0004743903409689665\n",
            "Batch 9570 done: batch loss 0.0005136604304425418\n",
            "Batch 9580 done: batch loss 0.0004946634289808571\n",
            "Batch 9590 done: batch loss 0.0004893132136203349\n",
            "Batch 9600 done: batch loss 0.0004962484817951918\n",
            "Batch 9610 done: batch loss 0.00047934046597220004\n",
            "Batch 9620 done: batch loss 0.0004637718084268272\n",
            "Batch 9630 done: batch loss 0.0005366633995436132\n",
            "Batch 9640 done: batch loss 0.0004801395698450506\n",
            "Batch 9650 done: batch loss 0.000504388241097331\n",
            "Batch 9660 done: batch loss 0.0005252619157545269\n",
            "Batch 9670 done: batch loss 0.000517538283020258\n",
            "Batch 9680 done: batch loss 0.0004927237750962377\n",
            "Batch 9690 done: batch loss 0.0005149021162651479\n",
            "Batch 9700 done: batch loss 0.000500724243465811\n",
            "Batch 9710 done: batch loss 0.0005148854688741267\n",
            "Batch 9720 done: batch loss 0.0004872073186561465\n",
            "Batch 9730 done: batch loss 0.00047760122106410563\n",
            "Batch 9740 done: batch loss 0.0005156650440767407\n",
            "Batch 9750 done: batch loss 0.0005188735667616129\n",
            "Batch 9760 done: batch loss 0.0004966029082424939\n",
            "Batch 9770 done: batch loss 0.0004896968021057546\n",
            "Batch 9780 done: batch loss 0.0005090991617180407\n",
            "Batch 9790 done: batch loss 0.0004930201102979481\n",
            "Batch 9800 done: batch loss 0.0005047701415605843\n",
            "Batch 9810 done: batch loss 0.0004776975838467479\n",
            "Batch 9820 done: batch loss 0.000511725025717169\n",
            "Batch 9830 done: batch loss 0.000499952700920403\n",
            "Batch 9840 done: batch loss 0.0005087162717245519\n",
            "Batch 9850 done: batch loss 0.000522900721989572\n",
            "Batch 9860 done: batch loss 0.0004925125977024436\n",
            "Batch 9870 done: batch loss 0.0005237142322584987\n",
            "Batch 9880 done: batch loss 0.0005293258582241833\n",
            "Batch 9890 done: batch loss 0.000517050561029464\n",
            "Batch 9900 done: batch loss 0.0005291436100378633\n",
            "Batch 9910 done: batch loss 0.0005272207199595869\n",
            "Batch 9920 done: batch loss 0.000492725579533726\n",
            "Batch 9930 done: batch loss 0.0005020691896788776\n",
            "Batch 9940 done: batch loss 0.0005023099947720766\n",
            "Batch 9950 done: batch loss 0.0005099726258777082\n",
            "Batch 9960 done: batch loss 0.0005384984542615712\n",
            "Batch 9970 done: batch loss 0.0005149707430973649\n",
            "Batch 9980 done: batch loss 0.0004751073138322681\n",
            "Batch 9990 done: batch loss 0.0005259375320747495\n",
            "Batch 10000 done: batch loss 0.0005703143542632461\n",
            "Batch 10010 done: batch loss 0.000489919213578105\n",
            "Batch 10020 done: batch loss 0.00048082147259265184\n",
            "Batch 10030 done: batch loss 0.0005045606521889567\n",
            "Batch 10040 done: batch loss 0.0005349799757823348\n",
            "Batch 10050 done: batch loss 0.00048341360525228083\n",
            "Batch 10060 done: batch loss 0.0005241917679086328\n",
            "Batch 10070 done: batch loss 0.000522416434250772\n",
            "Batch 10080 done: batch loss 0.000503928866237402\n",
            "Batch 10090 done: batch loss 0.0005601273733191192\n",
            "Batch 10100 done: batch loss 0.0004991124151274562\n",
            "Batch 10110 done: batch loss 0.0005335949244908988\n",
            "Batch 10120 done: batch loss 0.0005163332098163664\n",
            "Batch 10130 done: batch loss 0.0005174356047064066\n",
            "Batch 10140 done: batch loss 0.00047926895786076784\n",
            "Batch 10150 done: batch loss 0.0005151903606019914\n",
            "Batch 10160 done: batch loss 0.0005150781944394112\n",
            "Batch 10170 done: batch loss 0.0004664305306505412\n",
            "Batch 10180 done: batch loss 0.0005024197744205594\n",
            "Batch 10190 done: batch loss 0.0004963112296536565\n",
            "Batch 10200 done: batch loss 0.000496255059260875\n",
            "Batch 10210 done: batch loss 0.000492515682708472\n",
            "Batch 10220 done: batch loss 0.0005066166631877422\n",
            "Batch 10230 done: batch loss 0.00048756966134533286\n",
            "Batch 10240 done: batch loss 0.0005255537107586861\n",
            "Batch 10250 done: batch loss 0.0004969025612808764\n",
            "Batch 10260 done: batch loss 0.00047074988833628595\n",
            "Batch 10270 done: batch loss 0.00046330393524840474\n",
            "Batch 10280 done: batch loss 0.00048494923976249993\n",
            "Batch 10290 done: batch loss 0.0004780542221851647\n",
            "Batch 10300 done: batch loss 0.0005016186041757464\n",
            "Batch 10310 done: batch loss 0.0005424065748229623\n",
            "Batch 10320 done: batch loss 0.0005097472458146513\n",
            "Batch 10330 done: batch loss 0.0005187919014133513\n",
            "Batch 10340 done: batch loss 0.0004908532719127834\n",
            "Batch 10350 done: batch loss 0.00047849645488895476\n",
            "Batch 10360 done: batch loss 0.0004893012228421867\n",
            "Batch 10370 done: batch loss 0.0005232802359387279\n",
            "Batch 10380 done: batch loss 0.0005317255272530019\n",
            "Batch 10390 done: batch loss 0.0004933696473017335\n",
            "Batch 10400 done: batch loss 0.000493658590130508\n",
            "Batch 10410 done: batch loss 0.0005421154201030731\n",
            "Batch 10420 done: batch loss 0.00046860898146405816\n",
            "Batch 10430 done: batch loss 0.0004921934450976551\n",
            "Batch 10440 done: batch loss 0.0004892900469712913\n",
            "Batch 10450 done: batch loss 0.0004995045601390302\n",
            "Batch 10460 done: batch loss 0.0005203387117944658\n",
            "Batch 10470 done: batch loss 0.0004972248570993543\n",
            "Batch 10480 done: batch loss 0.0004977034404873848\n",
            "Batch 10490 done: batch loss 0.0004854529688600451\n",
            "Batch 10500 done: batch loss 0.0004998474614694715\n",
            "Batch 10510 done: batch loss 0.00047724763862788677\n",
            "Batch 10520 done: batch loss 0.0004885720554739237\n",
            "Batch 10530 done: batch loss 0.000495722284540534\n",
            "Batch 10540 done: batch loss 0.0004962087259627879\n",
            "Batch 10550 done: batch loss 0.0005151672521606088\n",
            "Batch 10560 done: batch loss 0.0004856965970247984\n",
            "Batch 10570 done: batch loss 0.0004720040305983275\n",
            "Batch 10580 done: batch loss 0.00047775209532119334\n",
            "Batch 10590 done: batch loss 0.0005040161195211112\n",
            "Batch 10600 done: batch loss 0.00045827761641703546\n",
            "Batch 10610 done: batch loss 0.000478084955830127\n",
            "Batch 10620 done: batch loss 0.0005200375453568995\n",
            "Batch 10630 done: batch loss 0.00048721031635068357\n",
            "Batch 10640 done: batch loss 0.0004881244385614991\n",
            "Batch 10650 done: batch loss 0.0005271610571071506\n",
            "Batch 10660 done: batch loss 0.0005046352744102478\n",
            "Batch 10670 done: batch loss 0.0005314888549037278\n",
            "Batch 10680 done: batch loss 0.0005452024633996189\n",
            "Batch 10690 done: batch loss 0.0005022388068027794\n",
            "Batch 10700 done: batch loss 0.0005467098671942949\n",
            "Batch 10710 done: batch loss 0.0005445390706881881\n",
            "Batch 10720 done: batch loss 0.0005034690257161856\n",
            "Batch 10730 done: batch loss 0.0005414317129179835\n",
            "Batch 10740 done: batch loss 0.0005175004480406642\n",
            "Batch 10750 done: batch loss 0.0005602380842901766\n",
            "Batch 10760 done: batch loss 0.000500911963172257\n",
            "Batch 10770 done: batch loss 0.0005160527653060853\n",
            "Batch 10780 done: batch loss 0.00048384148976765573\n",
            "Batch 10790 done: batch loss 0.0005344846867956221\n",
            "Batch 10800 done: batch loss 0.0005083177238702774\n",
            "Batch 10810 done: batch loss 0.0005124204908497632\n",
            "Batch 10820 done: batch loss 0.0005160575965419412\n",
            "Batch 10830 done: batch loss 0.0005135164828971028\n",
            "Batch 10840 done: batch loss 0.0004895674646832049\n",
            "Batch 10850 done: batch loss 0.0005219466402195394\n",
            "Batch 10860 done: batch loss 0.000510656856931746\n",
            "Batch 10870 done: batch loss 0.00047187553718686104\n",
            "Batch 10880 done: batch loss 0.0004969322471879423\n",
            "Batch 10890 done: batch loss 0.0004796396824531257\n",
            "Batch 10900 done: batch loss 0.0005126917967572808\n",
            "Batch 10910 done: batch loss 0.0004571672761812806\n",
            "Batch 10920 done: batch loss 0.0004810405080206692\n",
            "Batch 10930 done: batch loss 0.0004937884514220059\n",
            "Batch 10940 done: batch loss 0.0004941134247928858\n",
            "Batch 10950 done: batch loss 0.0004907138645648956\n",
            "Batch 10960 done: batch loss 0.0004901834763586521\n",
            "Batch 10970 done: batch loss 0.000539549917448312\n",
            "Batch 10980 done: batch loss 0.0005220375605858862\n",
            "Batch 10990 done: batch loss 0.0005051617044955492\n",
            "Batch 11000 done: batch loss 0.00047822101623751223\n",
            "Batch 11010 done: batch loss 0.0005052126361988485\n",
            "Batch 11020 done: batch loss 0.00048693246208131313\n",
            "Batch 11030 done: batch loss 0.0005247729714028537\n",
            "Batch 11040 done: batch loss 0.0005226089269854128\n",
            "Batch 11050 done: batch loss 0.0005264148931019008\n",
            "Batch 11060 done: batch loss 0.00046416177065111697\n",
            "Batch 11070 done: batch loss 0.00047384481877088547\n",
            "Batch 11080 done: batch loss 0.0005091833882033825\n",
            "Batch 11090 done: batch loss 0.0004668524779845029\n",
            "Batch 11100 done: batch loss 0.0005309405387379229\n",
            "Batch 11110 done: batch loss 0.0004921626532450318\n",
            "Batch 11120 done: batch loss 0.0005199817242100835\n",
            "Batch 11130 done: batch loss 0.0005062876152805984\n",
            "Batch 11140 done: batch loss 0.0004774104163516313\n",
            "Batch 11150 done: batch loss 0.00047341157915070653\n",
            "Batch 11160 done: batch loss 0.00046688810107298195\n",
            "Batch 11170 done: batch loss 0.0004804686177521944\n",
            "Batch 11180 done: batch loss 0.0005094318767078221\n",
            "Batch 11190 done: batch loss 0.0004831142723560333\n",
            "Batch 11200 done: batch loss 0.0004936624318361282\n",
            "Batch 11210 done: batch loss 0.0005069561884738505\n",
            "Batch 11220 done: batch loss 0.0004891359712928534\n",
            "Batch 11230 done: batch loss 0.0004976968048140407\n",
            "Batch 11240 done: batch loss 0.0005007072468288243\n",
            "Batch 11250 done: batch loss 0.0004656788660213351\n",
            "Batch 11260 done: batch loss 0.0005106836324557662\n",
            "Batch 11270 done: batch loss 0.0004900525091215968\n",
            "Batch 11280 done: batch loss 0.0005531521164812148\n",
            "Batch 11290 done: batch loss 0.0005044998833909631\n",
            "Batch 11300 done: batch loss 0.0004619884421117604\n",
            "Batch 11310 done: batch loss 0.00048738293116912246\n",
            "Batch 11320 done: batch loss 0.0005008734879083931\n",
            "Batch 11330 done: batch loss 0.00048606839845888317\n",
            "Batch 11340 done: batch loss 0.0005001720855943859\n",
            "Batch 11350 done: batch loss 0.0004651024646591395\n",
            "Batch 11360 done: batch loss 0.000511059770360589\n",
            "Batch 11370 done: batch loss 0.0005058884271420538\n",
            "Batch 11380 done: batch loss 0.00048716639867052436\n",
            "Batch 11390 done: batch loss 0.00048182258615270257\n",
            "Batch 11400 done: batch loss 0.0005028834566473961\n",
            "Batch 11410 done: batch loss 0.0005496876547113061\n",
            "Batch 11420 done: batch loss 0.00046219132491387427\n",
            "Batch 11430 done: batch loss 0.0005049929022789001\n",
            "Batch 11440 done: batch loss 0.0005097441026009619\n",
            "Batch 11450 done: batch loss 0.0005113024380989373\n",
            "Batch 11460 done: batch loss 0.0004836202133446932\n",
            "Batch 11470 done: batch loss 0.0004960340447723866\n",
            "Batch 11480 done: batch loss 0.0004649071197491139\n",
            "Batch 11490 done: batch loss 0.0005139459972269833\n",
            "Batch 11500 done: batch loss 0.0004979189834557474\n",
            "Batch 11510 done: batch loss 0.0004988768487237394\n",
            "Batch 11520 done: batch loss 0.0004938851343467832\n",
            "Batch 11530 done: batch loss 0.0004985210252925754\n",
            "Batch 11540 done: batch loss 0.0005025465507060289\n",
            "Batch 11550 done: batch loss 0.0005290420958772302\n",
            "Batch 11560 done: batch loss 0.0005136464023962617\n",
            "Batch 11570 done: batch loss 0.00048152232193388045\n",
            "Batch 11580 done: batch loss 0.0005017784424126148\n",
            "Batch 11590 done: batch loss 0.0005010949680581689\n",
            "Batch 11600 done: batch loss 0.000483209325466305\n",
            "Batch 11610 done: batch loss 0.0004773468244820833\n",
            "Batch 11620 done: batch loss 0.0004843014176003635\n",
            "Batch 11630 done: batch loss 0.0005877491785213351\n",
            "Batch 11640 done: batch loss 0.00048408337170258164\n",
            "Batch 11650 done: batch loss 0.0004694296221714467\n",
            "Batch 11660 done: batch loss 0.000481466471683234\n",
            "Batch 11670 done: batch loss 0.000492175982799381\n",
            "Batch 11680 done: batch loss 0.000494323845487088\n",
            "Batch 11690 done: batch loss 0.0005248934030532837\n",
            "Batch 11700 done: batch loss 0.0004977878998033702\n",
            "Batch 11710 done: batch loss 0.0004973292234353721\n",
            "Batch 11720 done: batch loss 0.0004949065041728318\n",
            "Batch 11730 done: batch loss 0.0004952886956743896\n",
            "Batch 11740 done: batch loss 0.0005055453511886299\n",
            "Batch 11750 done: batch loss 0.0005238045123405755\n",
            "Batch 11760 done: batch loss 0.000506347743794322\n",
            "Batch 11770 done: batch loss 0.0005247722147032619\n",
            "Batch 11780 done: batch loss 0.0004938740166835487\n",
            "Batch 11790 done: batch loss 0.0005324764060787857\n",
            "Batch 11800 done: batch loss 0.0005321500357240438\n",
            "Batch 11810 done: batch loss 0.00048706846428103745\n",
            "Batch 11820 done: batch loss 0.0004933407180942595\n",
            "Batch 11830 done: batch loss 0.0005110601196065545\n",
            "Batch 11840 done: batch loss 0.00045901472913101315\n",
            "Batch 11850 done: batch loss 0.000504808675032109\n",
            "Batch 11860 done: batch loss 0.0004622342821676284\n",
            "Batch 11870 done: batch loss 0.0004957802593708038\n",
            "Batch 11880 done: batch loss 0.0005092688952572644\n",
            "Batch 11890 done: batch loss 0.0004862152854911983\n",
            "Batch 11900 done: batch loss 0.0004949960275553167\n",
            "Batch 11910 done: batch loss 0.0004941506194882095\n",
            "Batch 11920 done: batch loss 0.0004898227634839714\n",
            "Batch 11930 done: batch loss 0.0004972036695107818\n",
            "Batch 11940 done: batch loss 0.0004736171686090529\n",
            "Batch 11950 done: batch loss 0.0005495457444339991\n",
            "Batch 11960 done: batch loss 0.0005366932018660009\n",
            "Batch 11970 done: batch loss 0.0004883932997472584\n",
            "Batch 11980 done: batch loss 0.0004759417788591236\n",
            "Batch 11990 done: batch loss 0.0005304431542754173\n",
            "Batch 12000 done: batch loss 0.0004891917924396694\n",
            "Batch 12010 done: batch loss 0.0004794824053533375\n",
            "Batch 12020 done: batch loss 0.0005165039910934865\n",
            "Batch 12030 done: batch loss 0.0004975591436959803\n",
            "Batch 12040 done: batch loss 0.0004907962866127491\n",
            "Batch 12050 done: batch loss 0.00046139140613377094\n",
            "Batch 12060 done: batch loss 0.0004795640707015991\n",
            "Batch 12070 done: batch loss 0.0004562485555652529\n",
            "Batch 12080 done: batch loss 0.0004989280132576823\n",
            "Batch 12090 done: batch loss 0.0004810845130123198\n",
            "Batch 12100 done: batch loss 0.0005238406592980027\n",
            "Batch 12110 done: batch loss 0.00046639982610940933\n",
            "Batch 12120 done: batch loss 0.0005149038042873144\n",
            "Batch 12130 done: batch loss 0.0004975044284947217\n",
            "Batch 12140 done: batch loss 0.00048333368613384664\n",
            "Batch 12150 done: batch loss 0.0004651377385016531\n",
            "Batch 12160 done: batch loss 0.00045642288750968874\n",
            "Batch 12170 done: batch loss 0.0004799020243808627\n",
            "Batch 12180 done: batch loss 0.0004794660781044513\n",
            "Batch 12190 done: batch loss 0.0004605516733136028\n",
            "Batch 12200 done: batch loss 0.0005113985389471054\n",
            "Batch 12210 done: batch loss 0.00047559288213960826\n",
            "Batch 12220 done: batch loss 0.0005235975258983672\n",
            "Batch 12230 done: batch loss 0.0005228297668509185\n",
            "Batch 12240 done: batch loss 0.0005349370767362416\n",
            "Batch 12250 done: batch loss 0.0005159706925041974\n",
            "Batch 12260 done: batch loss 0.000530884659383446\n",
            "Batch 12270 done: batch loss 0.0005265572108328342\n",
            "Batch 12280 done: batch loss 0.000513771316036582\n",
            "Batch 12290 done: batch loss 0.0005033724009990692\n",
            "Batch 12300 done: batch loss 0.0005001492099836469\n",
            "Batch 12310 done: batch loss 0.0004782534670084715\n",
            "Batch 12320 done: batch loss 0.0005048004095442593\n",
            "Batch 12330 done: batch loss 0.0004679822304751724\n",
            "Batch 12340 done: batch loss 0.0004792103136423975\n",
            "Batch 12350 done: batch loss 0.00048096326645463705\n",
            "Batch 12360 done: batch loss 0.00052732287440449\n",
            "Batch 12370 done: batch loss 0.00047574687050655484\n",
            "Batch 12380 done: batch loss 0.0005040406249463558\n",
            "Batch 12390 done: batch loss 0.0004614504869095981\n",
            "Batch 12400 done: batch loss 0.0004994678893126547\n",
            "Batch 12410 done: batch loss 0.0004864961083512753\n",
            "Batch 12420 done: batch loss 0.0004928342532366514\n",
            "Batch 12430 done: batch loss 0.0005210005328990519\n",
            "Batch 12440 done: batch loss 0.0004657208628486842\n",
            "Batch 12450 done: batch loss 0.0004889308474957943\n",
            "Batch 12460 done: batch loss 0.0005287106614559889\n",
            "Batch 12470 done: batch loss 0.0005002333200536668\n",
            "Batch 12480 done: batch loss 0.0005052361520938575\n",
            "Batch 12490 done: batch loss 0.00046962930355221033\n",
            "Batch 12500 done: batch loss 0.0004966779379174113\n",
            "Batch 12510 done: batch loss 0.0004836002190131694\n",
            "Batch 12520 done: batch loss 0.000507157645188272\n",
            "Batch 12530 done: batch loss 0.0004511391744017601\n",
            "Batch 12540 done: batch loss 0.0004940590006299317\n",
            "Batch 12550 done: batch loss 0.000505437608808279\n",
            "Batch 12560 done: batch loss 0.0005061678821220994\n",
            "Batch 12570 done: batch loss 0.0005096160457469523\n",
            "Batch 12580 done: batch loss 0.0005109013873152435\n",
            "Batch 12590 done: batch loss 0.0005030892789363861\n",
            "Batch 12600 done: batch loss 0.0004674400552175939\n",
            "Batch 12610 done: batch loss 0.0004972518072463572\n",
            "Batch 12620 done: batch loss 0.0004918622435070574\n",
            "Batch 12630 done: batch loss 0.0005074864602647722\n",
            "Batch 12640 done: batch loss 0.0004935383913107216\n",
            "Batch 12650 done: batch loss 0.0004572478646878153\n",
            "Batch 12660 done: batch loss 0.0004664042207878083\n",
            "Batch 12670 done: batch loss 0.0004988783621229231\n",
            "Batch 12680 done: batch loss 0.0004870622942689806\n",
            "Batch 12690 done: batch loss 0.0004706496838480234\n",
            "Batch 12700 done: batch loss 0.0005115116364322603\n",
            "Batch 12710 done: batch loss 0.0005035563372075558\n",
            "Batch 12720 done: batch loss 0.0005400748923420906\n",
            "Batch 12730 done: batch loss 0.0004764657642226666\n",
            "Batch 12740 done: batch loss 0.00045569162466563284\n",
            "Batch 12750 done: batch loss 0.00048558844719082117\n",
            "Batch 12760 done: batch loss 0.0004940768121741712\n",
            "Batch 12770 done: batch loss 0.0005035843933001161\n",
            "Batch 12780 done: batch loss 0.0004801782197318971\n",
            "Batch 12790 done: batch loss 0.0005048351013101637\n",
            "Batch 12800 done: batch loss 0.0005284909857437015\n",
            "Batch 12810 done: batch loss 0.0005098519613966346\n",
            "Batch 12820 done: batch loss 0.0004880602064076811\n",
            "Batch 12830 done: batch loss 0.0005474112113006413\n",
            "Batch 12840 done: batch loss 0.00047922617522999644\n",
            "Batch 12850 done: batch loss 0.000502815586514771\n",
            "Batch 12860 done: batch loss 0.0005050247418694198\n",
            "Batch 12870 done: batch loss 0.00046276848297566175\n",
            "Batch 12880 done: batch loss 0.00045732964645139873\n",
            "Batch 12890 done: batch loss 0.0004890825366601348\n",
            "Batch 12900 done: batch loss 0.000553024816326797\n",
            "Batch 12910 done: batch loss 0.00047045055544003844\n",
            "Batch 12920 done: batch loss 0.0005104914307594299\n",
            "Batch 12930 done: batch loss 0.000512752216309309\n",
            "Batch 12940 done: batch loss 0.0004822721821255982\n",
            "Batch 12950 done: batch loss 0.0005006439751014113\n",
            "Batch 12960 done: batch loss 0.00047147853183560073\n",
            "Batch 12970 done: batch loss 0.00046000618021935225\n",
            "Batch 12980 done: batch loss 0.0005159006104804575\n",
            "Batch 12990 done: batch loss 0.0004902755608782172\n",
            "Batch 13000 done: batch loss 0.0004959214711561799\n",
            "Batch 13010 done: batch loss 0.0004800090391654521\n",
            "Batch 13020 done: batch loss 0.0005128697957843542\n",
            "Batch 13030 done: batch loss 0.0005417492939159274\n",
            "Batch 13040 done: batch loss 0.0005167756462469697\n",
            "Batch 13050 done: batch loss 0.000478004920296371\n",
            "Batch 13060 done: batch loss 0.000481344701256603\n",
            "Batch 13070 done: batch loss 0.0005193485412746668\n",
            "Batch 13080 done: batch loss 0.0005115533131174743\n",
            "Batch 13090 done: batch loss 0.0005056245718151331\n",
            "Batch 13100 done: batch loss 0.0005021048127673566\n",
            "Batch 13110 done: batch loss 0.0004919386701658368\n",
            "Batch 13120 done: batch loss 0.0005218209116719663\n",
            "Batch 13130 done: batch loss 0.0005043630953878164\n",
            "Batch 13140 done: batch loss 0.0004811756662093103\n",
            "Batch 13150 done: batch loss 0.000475112086860463\n",
            "Batch 13160 done: batch loss 0.0004781179304700345\n",
            "Batch 13170 done: batch loss 0.0004811388498637825\n",
            "Batch 13180 done: batch loss 0.00048641866305842996\n",
            "Batch 13190 done: batch loss 0.0004724726895801723\n",
            "Batch 13200 done: batch loss 0.0004637886304408312\n",
            "Batch 13210 done: batch loss 0.00047573470510542393\n",
            "Batch 13220 done: batch loss 0.00048390464507974684\n",
            "Batch 13230 done: batch loss 0.0004642524290829897\n",
            "Batch 13240 done: batch loss 0.0004979444784112275\n",
            "Batch 13250 done: batch loss 0.00046057626605033875\n",
            "Batch 13260 done: batch loss 0.00046895365812815726\n",
            "Batch 13270 done: batch loss 0.0004848487442359328\n",
            "Batch 13280 done: batch loss 0.000508090655785054\n",
            "Batch 13290 done: batch loss 0.00046804058365523815\n",
            "Batch 13300 done: batch loss 0.00048687003436498344\n",
            "Batch 13310 done: batch loss 0.00048098183469846845\n",
            "Batch 13320 done: batch loss 0.0005281355697661638\n",
            "Batch 13330 done: batch loss 0.0005013435147702694\n",
            "Batch 13340 done: batch loss 0.0005453047342598438\n",
            "Batch 13350 done: batch loss 0.0004592064942698926\n",
            "Batch 13360 done: batch loss 0.0005000732489861548\n",
            "Batch 13370 done: batch loss 0.0005564315360970795\n",
            "Batch 13380 done: batch loss 0.00046788802137598395\n",
            "Batch 13390 done: batch loss 0.0004965354455634952\n",
            "Batch 13400 done: batch loss 0.00046814599772915244\n",
            "Batch 13410 done: batch loss 0.0005040767136961222\n",
            "Batch 13420 done: batch loss 0.0004908633418381214\n",
            "Batch 13430 done: batch loss 0.00045625248458236456\n",
            "Batch 13440 done: batch loss 0.00048204863560386\n",
            "Batch 13450 done: batch loss 0.0005047991871833801\n",
            "Batch 13460 done: batch loss 0.0005136351101100445\n",
            "Batch 13470 done: batch loss 0.00046166242100298405\n",
            "Batch 13480 done: batch loss 0.0004814717685803771\n",
            "Batch 13490 done: batch loss 0.0004942940431647003\n",
            "Batch 13500 done: batch loss 0.0005168133648112416\n",
            "Batch 13510 done: batch loss 0.0004750365042127669\n",
            "Batch 13520 done: batch loss 0.0004909832496196032\n",
            "Batch 13530 done: batch loss 0.0005097337416373193\n",
            "Batch 13540 done: batch loss 0.0005142797599546611\n",
            "Batch 13550 done: batch loss 0.00048802027595229447\n",
            "Batch 13560 done: batch loss 0.0004892739234492183\n",
            "Batch 13570 done: batch loss 0.00047679204726591706\n",
            "Batch 13580 done: batch loss 0.00048763855011202395\n",
            "Batch 13590 done: batch loss 0.0004551016609184444\n",
            "Batch 13600 done: batch loss 0.0005026836297474802\n",
            "Batch 13610 done: batch loss 0.00048166123451665044\n",
            "Batch 13620 done: batch loss 0.0005090134218335152\n",
            "Batch 13630 done: batch loss 0.0004706875770352781\n",
            "Batch 13640 done: batch loss 0.000496221415232867\n",
            "Batch 13650 done: batch loss 0.0005240740138106048\n",
            "Batch 13660 done: batch loss 0.0005699520697817206\n",
            "Batch 13670 done: batch loss 0.00048085785238072276\n",
            "Batch 13680 done: batch loss 0.00048354800674133003\n",
            "Batch 13690 done: batch loss 0.0005324516678228974\n",
            "Batch 13700 done: batch loss 0.0004823625786229968\n",
            "Batch 13710 done: batch loss 0.0004742775927297771\n",
            "Batch 13720 done: batch loss 0.0004926849505864084\n",
            "Batch 13730 done: batch loss 0.0004820493049919605\n",
            "Batch 13740 done: batch loss 0.0004825982032343745\n",
            "Batch 13750 done: batch loss 0.0005165527109056711\n",
            "Batch 13760 done: batch loss 0.0004863405192736536\n",
            "Batch 13770 done: batch loss 0.0005176624399609864\n",
            "Batch 13780 done: batch loss 0.0005010669119656086\n",
            "Batch 13790 done: batch loss 0.0005180559819564223\n",
            "Batch 13800 done: batch loss 0.0005017152871005237\n",
            "Batch 13810 done: batch loss 0.0005219726008363068\n",
            "Batch 13820 done: batch loss 0.0005368012352846563\n",
            "Batch 13830 done: batch loss 0.0004651383787859231\n",
            "Batch 13840 done: batch loss 0.0004573051701299846\n",
            "Batch 13850 done: batch loss 0.0004891033750027418\n",
            "Batch 13860 done: batch loss 0.000464331591501832\n",
            "Batch 13870 done: batch loss 0.0004854366707149893\n",
            "Batch 13880 done: batch loss 0.0005261301412247121\n",
            "Batch 13890 done: batch loss 0.0004765038611367345\n",
            "Batch 13900 done: batch loss 0.0004710064094979316\n",
            "Batch 13910 done: batch loss 0.00047850198461674154\n",
            "Batch 13920 done: batch loss 0.000528401171322912\n",
            "Batch 13930 done: batch loss 0.0004695737734436989\n",
            "Batch 13940 done: batch loss 0.00046322151320055127\n",
            "Batch 13950 done: batch loss 0.0004996575298719108\n",
            "Batch 13960 done: batch loss 0.0005131220677867532\n",
            "Batch 13970 done: batch loss 0.0004650623013731092\n",
            "Batch 13980 done: batch loss 0.00048434152267873287\n",
            "Batch 13990 done: batch loss 0.0004823608906008303\n",
            "Batch 14000 done: batch loss 0.0004820792528335005\n",
            "Batch 14010 done: batch loss 0.0004665080632548779\n",
            "Batch 14020 done: batch loss 0.000473441876238212\n",
            "Batch 14030 done: batch loss 0.000500463240314275\n",
            "Batch 14040 done: batch loss 0.0005156875704415143\n",
            "Batch 14050 done: batch loss 0.0004873996367678046\n",
            "Batch 14060 done: batch loss 0.0004782240139320493\n",
            "Batch 14070 done: batch loss 0.0004611083131749183\n",
            "Batch 14080 done: batch loss 0.0005102585419081151\n",
            "Batch 14090 done: batch loss 0.0005395147018134594\n",
            "Batch 14100 done: batch loss 0.0004717535339295864\n",
            "Batch 14110 done: batch loss 0.000521003850735724\n",
            "Batch 14120 done: batch loss 0.0005013507907278836\n",
            "Batch 14130 done: batch loss 0.0004603914567269385\n",
            "Batch 14140 done: batch loss 0.0005119693814776838\n",
            "Batch 14150 done: batch loss 0.0005360402283258736\n",
            "Batch 14160 done: batch loss 0.0005209179944358766\n",
            "Batch 14170 done: batch loss 0.000491141399834305\n",
            "Batch 14180 done: batch loss 0.0004908698028884828\n",
            "Batch 14190 done: batch loss 0.000532708247192204\n",
            "Batch 14200 done: batch loss 0.0005259839235804975\n",
            "Batch 14210 done: batch loss 0.0004704727907665074\n",
            "Batch 14220 done: batch loss 0.0004891963326372206\n",
            "Batch 14230 done: batch loss 0.00046586766256950796\n",
            "Batch 14240 done: batch loss 0.0005001891986466944\n",
            "Batch 14250 done: batch loss 0.0004979404620826244\n",
            "Batch 14260 done: batch loss 0.0005093687796033919\n",
            "Batch 14270 done: batch loss 0.00048821416567079723\n",
            "Batch 14280 done: batch loss 0.0005063815042376518\n",
            "Batch 14290 done: batch loss 0.0005012581241317093\n",
            "Batch 14300 done: batch loss 0.0005152321537025273\n",
            "Batch 14310 done: batch loss 0.0005045378929935396\n",
            "Batch 14320 done: batch loss 0.0005044742720201612\n",
            "Batch 14330 done: batch loss 0.0004508721176534891\n",
            "Batch 14340 done: batch loss 0.0004639684339053929\n",
            "Batch 14350 done: batch loss 0.00047149640158750117\n",
            "Batch 14360 done: batch loss 0.0005305781378410757\n",
            "Batch 14370 done: batch loss 0.0005657997098751366\n",
            "Batch 14380 done: batch loss 0.0004566515563055873\n",
            "Batch 14390 done: batch loss 0.0004914795281365514\n",
            "Batch 14400 done: batch loss 0.0004680814454331994\n",
            "Batch 14410 done: batch loss 0.0004884744412265718\n",
            "Batch 14420 done: batch loss 0.0005120964488014579\n",
            "Batch 14430 done: batch loss 0.0004957522032782435\n",
            "Batch 14440 done: batch loss 0.0004959997022524476\n",
            "Batch 14450 done: batch loss 0.0005152529920451343\n",
            "Batch 14460 done: batch loss 0.0004843315982725471\n",
            "Batch 14470 done: batch loss 0.0005167029448784888\n",
            "Batch 14480 done: batch loss 0.0004890858544968069\n",
            "Batch 14490 done: batch loss 0.000539539847522974\n",
            "Batch 14500 done: batch loss 0.000478887464851141\n",
            "Batch 14510 done: batch loss 0.0004785842029377818\n",
            "Batch 14520 done: batch loss 0.0005011850735172629\n",
            "Batch 14530 done: batch loss 0.00048707087989896536\n",
            "Batch 14540 done: batch loss 0.0004786686913575977\n",
            "Batch 14550 done: batch loss 0.00048232232802547514\n",
            "Batch 14560 done: batch loss 0.0004946483531966805\n",
            "Batch 14570 done: batch loss 0.00046824952005408704\n",
            "Batch 14580 done: batch loss 0.000529684592038393\n",
            "Batch 14590 done: batch loss 0.0005334348534233868\n",
            "Batch 14600 done: batch loss 0.00048648074152879417\n",
            "Batch 14610 done: batch loss 0.0005219703889451921\n",
            "Batch 14620 done: batch loss 0.0004648663161788136\n",
            "Batch 14630 done: batch loss 0.0004667936300393194\n",
            "Batch 14640 done: batch loss 0.0004803757765330374\n",
            "Batch 14650 done: batch loss 0.0005240248865447938\n",
            "Batch 14660 done: batch loss 0.0005016957875341177\n",
            "Batch 14670 done: batch loss 0.0004850150435231626\n",
            "Batch 14680 done: batch loss 0.0005561054567806423\n",
            "Batch 14690 done: batch loss 0.0004551016027107835\n",
            "Batch 14700 done: batch loss 0.0005006407736800611\n",
            "Batch 14710 done: batch loss 0.0004896171158179641\n",
            "Batch 14720 done: batch loss 0.00045891443733125925\n",
            "Batch 14730 done: batch loss 0.0005040414980612695\n",
            "Batch 14740 done: batch loss 0.0004875415761489421\n",
            "Batch 14750 done: batch loss 0.0004843662027269602\n",
            "Batch 14760 done: batch loss 0.00048569179489277303\n",
            "Batch 14770 done: batch loss 0.0004969086730852723\n",
            "Batch 14780 done: batch loss 0.0004598865343723446\n",
            "Batch 14790 done: batch loss 0.0004816165892407298\n",
            "Batch 14800 done: batch loss 0.0004958717618137598\n",
            "Batch 14810 done: batch loss 0.00047137815272435546\n",
            "Batch 14820 done: batch loss 0.0005071513587608933\n",
            "Batch 14830 done: batch loss 0.0005275308503769338\n",
            "Batch 14840 done: batch loss 0.0004747023631352931\n",
            "Batch 14850 done: batch loss 0.0005270930123515427\n",
            "Batch 14860 done: batch loss 0.000503424322232604\n",
            "Batch 14870 done: batch loss 0.0004474935994949192\n",
            "Batch 14880 done: batch loss 0.0005178007995709777\n",
            "Batch 14890 done: batch loss 0.00048274846631102264\n",
            "Batch 14900 done: batch loss 0.0004818688612431288\n",
            "Batch 14910 done: batch loss 0.0004941985826008022\n",
            "Batch 14920 done: batch loss 0.00045609509106725454\n",
            "Batch 14930 done: batch loss 0.0004824667703360319\n",
            "Batch 14940 done: batch loss 0.0004953645402565598\n",
            "Batch 14950 done: batch loss 0.0004975278279744089\n",
            "Batch 14960 done: batch loss 0.00046834509703330696\n",
            "Batch 14970 done: batch loss 0.0004981076344847679\n",
            "Batch 14980 done: batch loss 0.0004901423235423863\n",
            "Batch 14990 done: batch loss 0.00049471395323053\n",
            "Batch 15000 done: batch loss 0.0004875526065006852\n",
            "Batch 15010 done: batch loss 0.00048069877084344625\n",
            "Batch 15020 done: batch loss 0.0004673252406064421\n",
            "Batch 15030 done: batch loss 0.0005248787929303944\n",
            "Batch 15040 done: batch loss 0.0004906703252345324\n",
            "Batch 15050 done: batch loss 0.0004759498406201601\n",
            "Batch 15060 done: batch loss 0.00047226191964000463\n",
            "Batch 15070 done: batch loss 0.0004768904182128608\n",
            "Batch 15080 done: batch loss 0.0004695169918704778\n",
            "Batch 15090 done: batch loss 0.00048728007823228836\n",
            "Batch 15100 done: batch loss 0.0004822006158065051\n",
            "Batch 15110 done: batch loss 0.0004941270453855395\n",
            "Batch 15120 done: batch loss 0.0004883748479187489\n",
            "Batch 15130 done: batch loss 0.00046542350901290774\n",
            "Batch 15140 done: batch loss 0.0005155439721420407\n",
            "Batch 15150 done: batch loss 0.00046783924335613847\n",
            "Batch 15160 done: batch loss 0.0004893502919003367\n",
            "Batch 15170 done: batch loss 0.0004883672809228301\n",
            "Batch 15180 done: batch loss 0.0004804786294698715\n",
            "Batch 15190 done: batch loss 0.0004938248894177377\n",
            "Batch 15200 done: batch loss 0.0005357391200959682\n",
            "Batch 15210 done: batch loss 0.0005215242854319513\n",
            "Batch 15220 done: batch loss 0.0005005699349567294\n",
            "Batch 15230 done: batch loss 0.0004919002531096339\n",
            "Batch 15240 done: batch loss 0.0004817323642782867\n",
            "Batch 15250 done: batch loss 0.00046453700633719563\n",
            "Batch 15260 done: batch loss 0.0005357991904020309\n",
            "Batch 15270 done: batch loss 0.00047797945444472134\n",
            "Batch 15280 done: batch loss 0.000495224492624402\n",
            "Batch 15290 done: batch loss 0.0004969011060893536\n",
            "Batch 15300 done: batch loss 0.0004710362118203193\n",
            "Batch 15310 done: batch loss 0.0005069717299193144\n",
            "Batch 15320 done: batch loss 0.0005057684029452503\n",
            "Batch 15330 done: batch loss 0.000452374602900818\n",
            "Batch 15340 done: batch loss 0.0004806289216503501\n",
            "Batch 15350 done: batch loss 0.0004726137558463961\n",
            "Batch 15360 done: batch loss 0.0004780333547387272\n",
            "Batch 15370 done: batch loss 0.0005084298318251967\n",
            "Batch 15380 done: batch loss 0.0004649580514524132\n",
            "Batch 15390 done: batch loss 0.00047255863319151103\n",
            "Batch 15400 done: batch loss 0.0005205132183618844\n",
            "Batch 15410 done: batch loss 0.0004737598937936127\n",
            "Batch 15420 done: batch loss 0.0005254206480458379\n",
            "Batch 15430 done: batch loss 0.0004735439724754542\n",
            "Batch 15440 done: batch loss 0.0005037731025367975\n",
            "Batch 15450 done: batch loss 0.0004840659094043076\n",
            "Batch 15460 done: batch loss 0.0005063362186774611\n",
            "Batch 15470 done: batch loss 0.00048615256673656404\n",
            "Batch 15480 done: batch loss 0.0004570073797367513\n",
            "Batch 15490 done: batch loss 0.0004752588283736259\n",
            "Batch 15500 done: batch loss 0.0004899359191767871\n",
            "Batch 15510 done: batch loss 0.0004850882978644222\n",
            "Batch 15520 done: batch loss 0.0005013239569962025\n",
            "Batch 15530 done: batch loss 0.00048179589794017375\n",
            "Batch 15540 done: batch loss 0.0004919357015751302\n",
            "Batch 15550 done: batch loss 0.0004900522762909532\n",
            "Batch 15560 done: batch loss 0.00046425635810010135\n",
            "Batch 15570 done: batch loss 0.0004645535664167255\n",
            "Batch 15580 done: batch loss 0.00046026945346966386\n",
            "Batch 15590 done: batch loss 0.0004869091499131173\n",
            "Batch 15600 done: batch loss 0.0004961653612554073\n",
            "Batch 15610 done: batch loss 0.0004923916421830654\n",
            "Batch 15620 done: batch loss 0.0005133788217790425\n",
            "Batch 15630 done: batch loss 0.0004929921124130487\n",
            "Batch 15640 done: batch loss 0.00046889070654287934\n",
            "Batch 15650 done: batch loss 0.0005019017262384295\n",
            "Batch 15660 done: batch loss 0.0004986937856301665\n",
            "Batch 15670 done: batch loss 0.00047648107283748686\n",
            "Batch 15680 done: batch loss 0.00046972918789833784\n",
            "Batch 15690 done: batch loss 0.00048029873869381845\n",
            "Batch 15700 done: batch loss 0.0004895254969596863\n",
            "Batch 15710 done: batch loss 0.0005067791207693517\n",
            "Batch 15720 done: batch loss 0.0004887396353296936\n",
            "Batch 15730 done: batch loss 0.0005044080317020416\n",
            "Batch 15740 done: batch loss 0.0004978576907888055\n",
            "Batch 15750 done: batch loss 0.0005218841251917183\n",
            "Batch 15760 done: batch loss 0.0004854417347814888\n",
            "Batch 15770 done: batch loss 0.0004775100969709456\n",
            "Batch 15780 done: batch loss 0.0004977956414222717\n",
            "Batch 15790 done: batch loss 0.00046785615268163383\n",
            "Batch 15800 done: batch loss 0.0004915703902952373\n",
            "Batch 15810 done: batch loss 0.0004887787508778274\n",
            "Batch 15820 done: batch loss 0.00047916051698848605\n",
            "Batch 15830 done: batch loss 0.0005105387535877526\n",
            "Batch 15840 done: batch loss 0.0004954510368406773\n",
            "Batch 15850 done: batch loss 0.00045758893247693777\n",
            "Batch 15860 done: batch loss 0.0004503272648435086\n",
            "Batch 15870 done: batch loss 0.0004840959736611694\n",
            "Batch 15880 done: batch loss 0.0004524872056208551\n",
            "Batch 15890 done: batch loss 0.0004728703643195331\n",
            "Batch 15900 done: batch loss 0.0004696274409070611\n",
            "Batch 15910 done: batch loss 0.00046037029824219644\n",
            "Batch 15920 done: batch loss 0.0004900872590951622\n",
            "Batch 15930 done: batch loss 0.0005038512172177434\n",
            "Batch 15940 done: batch loss 0.0005006802384741604\n",
            "Batch 15950 done: batch loss 0.0005297641037032008\n",
            "Batch 15960 done: batch loss 0.00046245387056842446\n",
            "Batch 15970 done: batch loss 0.0004960191436111927\n",
            "Batch 15980 done: batch loss 0.00045780790969729424\n",
            "Batch 15990 done: batch loss 0.000522264395840466\n",
            "Batch 16000 done: batch loss 0.0004466933314688504\n",
            "Batch 16010 done: batch loss 0.00047272798838093877\n",
            "Batch 16020 done: batch loss 0.0004865638038609177\n",
            "Batch 16030 done: batch loss 0.0004986525746062398\n",
            "Batch 16040 done: batch loss 0.00045043029240332544\n",
            "Batch 16050 done: batch loss 0.00047580135287716985\n",
            "Batch 16060 done: batch loss 0.0005067677120678127\n",
            "Batch 16070 done: batch loss 0.0005027670413255692\n",
            "Batch 16080 done: batch loss 0.0004621806729119271\n",
            "Batch 16090 done: batch loss 0.00048236994189210236\n",
            "Batch 16100 done: batch loss 0.0005073633510619402\n",
            "Batch 16110 done: batch loss 0.00047144986456260085\n",
            "Batch 16120 done: batch loss 0.0005018694791942835\n",
            "Batch 16130 done: batch loss 0.00047410355182364583\n",
            "Batch 16140 done: batch loss 0.00046390300849452615\n",
            "Batch 16150 done: batch loss 0.0005178546416573226\n",
            "Batch 16160 done: batch loss 0.0004806577053386718\n",
            "Batch 16170 done: batch loss 0.0005083941505290568\n",
            "Batch 16180 done: batch loss 0.00048691401025280356\n",
            "Batch 16190 done: batch loss 0.0005054372013546526\n",
            "Batch 16200 done: batch loss 0.00046954461140558124\n",
            "Batch 16210 done: batch loss 0.00048773360322229564\n",
            "Batch 16220 done: batch loss 0.00047111004823818803\n",
            "Batch 16230 done: batch loss 0.00048489903565496206\n",
            "Batch 16240 done: batch loss 0.0004824033530894667\n",
            "Batch 16250 done: batch loss 0.00047929276479408145\n",
            "Batch 16260 done: batch loss 0.0004834685823880136\n",
            "Batch 16270 done: batch loss 0.0004945675609633327\n",
            "Batch 16280 done: batch loss 0.0004813210980501026\n",
            "Batch 16290 done: batch loss 0.0004668096371460706\n",
            "Batch 16300 done: batch loss 0.0004925185348838568\n",
            "Batch 16310 done: batch loss 0.0004812243569176644\n",
            "Batch 16320 done: batch loss 0.00046388249029405415\n",
            "Batch 16330 done: batch loss 0.00048208015505224466\n",
            "Batch 16340 done: batch loss 0.0004927276750095189\n",
            "Batch 16350 done: batch loss 0.00045562838204205036\n",
            "Batch 16360 done: batch loss 0.0005026943399570882\n",
            "Batch 16370 done: batch loss 0.00048162476741708815\n",
            "Batch 16380 done: batch loss 0.0004928113194182515\n",
            "Batch 16390 done: batch loss 0.000496266467962414\n",
            "Batch 16400 done: batch loss 0.0004842512425966561\n",
            "Batch 16410 done: batch loss 0.0005094569642096758\n",
            "Batch 16420 done: batch loss 0.0004856084706261754\n",
            "Batch 16430 done: batch loss 0.00046213765745051205\n",
            "Batch 16440 done: batch loss 0.00045094871893525124\n",
            "Batch 16450 done: batch loss 0.0004864900838583708\n",
            "Batch 16460 done: batch loss 0.0004617883241735399\n",
            "Batch 16470 done: batch loss 0.00045027121086604893\n",
            "Batch 16480 done: batch loss 0.0004961692611686885\n",
            "Batch 16490 done: batch loss 0.0004835579311475158\n",
            "Batch 16500 done: batch loss 0.0005059982649981976\n",
            "Batch 16510 done: batch loss 0.0005012117908336222\n",
            "Batch 16520 done: batch loss 0.0004808052908629179\n",
            "Batch 16530 done: batch loss 0.00048389885341748595\n",
            "Batch 16540 done: batch loss 0.0005528072360903025\n",
            "Batch 16550 done: batch loss 0.00045900713303126395\n",
            "Batch 16560 done: batch loss 0.00046319206012412906\n",
            "Batch 16570 done: batch loss 0.0004920042701996863\n",
            "Batch 16580 done: batch loss 0.0005198543658480048\n",
            "Batch 16590 done: batch loss 0.0004354712145868689\n",
            "Batch 16600 done: batch loss 0.0004986757994629443\n",
            "Batch 16610 done: batch loss 0.00046273297630250454\n",
            "Batch 16620 done: batch loss 0.0004835503059439361\n",
            "Batch 16630 done: batch loss 0.0004697661497630179\n",
            "Batch 16640 done: batch loss 0.00047762072063051164\n",
            "Batch 16650 done: batch loss 0.0004668880719691515\n",
            "Batch 16660 done: batch loss 0.0004931078874506056\n",
            "Batch 16670 done: batch loss 0.0004750171210616827\n",
            "Batch 16680 done: batch loss 0.00048095814418047667\n",
            "Batch 16690 done: batch loss 0.0005218634614720941\n",
            "Batch 16700 done: batch loss 0.0004959518555551767\n",
            "Batch 16710 done: batch loss 0.0004652711795642972\n",
            "Batch 16720 done: batch loss 0.0005081021226942539\n",
            "Batch 16730 done: batch loss 0.0005200588493607938\n",
            "Batch 16740 done: batch loss 0.00048269584658555686\n",
            "Batch 16750 done: batch loss 0.0005089124315418303\n",
            "Batch 16760 done: batch loss 0.0004805077041964978\n",
            "Batch 16770 done: batch loss 0.0004645470471587032\n",
            "Batch 16780 done: batch loss 0.000479613256175071\n",
            "Batch 16790 done: batch loss 0.0004936100449413061\n",
            "Batch 16800 done: batch loss 0.0004755316476803273\n",
            "Batch 16810 done: batch loss 0.0004686028987634927\n",
            "Batch 16820 done: batch loss 0.00046994592412374914\n",
            "Batch 16830 done: batch loss 0.0004767811333294958\n",
            "Batch 16840 done: batch loss 0.0004869612166658044\n",
            "Batch 16850 done: batch loss 0.00045448713353835046\n",
            "Batch 16860 done: batch loss 0.0005033613997511566\n",
            "Batch 16870 done: batch loss 0.0004615227226167917\n",
            "Batch 16880 done: batch loss 0.00044985683052800596\n",
            "Batch 16890 done: batch loss 0.0004782751784659922\n",
            "Batch 16900 done: batch loss 0.0004620345134753734\n",
            "Batch 16910 done: batch loss 0.00047287263441830873\n",
            "Batch 16920 done: batch loss 0.00044134780182503164\n",
            "Batch 16930 done: batch loss 0.00046503619523718953\n",
            "Batch 16940 done: batch loss 0.0004713106609415263\n",
            "Batch 16950 done: batch loss 0.00047979384544305503\n",
            "Batch 16960 done: batch loss 0.0004999095108360052\n",
            "Batch 16970 done: batch loss 0.00046631175791844726\n",
            "Batch 16980 done: batch loss 0.00048569170758128166\n",
            "Batch 16990 done: batch loss 0.0004885357921011746\n",
            "Batch 17000 done: batch loss 0.00047296102275140584\n",
            "Batch 17010 done: batch loss 0.00046969379764050245\n",
            "Batch 17020 done: batch loss 0.0005232878029346466\n",
            "Batch 17030 done: batch loss 0.0004951527807861567\n",
            "Batch 17040 done: batch loss 0.00046170136192813516\n",
            "Batch 17050 done: batch loss 0.0004989640437997878\n",
            "Batch 17060 done: batch loss 0.00047248537885025144\n",
            "Batch 17070 done: batch loss 0.0004607845039572567\n",
            "Batch 17080 done: batch loss 0.0004974151961505413\n",
            "Batch 17090 done: batch loss 0.0004747570783365518\n",
            "Batch 17100 done: batch loss 0.0004841009504161775\n",
            "Batch 17110 done: batch loss 0.00047614495269954205\n",
            "Batch 17120 done: batch loss 0.00047353742411360145\n",
            "Batch 17130 done: batch loss 0.0004642517014872283\n",
            "Batch 17140 done: batch loss 0.0004733429232146591\n",
            "Batch 17150 done: batch loss 0.00047756009735167027\n",
            "Batch 17160 done: batch loss 0.0004983375547453761\n",
            "Batch 17170 done: batch loss 0.0005199522129260004\n",
            "Batch 17180 done: batch loss 0.00048737492761574686\n",
            "Batch 17190 done: batch loss 0.00046925919014029205\n",
            "Batch 17200 done: batch loss 0.00045295528252609074\n",
            "Batch 17210 done: batch loss 0.00046578081673942506\n",
            "Batch 17220 done: batch loss 0.0004770398372784257\n",
            "Batch 17230 done: batch loss 0.0004677110118791461\n",
            "Batch 17240 done: batch loss 0.000498088076710701\n",
            "Batch 17250 done: batch loss 0.0004954887554049492\n",
            "Batch 17260 done: batch loss 0.000510220299474895\n",
            "Batch 17270 done: batch loss 0.0004712559748440981\n",
            "Batch 17280 done: batch loss 0.0005236691795289516\n",
            "Batch 17290 done: batch loss 0.0004787286452483386\n",
            "Batch 17300 done: batch loss 0.000515319115947932\n",
            "Batch 17310 done: batch loss 0.0005457323277369142\n",
            "Batch 17320 done: batch loss 0.00048406407586298883\n",
            "Batch 17330 done: batch loss 0.0004727878258563578\n",
            "Batch 17340 done: batch loss 0.0005071316263638437\n",
            "Batch 17350 done: batch loss 0.0004797610454261303\n",
            "Batch 17360 done: batch loss 0.0004911719006486237\n",
            "Batch 17370 done: batch loss 0.0004907583352178335\n",
            "Batch 17380 done: batch loss 0.0004686983593273908\n",
            "Batch 17390 done: batch loss 0.0004459513584151864\n",
            "Batch 17400 done: batch loss 0.0004907328984700143\n",
            "Batch 17410 done: batch loss 0.0004900816711597145\n",
            "Batch 17420 done: batch loss 0.00046182519872672856\n",
            "Batch 17430 done: batch loss 0.0004926735418848693\n",
            "Batch 17440 done: batch loss 0.0004926158580929041\n",
            "Batch 17450 done: batch loss 0.0004831652622669935\n",
            "Batch 17460 done: batch loss 0.0005147490883246064\n",
            "Batch 17470 done: batch loss 0.00045730077545158565\n",
            "Batch 17480 done: batch loss 0.0004983649123460054\n",
            "Batch 17490 done: batch loss 0.0005003262776881456\n",
            "Batch 17500 done: batch loss 0.00048601452726870775\n",
            "Batch 17510 done: batch loss 0.0004907659022137523\n",
            "Batch 17520 done: batch loss 0.0005085514858365059\n",
            "Batch 17530 done: batch loss 0.0005236375727690756\n",
            "Batch 17540 done: batch loss 0.00047397773596458137\n",
            "Batch 17550 done: batch loss 0.0004587321018334478\n",
            "Batch 17560 done: batch loss 0.0005097328103147447\n",
            "Batch 17570 done: batch loss 0.0004778006114065647\n",
            "Batch 17580 done: batch loss 0.00047713256208226085\n",
            "Batch 17590 done: batch loss 0.0004599576932378113\n",
            "Batch 17600 done: batch loss 0.00045511560165323317\n",
            "Batch 17610 done: batch loss 0.0004961193189956248\n",
            "Batch 17620 done: batch loss 0.00047503848327323794\n",
            "Batch 17630 done: batch loss 0.0004768804647028446\n",
            "Batch 17640 done: batch loss 0.0005623038159683347\n",
            "Batch 17650 done: batch loss 0.0005148430936969817\n",
            "Batch 17660 done: batch loss 0.00048541423166170716\n",
            "Batch 17670 done: batch loss 0.00048012775368988514\n",
            "Batch 17680 done: batch loss 0.0005118286935612559\n",
            "Batch 17690 done: batch loss 0.00046684822882525623\n",
            "Batch 17700 done: batch loss 0.00048415918718092144\n",
            "Batch 17710 done: batch loss 0.0004954336327500641\n",
            "Batch 17720 done: batch loss 0.00048650195822119713\n",
            "Batch 17730 done: batch loss 0.00047745578922331333\n",
            "Batch 17740 done: batch loss 0.000475937471492216\n",
            "Batch 17750 done: batch loss 0.0005571181536652148\n",
            "Batch 17760 done: batch loss 0.0004634485230781138\n",
            "Batch 17770 done: batch loss 0.0005090502090752125\n",
            "Batch 17780 done: batch loss 0.0004926035762764513\n",
            "Batch 17790 done: batch loss 0.0004692857910413295\n",
            "Batch 17800 done: batch loss 0.0005075413500890136\n",
            "Batch 17810 done: batch loss 0.00047819517203606665\n",
            "Batch 17820 done: batch loss 0.000505955598782748\n",
            "Batch 17830 done: batch loss 0.0004548071592580527\n",
            "Batch 17840 done: batch loss 0.00047104526311159134\n",
            "Batch 17850 done: batch loss 0.0004895514575764537\n",
            "Batch 17860 done: batch loss 0.00048405598499812186\n",
            "Batch 17870 done: batch loss 0.0004809066595043987\n",
            "Batch 17880 done: batch loss 0.0005489123868755996\n",
            "Batch 17890 done: batch loss 0.00046499253949150443\n",
            "Batch 17900 done: batch loss 0.00047525190166197717\n",
            "Batch 17910 done: batch loss 0.0004648164031095803\n",
            "Batch 17920 done: batch loss 0.00046662145177833736\n",
            "Batch 17930 done: batch loss 0.0004474977031350136\n",
            "Batch 17940 done: batch loss 0.0004565449489746243\n",
            "Batch 17950 done: batch loss 0.0005132827209308743\n",
            "Batch 17960 done: batch loss 0.0005073838401585817\n",
            "Batch 17970 done: batch loss 0.0005063278367742896\n",
            "Batch 17980 done: batch loss 0.0004761973686981946\n",
            "Batch 17990 done: batch loss 0.0004995972267352045\n",
            "Batch 18000 done: batch loss 0.0005309578846208751\n",
            "Batch 18010 done: batch loss 0.00046026954078115523\n",
            "Batch 18020 done: batch loss 0.00048485316801816225\n",
            "Batch 18030 done: batch loss 0.00048302486538887024\n",
            "Batch 18040 done: batch loss 0.0005153974634595215\n",
            "Batch 18050 done: batch loss 0.0004728870408143848\n",
            "Batch 18060 done: batch loss 0.0004909035051241517\n",
            "Batch 18070 done: batch loss 0.0004927125410176814\n",
            "Batch 18080 done: batch loss 0.0004704944440163672\n",
            "Batch 18090 done: batch loss 0.0004982185200788081\n",
            "Batch 18100 done: batch loss 0.00045655836584046483\n",
            "Batch 18110 done: batch loss 0.00047437319881282747\n",
            "Batch 18120 done: batch loss 0.0004873896832577884\n",
            "Batch 18130 done: batch loss 0.0005064074648544192\n",
            "Batch 18140 done: batch loss 0.0004852472338825464\n",
            "Batch 18150 done: batch loss 0.000482239353004843\n",
            "Batch 18160 done: batch loss 0.0004671038768719882\n",
            "Batch 18170 done: batch loss 0.00047864255611784756\n",
            "Batch 18180 done: batch loss 0.00045460276305675507\n",
            "Batch 18190 done: batch loss 0.00048558905837126076\n",
            "Batch 18200 done: batch loss 0.0004551414167508483\n",
            "Batch 18210 done: batch loss 0.0005111873033456504\n",
            "Batch 18220 done: batch loss 0.00048802184755913913\n",
            "Batch 18230 done: batch loss 0.0004847008385695517\n",
            "Batch 18240 done: batch loss 0.000491684942971915\n",
            "Batch 18250 done: batch loss 0.00048420767416246235\n",
            "Batch 18260 done: batch loss 0.00046569472760893404\n",
            "Batch 18270 done: batch loss 0.0004954662290401757\n",
            "Batch 18280 done: batch loss 0.0004704361781477928\n",
            "Batch 18290 done: batch loss 0.0004686253087129444\n",
            "Batch 18300 done: batch loss 0.0004540946101769805\n",
            "Batch 18310 done: batch loss 0.0005011780303902924\n",
            "Batch 18320 done: batch loss 0.0004700011922977865\n",
            "Batch 18330 done: batch loss 0.00046793368528597057\n",
            "Batch 18340 done: batch loss 0.0004911291180178523\n",
            "Batch 18350 done: batch loss 0.0004789196827914566\n",
            "Batch 18360 done: batch loss 0.0005034919595345855\n",
            "Batch 18370 done: batch loss 0.00048600969603285193\n",
            "Batch 18380 done: batch loss 0.0005259025492705405\n",
            "Batch 18390 done: batch loss 0.0004963125684298575\n",
            "Batch 18400 done: batch loss 0.0005218417500145733\n",
            "Batch 18410 done: batch loss 0.00046388382907025516\n",
            "Batch 18420 done: batch loss 0.00047040992649272084\n",
            "Batch 18430 done: batch loss 0.0004736333794426173\n",
            "Batch 18440 done: batch loss 0.00045827936264686286\n",
            "Batch 18450 done: batch loss 0.00048315327148884535\n",
            "Batch 18460 done: batch loss 0.0004906035610474646\n",
            "Batch 18470 done: batch loss 0.00047279580030590296\n",
            "Batch 18480 done: batch loss 0.00046283972915261984\n",
            "Batch 18490 done: batch loss 0.0004619212122634053\n",
            "Batch 18500 done: batch loss 0.0004798744630534202\n",
            "Batch 18510 done: batch loss 0.0005089377518743277\n",
            "Batch 18520 done: batch loss 0.0004534251056611538\n",
            "Batch 18530 done: batch loss 0.0005321591161191463\n",
            "Batch 18540 done: batch loss 0.0004559470107778907\n",
            "Batch 18550 done: batch loss 0.0005020119715481997\n",
            "Batch 18560 done: batch loss 0.00048825552221387625\n",
            "Batch 18570 done: batch loss 0.00046319144894368947\n",
            "Batch 18580 done: batch loss 0.0005029765889048576\n",
            "Batch 18590 done: batch loss 0.0005191367235966027\n",
            "Batch 18600 done: batch loss 0.00042392106843180954\n",
            "Batch 18610 done: batch loss 0.0004878089530393481\n",
            "Batch 18620 done: batch loss 0.000486310658743605\n",
            "Batch 18630 done: batch loss 0.00045930477790534496\n",
            "Batch 18640 done: batch loss 0.0004673678195104003\n",
            "Batch 18650 done: batch loss 0.0004526499833445996\n",
            "Batch 18660 done: batch loss 0.00048128963680937886\n",
            "Batch 18670 done: batch loss 0.0004670543712563813\n",
            "Batch 18680 done: batch loss 0.0004992480389773846\n",
            "Batch 18690 done: batch loss 0.0004906820831820369\n",
            "Batch 18700 done: batch loss 0.000445716199465096\n",
            "Batch 18710 done: batch loss 0.0004643526626750827\n",
            "Batch 18720 done: batch loss 0.0004914923920296133\n",
            "Batch 18730 done: batch loss 0.0005027495208196342\n",
            "Batch 18740 done: batch loss 0.0004917249316349626\n",
            "Batch 18750 done: batch loss 0.0005132010555826128\n",
            "Batch 18760 done: batch loss 0.0005191261298023164\n",
            "Batch 18770 done: batch loss 0.0004647327587008476\n",
            "Batch 18780 done: batch loss 0.0004851197882089764\n",
            "Batch 18790 done: batch loss 0.0004668302717618644\n",
            "Batch 18800 done: batch loss 0.0004691562207881361\n",
            "Batch 18810 done: batch loss 0.0004889589617960155\n",
            "Batch 18820 done: batch loss 0.00047073332825675607\n",
            "Batch 18830 done: batch loss 0.00045808774302713573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1M8GSkh-GVC",
        "colab_type": "code",
        "outputId": "aff2813b-5846-4b14-b765-99be39425492",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=1001, batch_size=16)\n",
        "\n",
        "my_model.test(train_imgs[0:16])\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is False\n",
            "INFO:tensorflow:Restoring parameters from latest1-20000\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryasiSMz-GdA",
        "colab_type": "code",
        "outputId": "fe9554c5-24f2-4425-f0a6-ccbbc331ae08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        }
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 44508, 0, 0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 317, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 348, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 361, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 721, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 418, in handle\n",
            "    self.handle_one_request()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 406, in handle_one_request\n",
            "    method()\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 639, in do_GET\n",
            "    self.copyfile(f, self.wfile)\n",
            "  File \"/usr/lib/python3.6/http/server.py\", line 800, in copyfile\n",
            "    shutil.copyfileobj(source, outputfile)\n",
            "  File \"/usr/lib/python3.6/shutil.py\", line 82, in copyfileobj\n",
            "    fdst.write(buf)\n",
            "  File \"/usr/lib/python3.6/socketserver.py\", line 800, in write\n",
            "    self._sock.sendall(b)\n",
            "ConnectionResetError: [Errno 104] Connection reset by peer\n",
            "----------------------------------------\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-db00d0990b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/latest1.index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/latest1.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model/latest1.data-00000-of-00001'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmakTnm-GjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgTCmum-Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1B9Ick-GnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}