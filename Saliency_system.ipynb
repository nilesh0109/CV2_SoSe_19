{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saliency_system.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/Saliency_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwVzzyrpD2R",
        "colab_type": "code",
        "outputId": "a456b0e3-f1d2-4b31-f2c0-68ecbbeaf992",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import zipfile\n",
        "from google.colab import files, drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK13Xnq_pG2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/CV2 exercies/Archive.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EayiS3g6pJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "from __future__ import division\n",
        "from skimage.transform import resize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBWzAZLlpLcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT= '/tmp'\n",
        "NUM_IMAGES = 1200\n",
        "def load_train_data():\n",
        "\ttraining_img_directory = ROOT+'/data/train/images'\n",
        "\ttraining_fixation_directory = ROOT+'/data/train/fixations'\n",
        "\n",
        "\ttrain_imgs = np.zeros((NUM_IMAGES, 180, 320, 3), dtype=np.uint8)\n",
        "\ttrain_fixations = np.zeros((NUM_IMAGES, 180, 320, 1), dtype=np.uint8)\n",
        "\tfor i in range(1, NUM_IMAGES + 1):\n",
        "\t\timg_file = os.path.join(training_img_directory, '{:04d}.jpg'.format(i))\n",
        "\t\tfixation_file = os.path.join(training_fixation_directory, '{:04d}.jpg'.format(i))\n",
        "\t\ttrain_imgs[i-1] = imageio.imread(img_file)\n",
        "\t\tfixation = imageio.imread(fixation_file)\n",
        "\t\ttrain_fixations[i-1] = np.expand_dims(fixation, -1) # adds singleton dimension so fixation size is (180,320,1)\n",
        "\t\n",
        "\treturn train_imgs, train_fixations\n",
        "\n",
        "# Generator function will output one (image, target) tuple at a time,\n",
        "# and shuffle the data for each new epoch\n",
        "def data_generator(imgs, targets):\n",
        "\twhile True: # produce new epochs forever\n",
        "\t\t# Shuffle the data for this epoch\n",
        "\t\tidx = np.arange(imgs.shape[0])\n",
        "\t\tnp.random.shuffle(idx)\n",
        "\n",
        "\t\timgs = imgs[idx]\n",
        "\t\ttargets = targets[idx]\n",
        "\t\tfor i in range(imgs.shape[0]):\n",
        "\t\t\tyield imgs[i], targets[i]\n",
        "\n",
        "def get_batch_from_generator(gen, batchsize):\n",
        "\tbatch_imgs = []\n",
        "\tbatch_fixations = []\n",
        "\tfor i in range(batchsize):\n",
        "\t\timg, target = gen.__next__()\n",
        "\t\tbatch_imgs.append(img)\n",
        "\t\tbatch_fixations.append(target)\n",
        "\treturn np.array(batch_imgs), np.array(batch_fixations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFi73W_n0rEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "'''\n",
        "Model class\n",
        "'''\n",
        "\n",
        "class saliencyModel:\n",
        "  def __init__(self, model_weights=None, learning_rate=1e-4, batch_size=32, num_batches=100, prior_downsampling_factor=10):\n",
        "    self.dir_path = 'drive/My Drive/Colab Notebooks/CV2 exercies/'\n",
        "    self.model_path = self.dir_path+'model_cptk/my-model1'\n",
        "    self.lr_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = num_batches\n",
        "    self.prior_downsampling_factor = prior_downsampling_factor\n",
        "    self.input_h = 180\n",
        "    self.input_w = 320\n",
        "    self.prior_h = self.input_h /(2 * prior_downsampling_factor)\n",
        "    self.prior_w = self.input_w / (2 * prior_downsampling_factor)\n",
        "    self.reg_lambda = 1 / (self.prior_h * self.prior_w)\n",
        "    \n",
        "    if model_weights is not None:\n",
        "      self.load_weights(model_weights)\n",
        "    \n",
        "  def load_weights(self, model_weights):\n",
        "    vgg_weight_file = model_weights\n",
        "    self.weights = np.load(vgg_weight_file)\n",
        "  \n",
        "  def setup(self, mode= 'Train'):\n",
        "    self.input_images_placeholder = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 3])\n",
        "    self.target_images_placehodler = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 1])\n",
        "    \n",
        "    with tf.name_scope('preprocessing') as scope:\n",
        "      input_imgs = tf.image.convert_image_dtype(self.input_images_placeholder, tf.float32) * 255\n",
        "      fixations_normalized = tf.image.convert_image_dtype(self.target_images_placehodler, tf.float32)\n",
        "      mean = tf.constant([123.68 , 116.779 , 103.939], dtype = tf.float32, shape =[1,1,1,3], name ='img_mean')\n",
        "      imgs_normalized = input_imgs - mean\n",
        "\n",
        "    with tf.name_scope('conv1_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(imgs_normalized, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv1_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool1') as scope:\n",
        "      pool = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv2_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv2_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool2') as scope:\n",
        "      pool2 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv3_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool3') as scope:\n",
        "      pool3 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv4_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv4_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv4_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('pool4') as scope:\n",
        "      pool4 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "    \n",
        "    with tf.name_scope('conv5_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv5_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv5_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      conv5_3 = act\n",
        "      \n",
        "    with tf.name_scope('concat_featuremaps') as scope:\n",
        "      concatenated_feature_maps = tf.concat([pool2, pool3, pool4, conv5_3], axis=3)\n",
        "      print('IN_TRAINING_MODE is',mode =='Train')\n",
        "      regularized_feature_maps = tf.layers.dropout(concatenated_feature_maps, rate=0.5, training= mode =='Train')\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv1') as scope:\n",
        "      act_featuremaps_conv1 = tf.layers.conv2d(regularized_feature_maps, filters=64, kernel_size=(3,3), activation=tf.nn.relu, name='featuremaps_conv1', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv2') as scope:\n",
        "      act_featuremaps_conv2 = tf.layers.conv2d(act_featuremaps_conv1, filters=1, kernel_size=(1,1), activation=tf.nn.relu, name='featuremaps_conv2', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('Learned_prior') as scope:\n",
        "      #weight_initer = tf.random_normal_initializer(mean=0.0, stddev=0.1)\n",
        "      #prior_shape = tf.ceil(tf.divide(act_featuremaps_conv2.shape[1:3], self.prior_downsampling_factor))\n",
        "      prior_shape = (1, self.prior_h, self.prior_w,1)\n",
        "      #init = tf.ones(prior_shape, dtype=tf.dtypes.float32, name='prior')\n",
        "      #prior = tf.get_variable(name=\"prior\", dtype=tf.float32, shape=prior_shape, init=weight_initer)\n",
        "      prior = tf.Variable(tf.ones(prior_shape), name=\"prior\", trainable=True)\n",
        "      upsampled_prior = tf.image.resize_bilinear(prior, size=act_featuremaps_conv2.shape[1:3])\n",
        "      saliency_mixed = tf.multiply(act_featuremaps_conv2, upsampled_prior)\n",
        "      saliency_raw = tf.nn.relu(saliency_mixed)\n",
        "      \n",
        "    with tf.name_scope('loss') as scope:\n",
        "      upsampled_saliency = tf.image.resize_bilinear(saliency_raw, size=fixations_normalized.shape[1:3])\n",
        "      # normalize saliency\n",
        "      max_value_per_image = tf.reduce_max(upsampled_saliency, axis=[1,2,3], keepdims=True)\n",
        "      predicted_saliency = (upsampled_saliency / max_value_per_image)\n",
        "      \n",
        "      #target_shape = predicted_saliency.shape[1:3]\n",
        "      #target_downscaled = tf.image.resize_images(fixations_normalized, target_shape)\n",
        "      \n",
        "      # Loss function from Cornia et al. (2016) [with higher weight for salient pixels]\n",
        "      alpha = 1.01\n",
        "      weight = 1.0 / (alpha - fixations_normalized)\n",
        "      loss = tf.losses.mean_squared_error(labels=fixations_normalized, \n",
        "                        predictions=predicted_saliency, \n",
        "                        weights=weight)\n",
        "      regularizer = tf.nn.l2_loss(1 - prior)\n",
        "      loss += tf.reduce_mean(self.reg_lambda * regularizer)\n",
        "      \n",
        "    return predicted_saliency, loss\n",
        "    \n",
        "  def train(self, t_imgs, t_fixations):\n",
        "    \n",
        "    pred ,loss_op = self.setup(mode='Train')\n",
        "    # Optimizer settings from Cornia et al. (2016) [except for decay]\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr_rate, momentum=0.9, use_nesterov=True)\n",
        "    #optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_rate)\n",
        "    minimize_op = optimizer.minimize(loss_op)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver.restore(sess, \"model/latest1\")\n",
        "      gen = data_generator(t_imgs, t_fixations)\n",
        "      for b in range(self.num_batches):\n",
        "        batch_imgs, batch_fixations = get_batch_from_generator(gen, self.batch_size)\n",
        "        predication, batch_loss,_ = sess.run([pred, loss_op, minimize_op], feed_dict={self.input_images_placeholder: batch_imgs, self.target_images_placehodler: batch_fixations})\n",
        "\n",
        "        if b % 10 == 0:\n",
        "          #writer.add_summary(l_summary, global_step=b)\n",
        "          print('Batch {0} done: batch loss {1}'.format(b, batch_loss))\n",
        "        if b % 1000 == 0 and b != 0:\n",
        "          save_path = saver.save(sess, 'model/latest1', global_step=b)\n",
        "      save_path = saver.save(sess, 'model/latest1')\n",
        "      \n",
        "  def test(self, test_imgs):\n",
        "    pred_saliency, l = self.setup(mode='Test')\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      saver.restore(sess, 'model/latest1')\n",
        "      saliency = sess.run(pred_saliency, feed_dict={self.input_images_placeholder: test_imgs})\n",
        "      for i in range(len(test_imgs)):\n",
        "        print('saving images')\n",
        "        #upsampled_saliency = resize(saliency[i], output_shape=(180, 320, 1))\n",
        "        saliency_img = sess.run(tf.image.convert_image_dtype(saliency[i], tf.uint8))\n",
        "        imageio.imwrite(str(i)+'.jpg', saliency_img)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhmQ0g-1-GKV",
        "colab_type": "code",
        "outputId": "2f81df5d-f79b-4449-f2be-5c5b831610ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 12955
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=10001, batch_size=16)\n",
        "\n",
        "my_model.train(train_imgs[0:16], train_fixations[0:16])\n",
        "#my_model.train(train_imgs, train_fixations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is True\n",
            "INFO:tensorflow:Restoring parameters from model/latest1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/latest1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch 0 done: batch loss 0.00294758309610188\n",
            "Batch 10 done: batch loss 0.0028973747976124287\n",
            "Batch 20 done: batch loss 0.003148599062114954\n",
            "Batch 30 done: batch loss 0.0030143503099679947\n",
            "Batch 40 done: batch loss 0.0031057714950293303\n",
            "Batch 50 done: batch loss 0.0028524433728307486\n",
            "Batch 60 done: batch loss 0.0027779564261436462\n",
            "Batch 70 done: batch loss 0.002915040124207735\n",
            "Batch 80 done: batch loss 0.0026885790284723043\n",
            "Batch 90 done: batch loss 0.002928053494542837\n",
            "Batch 100 done: batch loss 0.0027639653999358416\n",
            "Batch 110 done: batch loss 0.0026659821160137653\n",
            "Batch 120 done: batch loss 0.002714570378884673\n",
            "Batch 130 done: batch loss 0.002675824100151658\n",
            "Batch 140 done: batch loss 0.002900246065109968\n",
            "Batch 150 done: batch loss 0.003038170514628291\n",
            "Batch 160 done: batch loss 0.002761057810857892\n",
            "Batch 170 done: batch loss 0.00280189816839993\n",
            "Batch 180 done: batch loss 0.0027426520828157663\n",
            "Batch 190 done: batch loss 0.0025910220574587584\n",
            "Batch 200 done: batch loss 0.0024872999638319016\n",
            "Batch 210 done: batch loss 0.002677993383258581\n",
            "Batch 220 done: batch loss 0.002692071022465825\n",
            "Batch 230 done: batch loss 0.0026681397575885057\n",
            "Batch 240 done: batch loss 0.002652730094268918\n",
            "Batch 250 done: batch loss 0.002492450876161456\n",
            "Batch 260 done: batch loss 0.002645415486767888\n",
            "Batch 270 done: batch loss 0.0026244691107422113\n",
            "Batch 280 done: batch loss 0.002634955570101738\n",
            "Batch 290 done: batch loss 0.0027478504925966263\n",
            "Batch 300 done: batch loss 0.002620435319840908\n",
            "Batch 310 done: batch loss 0.0025295137893408537\n",
            "Batch 320 done: batch loss 0.002595835365355015\n",
            "Batch 330 done: batch loss 0.0024668811820447445\n",
            "Batch 340 done: batch loss 0.002491018269211054\n",
            "Batch 350 done: batch loss 0.0027114609256386757\n",
            "Batch 360 done: batch loss 0.002512713661417365\n",
            "Batch 370 done: batch loss 0.0025745974853634834\n",
            "Batch 380 done: batch loss 0.002725253812968731\n",
            "Batch 390 done: batch loss 0.002412855625152588\n",
            "Batch 400 done: batch loss 0.0023046075366437435\n",
            "Batch 410 done: batch loss 0.0025073427241295576\n",
            "Batch 420 done: batch loss 0.002440833020955324\n",
            "Batch 430 done: batch loss 0.002572233323007822\n",
            "Batch 440 done: batch loss 0.0024989459197968245\n",
            "Batch 450 done: batch loss 0.0023523569107055664\n",
            "Batch 460 done: batch loss 0.0025863524060696363\n",
            "Batch 470 done: batch loss 0.002384176477789879\n",
            "Batch 480 done: batch loss 0.0023706003557890654\n",
            "Batch 490 done: batch loss 0.0023956431541591883\n",
            "Batch 500 done: batch loss 0.002447207923978567\n",
            "Batch 510 done: batch loss 0.002542115980759263\n",
            "Batch 520 done: batch loss 0.002490775194019079\n",
            "Batch 530 done: batch loss 0.0023831187281757593\n",
            "Batch 540 done: batch loss 0.0023626647889614105\n",
            "Batch 550 done: batch loss 0.002399537479504943\n",
            "Batch 560 done: batch loss 0.002350695664063096\n",
            "Batch 570 done: batch loss 0.0022928996477276087\n",
            "Batch 580 done: batch loss 0.0022487868554890156\n",
            "Batch 590 done: batch loss 0.0023700790479779243\n",
            "Batch 600 done: batch loss 0.0024233933072537184\n",
            "Batch 610 done: batch loss 0.002331638475880027\n",
            "Batch 620 done: batch loss 0.002295272657647729\n",
            "Batch 630 done: batch loss 0.0021731061860919\n",
            "Batch 640 done: batch loss 0.0021666307002305984\n",
            "Batch 650 done: batch loss 0.00235114898532629\n",
            "Batch 660 done: batch loss 0.0022510087583214045\n",
            "Batch 670 done: batch loss 0.002146047307178378\n",
            "Batch 680 done: batch loss 0.0023020338267087936\n",
            "Batch 690 done: batch loss 0.0022830823436379433\n",
            "Batch 700 done: batch loss 0.0022181705571711063\n",
            "Batch 710 done: batch loss 0.0022673963103443384\n",
            "Batch 720 done: batch loss 0.002307960530743003\n",
            "Batch 730 done: batch loss 0.0022223927080631256\n",
            "Batch 740 done: batch loss 0.002255534054711461\n",
            "Batch 750 done: batch loss 0.002228198107331991\n",
            "Batch 760 done: batch loss 0.002157248556613922\n",
            "Batch 770 done: batch loss 0.002181203104555607\n",
            "Batch 780 done: batch loss 0.00219932128675282\n",
            "Batch 790 done: batch loss 0.0021114503033459187\n",
            "Batch 800 done: batch loss 0.002217847155407071\n",
            "Batch 810 done: batch loss 0.0022353301756083965\n",
            "Batch 820 done: batch loss 0.002051746938377619\n",
            "Batch 830 done: batch loss 0.002139566233381629\n",
            "Batch 840 done: batch loss 0.0020942934788763523\n",
            "Batch 850 done: batch loss 0.0020791045390069485\n",
            "Batch 860 done: batch loss 0.002139174845069647\n",
            "Batch 870 done: batch loss 0.0022045429795980453\n",
            "Batch 880 done: batch loss 0.002029671100899577\n",
            "Batch 890 done: batch loss 0.0021763844415545464\n",
            "Batch 900 done: batch loss 0.002184757962822914\n",
            "Batch 910 done: batch loss 0.002071464667096734\n",
            "Batch 920 done: batch loss 0.002274926984682679\n",
            "Batch 930 done: batch loss 0.0019704713486135006\n",
            "Batch 940 done: batch loss 0.002154237125068903\n",
            "Batch 950 done: batch loss 0.002133243251591921\n",
            "Batch 960 done: batch loss 0.0019877704326063395\n",
            "Batch 970 done: batch loss 0.00202560075558722\n",
            "Batch 980 done: batch loss 0.002085312968119979\n",
            "Batch 990 done: batch loss 0.002044635359197855\n",
            "Batch 1000 done: batch loss 0.0019672412890940905\n",
            "Batch 1010 done: batch loss 0.0021333207841962576\n",
            "Batch 1020 done: batch loss 0.001985010225325823\n",
            "Batch 1030 done: batch loss 0.0020661698654294014\n",
            "Batch 1040 done: batch loss 0.002034642966464162\n",
            "Batch 1050 done: batch loss 0.001973426202312112\n",
            "Batch 1060 done: batch loss 0.001985712442547083\n",
            "Batch 1070 done: batch loss 0.002121513243764639\n",
            "Batch 1080 done: batch loss 0.002017765073105693\n",
            "Batch 1090 done: batch loss 0.0019415615824982524\n",
            "Batch 1100 done: batch loss 0.0021254129242151976\n",
            "Batch 1110 done: batch loss 0.002025292720645666\n",
            "Batch 1120 done: batch loss 0.0020339416805654764\n",
            "Batch 1130 done: batch loss 0.002093270421028137\n",
            "Batch 1140 done: batch loss 0.001978190615773201\n",
            "Batch 1150 done: batch loss 0.002037196885794401\n",
            "Batch 1160 done: batch loss 0.0019343304447829723\n",
            "Batch 1170 done: batch loss 0.002158492337912321\n",
            "Batch 1180 done: batch loss 0.001923431409522891\n",
            "Batch 1190 done: batch loss 0.0019179515074938536\n",
            "Batch 1200 done: batch loss 0.0019398931181058288\n",
            "Batch 1210 done: batch loss 0.001984546659514308\n",
            "Batch 1220 done: batch loss 0.0020260894671082497\n",
            "Batch 1230 done: batch loss 0.0020261036697775126\n",
            "Batch 1240 done: batch loss 0.002019731095060706\n",
            "Batch 1250 done: batch loss 0.0018538587028160691\n",
            "Batch 1260 done: batch loss 0.0020339603070169687\n",
            "Batch 1270 done: batch loss 0.001903638825751841\n",
            "Batch 1280 done: batch loss 0.002102519618347287\n",
            "Batch 1290 done: batch loss 0.0019067567773163319\n",
            "Batch 1300 done: batch loss 0.0019077328033745289\n",
            "Batch 1310 done: batch loss 0.0019245867151767015\n",
            "Batch 1320 done: batch loss 0.0019241039408370852\n",
            "Batch 1330 done: batch loss 0.0019457479938864708\n",
            "Batch 1340 done: batch loss 0.001937930821441114\n",
            "Batch 1350 done: batch loss 0.001925949240103364\n",
            "Batch 1360 done: batch loss 0.0019376695854589343\n",
            "Batch 1370 done: batch loss 0.0019611187744885683\n",
            "Batch 1380 done: batch loss 0.0018198613543063402\n",
            "Batch 1390 done: batch loss 0.001960426103323698\n",
            "Batch 1400 done: batch loss 0.0020433065947145224\n",
            "Batch 1410 done: batch loss 0.00203167786821723\n",
            "Batch 1420 done: batch loss 0.00185841741040349\n",
            "Batch 1430 done: batch loss 0.001896330271847546\n",
            "Batch 1440 done: batch loss 0.0018789523746818304\n",
            "Batch 1450 done: batch loss 0.0019695069640874863\n",
            "Batch 1460 done: batch loss 0.0017823687521740794\n",
            "Batch 1470 done: batch loss 0.0018381387926638126\n",
            "Batch 1480 done: batch loss 0.001991416560485959\n",
            "Batch 1490 done: batch loss 0.001808390486985445\n",
            "Batch 1500 done: batch loss 0.0019264621660113335\n",
            "Batch 1510 done: batch loss 0.001778806559741497\n",
            "Batch 1520 done: batch loss 0.0017571444623172283\n",
            "Batch 1530 done: batch loss 0.0019636352080851793\n",
            "Batch 1540 done: batch loss 0.0018715583719313145\n",
            "Batch 1550 done: batch loss 0.00183124840259552\n",
            "Batch 1560 done: batch loss 0.0018171396804973483\n",
            "Batch 1570 done: batch loss 0.0019916174933314323\n",
            "Batch 1580 done: batch loss 0.0018817258533090353\n",
            "Batch 1590 done: batch loss 0.001830728491768241\n",
            "Batch 1600 done: batch loss 0.0017595383105799556\n",
            "Batch 1610 done: batch loss 0.001757918857038021\n",
            "Batch 1620 done: batch loss 0.0018057627603411674\n",
            "Batch 1630 done: batch loss 0.0018368259770795703\n",
            "Batch 1640 done: batch loss 0.0018364645075052977\n",
            "Batch 1650 done: batch loss 0.0018232854781672359\n",
            "Batch 1660 done: batch loss 0.0018063172465190291\n",
            "Batch 1670 done: batch loss 0.001942023285664618\n",
            "Batch 1680 done: batch loss 0.0018826763844117522\n",
            "Batch 1690 done: batch loss 0.0018641718197613955\n",
            "Batch 1700 done: batch loss 0.0017907280707731843\n",
            "Batch 1710 done: batch loss 0.0018612730782479048\n",
            "Batch 1720 done: batch loss 0.001761112012900412\n",
            "Batch 1730 done: batch loss 0.0017445090925320983\n",
            "Batch 1740 done: batch loss 0.0017625477630645037\n",
            "Batch 1750 done: batch loss 0.0017813049489632249\n",
            "Batch 1760 done: batch loss 0.0018128337105736136\n",
            "Batch 1770 done: batch loss 0.0017952495254576206\n",
            "Batch 1780 done: batch loss 0.0017937136581167579\n",
            "Batch 1790 done: batch loss 0.0017417763592675328\n",
            "Batch 1800 done: batch loss 0.0017538175452500582\n",
            "Batch 1810 done: batch loss 0.0017681523459032178\n",
            "Batch 1820 done: batch loss 0.0018308466533198953\n",
            "Batch 1830 done: batch loss 0.0017325596418231726\n",
            "Batch 1840 done: batch loss 0.0017135331872850657\n",
            "Batch 1850 done: batch loss 0.0017241713358089328\n",
            "Batch 1860 done: batch loss 0.0017157732509076595\n",
            "Batch 1870 done: batch loss 0.0017954674549400806\n",
            "Batch 1880 done: batch loss 0.0017679479205980897\n",
            "Batch 1890 done: batch loss 0.001744262408465147\n",
            "Batch 1900 done: batch loss 0.0016086343675851822\n",
            "Batch 1910 done: batch loss 0.0017228755168616772\n",
            "Batch 1920 done: batch loss 0.0018211627611890435\n",
            "Batch 1930 done: batch loss 0.00172861956525594\n",
            "Batch 1940 done: batch loss 0.0016858141170814633\n",
            "Batch 1950 done: batch loss 0.0017060446552932262\n",
            "Batch 1960 done: batch loss 0.0016547718551009893\n",
            "Batch 1970 done: batch loss 0.00183305109385401\n",
            "Batch 1980 done: batch loss 0.0016876214649528265\n",
            "Batch 1990 done: batch loss 0.001809431123547256\n",
            "Batch 2000 done: batch loss 0.0018033076776191592\n",
            "Batch 2010 done: batch loss 0.00172857113648206\n",
            "Batch 2020 done: batch loss 0.001774942153133452\n",
            "Batch 2030 done: batch loss 0.001652305480092764\n",
            "Batch 2040 done: batch loss 0.001783299958333373\n",
            "Batch 2050 done: batch loss 0.0016304072923958302\n",
            "Batch 2060 done: batch loss 0.0016202840488404036\n",
            "Batch 2070 done: batch loss 0.001773008843883872\n",
            "Batch 2080 done: batch loss 0.0017020255327224731\n",
            "Batch 2090 done: batch loss 0.001737545127980411\n",
            "Batch 2100 done: batch loss 0.001608254387974739\n",
            "Batch 2110 done: batch loss 0.0016409587115049362\n",
            "Batch 2120 done: batch loss 0.0015978148439899087\n",
            "Batch 2130 done: batch loss 0.0016377052525058389\n",
            "Batch 2140 done: batch loss 0.0017061381367966533\n",
            "Batch 2150 done: batch loss 0.001732616568915546\n",
            "Batch 2160 done: batch loss 0.0016766211483627558\n",
            "Batch 2170 done: batch loss 0.0016486422391608357\n",
            "Batch 2180 done: batch loss 0.0016329716891050339\n",
            "Batch 2190 done: batch loss 0.001621854375116527\n",
            "Batch 2200 done: batch loss 0.001635048072785139\n",
            "Batch 2210 done: batch loss 0.0016539017669856548\n",
            "Batch 2220 done: batch loss 0.0016403772169724107\n",
            "Batch 2230 done: batch loss 0.001568709034472704\n",
            "Batch 2240 done: batch loss 0.0016223926795646548\n",
            "Batch 2250 done: batch loss 0.0016284594312310219\n",
            "Batch 2260 done: batch loss 0.0017207891214638948\n",
            "Batch 2270 done: batch loss 0.001661765156313777\n",
            "Batch 2280 done: batch loss 0.001609294442459941\n",
            "Batch 2290 done: batch loss 0.0016264286823570728\n",
            "Batch 2300 done: batch loss 0.0016698164399713278\n",
            "Batch 2310 done: batch loss 0.001573126413859427\n",
            "Batch 2320 done: batch loss 0.0015498093562200665\n",
            "Batch 2330 done: batch loss 0.0016095740720629692\n",
            "Batch 2340 done: batch loss 0.0015188534744083881\n",
            "Batch 2350 done: batch loss 0.0015108403749763966\n",
            "Batch 2360 done: batch loss 0.0016266517341136932\n",
            "Batch 2370 done: batch loss 0.0017133780056610703\n",
            "Batch 2380 done: batch loss 0.0016457452438771725\n",
            "Batch 2390 done: batch loss 0.0015803409041836858\n",
            "Batch 2400 done: batch loss 0.0015595651930198073\n",
            "Batch 2410 done: batch loss 0.001608226215466857\n",
            "Batch 2420 done: batch loss 0.0015415438683703542\n",
            "Batch 2430 done: batch loss 0.0015135249122977257\n",
            "Batch 2440 done: batch loss 0.001640915754251182\n",
            "Batch 2450 done: batch loss 0.0015828745672479272\n",
            "Batch 2460 done: batch loss 0.0015700709773227572\n",
            "Batch 2470 done: batch loss 0.0015796385705471039\n",
            "Batch 2480 done: batch loss 0.0016525435494259\n",
            "Batch 2490 done: batch loss 0.0015699274372309446\n",
            "Batch 2500 done: batch loss 0.0015307511202991009\n",
            "Batch 2510 done: batch loss 0.0015284623950719833\n",
            "Batch 2520 done: batch loss 0.001647566445171833\n",
            "Batch 2530 done: batch loss 0.0015884003369137645\n",
            "Batch 2540 done: batch loss 0.0016205066349357367\n",
            "Batch 2550 done: batch loss 0.0017230529338121414\n",
            "Batch 2560 done: batch loss 0.001603110576979816\n",
            "Batch 2570 done: batch loss 0.0016198744997382164\n",
            "Batch 2580 done: batch loss 0.0015431413194164634\n",
            "Batch 2590 done: batch loss 0.001591220498085022\n",
            "Batch 2600 done: batch loss 0.0015266821719706059\n",
            "Batch 2610 done: batch loss 0.0015946386847645044\n",
            "Batch 2620 done: batch loss 0.0014873917680233717\n",
            "Batch 2630 done: batch loss 0.001508362591266632\n",
            "Batch 2640 done: batch loss 0.0015096345450729132\n",
            "Batch 2650 done: batch loss 0.0015294732293114066\n",
            "Batch 2660 done: batch loss 0.0015524488408118486\n",
            "Batch 2670 done: batch loss 0.0015884769381955266\n",
            "Batch 2680 done: batch loss 0.0014800261706113815\n",
            "Batch 2690 done: batch loss 0.0015669206622987986\n",
            "Batch 2700 done: batch loss 0.0015502417227253318\n",
            "Batch 2710 done: batch loss 0.0015223703812807798\n",
            "Batch 2720 done: batch loss 0.0016090264543890953\n",
            "Batch 2730 done: batch loss 0.0015533213736489415\n",
            "Batch 2740 done: batch loss 0.0015567094087600708\n",
            "Batch 2750 done: batch loss 0.0015518624568358064\n",
            "Batch 2760 done: batch loss 0.0016177515499293804\n",
            "Batch 2770 done: batch loss 0.0014869532315060496\n",
            "Batch 2780 done: batch loss 0.001484419102780521\n",
            "Batch 2790 done: batch loss 0.0014703592751175165\n",
            "Batch 2800 done: batch loss 0.0015503352042287588\n",
            "Batch 2810 done: batch loss 0.0015624833758920431\n",
            "Batch 2820 done: batch loss 0.0015327077126130462\n",
            "Batch 2830 done: batch loss 0.0015696951886639\n",
            "Batch 2840 done: batch loss 0.001637389068491757\n",
            "Batch 2850 done: batch loss 0.0015575364232063293\n",
            "Batch 2860 done: batch loss 0.0015984452329576015\n",
            "Batch 2870 done: batch loss 0.0015732980100437999\n",
            "Batch 2880 done: batch loss 0.0015487285563722253\n",
            "Batch 2890 done: batch loss 0.0015288610011339188\n",
            "Batch 2900 done: batch loss 0.001515365787781775\n",
            "Batch 2910 done: batch loss 0.0014897120418027043\n",
            "Batch 2920 done: batch loss 0.0014938071835786104\n",
            "Batch 2930 done: batch loss 0.0014986756723374128\n",
            "Batch 2940 done: batch loss 0.0014675140846520662\n",
            "Batch 2950 done: batch loss 0.0015512255486100912\n",
            "Batch 2960 done: batch loss 0.0014870049199089408\n",
            "Batch 2970 done: batch loss 0.0014859768562018871\n",
            "Batch 2980 done: batch loss 0.001541575649753213\n",
            "Batch 2990 done: batch loss 0.0014257498551160097\n",
            "Batch 3000 done: batch loss 0.0014838632196187973\n",
            "Batch 3010 done: batch loss 0.001437509898096323\n",
            "Batch 3020 done: batch loss 0.0015794693026691675\n",
            "Batch 3030 done: batch loss 0.0015246258117258549\n",
            "Batch 3040 done: batch loss 0.001584487734362483\n",
            "Batch 3050 done: batch loss 0.0014916950603947043\n",
            "Batch 3060 done: batch loss 0.0016264643054455519\n",
            "Batch 3070 done: batch loss 0.0015413996297866106\n",
            "Batch 3080 done: batch loss 0.0015336312353610992\n",
            "Batch 3090 done: batch loss 0.001484980108216405\n",
            "Batch 3100 done: batch loss 0.0014351282734423876\n",
            "Batch 3110 done: batch loss 0.001424883957952261\n",
            "Batch 3120 done: batch loss 0.0015232979785650969\n",
            "Batch 3130 done: batch loss 0.001437979401089251\n",
            "Batch 3140 done: batch loss 0.0014654172118753195\n",
            "Batch 3150 done: batch loss 0.001452833879739046\n",
            "Batch 3160 done: batch loss 0.001535940682515502\n",
            "Batch 3170 done: batch loss 0.0014967041788622737\n",
            "Batch 3180 done: batch loss 0.001513698953203857\n",
            "Batch 3190 done: batch loss 0.0014397029299288988\n",
            "Batch 3200 done: batch loss 0.0014888348523527384\n",
            "Batch 3210 done: batch loss 0.0013923683436587453\n",
            "Batch 3220 done: batch loss 0.0015439868438988924\n",
            "Batch 3230 done: batch loss 0.0015178434550762177\n",
            "Batch 3240 done: batch loss 0.0015171755803748965\n",
            "Batch 3250 done: batch loss 0.0014861429808661342\n",
            "Batch 3260 done: batch loss 0.0014604802709072828\n",
            "Batch 3270 done: batch loss 0.0015082169556990266\n",
            "Batch 3280 done: batch loss 0.0013891328126192093\n",
            "Batch 3290 done: batch loss 0.0014518757816404104\n",
            "Batch 3300 done: batch loss 0.0014397776685655117\n",
            "Batch 3310 done: batch loss 0.001410154509358108\n",
            "Batch 3320 done: batch loss 0.0015384003054350615\n",
            "Batch 3330 done: batch loss 0.001496490091085434\n",
            "Batch 3340 done: batch loss 0.001349670928902924\n",
            "Batch 3350 done: batch loss 0.0014260243624448776\n",
            "Batch 3360 done: batch loss 0.001431488199159503\n",
            "Batch 3370 done: batch loss 0.0014662096509709954\n",
            "Batch 3380 done: batch loss 0.0014020019443705678\n",
            "Batch 3390 done: batch loss 0.0014112096978351474\n",
            "Batch 3400 done: batch loss 0.0013837376609444618\n",
            "Batch 3410 done: batch loss 0.0014186585322022438\n",
            "Batch 3420 done: batch loss 0.00158616341650486\n",
            "Batch 3430 done: batch loss 0.0014037194196134806\n",
            "Batch 3440 done: batch loss 0.0014708137605339289\n",
            "Batch 3450 done: batch loss 0.001384005299769342\n",
            "Batch 3460 done: batch loss 0.0015282878885045648\n",
            "Batch 3470 done: batch loss 0.001437881961464882\n",
            "Batch 3480 done: batch loss 0.0014107725583016872\n",
            "Batch 3490 done: batch loss 0.001371860969811678\n",
            "Batch 3500 done: batch loss 0.0014839490177109838\n",
            "Batch 3510 done: batch loss 0.001338392379693687\n",
            "Batch 3520 done: batch loss 0.0015038549900054932\n",
            "Batch 3530 done: batch loss 0.0015341213438659906\n",
            "Batch 3540 done: batch loss 0.001338381553068757\n",
            "Batch 3550 done: batch loss 0.0013952488079667091\n",
            "Batch 3560 done: batch loss 0.0013458970934152603\n",
            "Batch 3570 done: batch loss 0.0014654862461611629\n",
            "Batch 3580 done: batch loss 0.0013283415464684367\n",
            "Batch 3590 done: batch loss 0.0013661781558766961\n",
            "Batch 3600 done: batch loss 0.00153815234079957\n",
            "Batch 3610 done: batch loss 0.0013540558284148574\n",
            "Batch 3620 done: batch loss 0.0013187455479055643\n",
            "Batch 3630 done: batch loss 0.0014543860452249646\n",
            "Batch 3640 done: batch loss 0.0014759539626538754\n",
            "Batch 3650 done: batch loss 0.0013321854639798403\n",
            "Batch 3660 done: batch loss 0.0013913821894675493\n",
            "Batch 3670 done: batch loss 0.0015382019337266684\n",
            "Batch 3680 done: batch loss 0.0013377121649682522\n",
            "Batch 3690 done: batch loss 0.0013871113769710064\n",
            "Batch 3700 done: batch loss 0.0013860752806067467\n",
            "Batch 3710 done: batch loss 0.001352293067611754\n",
            "Batch 3720 done: batch loss 0.0013704196317121387\n",
            "Batch 3730 done: batch loss 0.0013636148069053888\n",
            "Batch 3740 done: batch loss 0.001406138064339757\n",
            "Batch 3750 done: batch loss 0.0013637332012876868\n",
            "Batch 3760 done: batch loss 0.0013431007973849773\n",
            "Batch 3770 done: batch loss 0.0014054147759452462\n",
            "Batch 3780 done: batch loss 0.0013438184978440404\n",
            "Batch 3790 done: batch loss 0.0014751688577234745\n",
            "Batch 3800 done: batch loss 0.0013589842710644007\n",
            "Batch 3810 done: batch loss 0.0014286437071859837\n",
            "Batch 3820 done: batch loss 0.0014574258821085095\n",
            "Batch 3830 done: batch loss 0.0014324901858344674\n",
            "Batch 3840 done: batch loss 0.0013595318887382746\n",
            "Batch 3850 done: batch loss 0.0012797508388757706\n",
            "Batch 3860 done: batch loss 0.0014252319233492017\n",
            "Batch 3870 done: batch loss 0.0013443386415019631\n",
            "Batch 3880 done: batch loss 0.0013637645170092583\n",
            "Batch 3890 done: batch loss 0.0013461787020787597\n",
            "Batch 3900 done: batch loss 0.0013512715231627226\n",
            "Batch 3910 done: batch loss 0.001381946261972189\n",
            "Batch 3920 done: batch loss 0.0013190575409680605\n",
            "Batch 3930 done: batch loss 0.001384787610732019\n",
            "Batch 3940 done: batch loss 0.001375175779685378\n",
            "Batch 3950 done: batch loss 0.0013146521523594856\n",
            "Batch 3960 done: batch loss 0.0013598337536677718\n",
            "Batch 3970 done: batch loss 0.001325328485108912\n",
            "Batch 3980 done: batch loss 0.0013665248407050967\n",
            "Batch 3990 done: batch loss 0.0014389300486072898\n",
            "Batch 4000 done: batch loss 0.0013809660449624062\n",
            "Batch 4010 done: batch loss 0.0013655904913321137\n",
            "Batch 4020 done: batch loss 0.0013178384397178888\n",
            "Batch 4030 done: batch loss 0.0014452795730903745\n",
            "Batch 4040 done: batch loss 0.0013824024936184287\n",
            "Batch 4050 done: batch loss 0.0013144738040864468\n",
            "Batch 4060 done: batch loss 0.001443040557205677\n",
            "Batch 4070 done: batch loss 0.0012870661448687315\n",
            "Batch 4080 done: batch loss 0.0012775217182934284\n",
            "Batch 4090 done: batch loss 0.0013468668330460787\n",
            "Batch 4100 done: batch loss 0.0013099458301439881\n",
            "Batch 4110 done: batch loss 0.0013791377423331141\n",
            "Batch 4120 done: batch loss 0.0013295471435412765\n",
            "Batch 4130 done: batch loss 0.001354226260446012\n",
            "Batch 4140 done: batch loss 0.001185293192975223\n",
            "Batch 4150 done: batch loss 0.001299622468650341\n",
            "Batch 4160 done: batch loss 0.0013346635969355702\n",
            "Batch 4170 done: batch loss 0.0013076880713924766\n",
            "Batch 4180 done: batch loss 0.00128569093067199\n",
            "Batch 4190 done: batch loss 0.0013170226011425257\n",
            "Batch 4200 done: batch loss 0.001297083916142583\n",
            "Batch 4210 done: batch loss 0.0013385952915996313\n",
            "Batch 4220 done: batch loss 0.001246592146344483\n",
            "Batch 4230 done: batch loss 0.0013745005708187819\n",
            "Batch 4240 done: batch loss 0.0013485046802088618\n",
            "Batch 4250 done: batch loss 0.001320695853792131\n",
            "Batch 4260 done: batch loss 0.0013607597211375833\n",
            "Batch 4270 done: batch loss 0.001282595214433968\n",
            "Batch 4280 done: batch loss 0.0012544672936201096\n",
            "Batch 4290 done: batch loss 0.0012560738250613213\n",
            "Batch 4300 done: batch loss 0.0013356165727600455\n",
            "Batch 4310 done: batch loss 0.0013548958813771605\n",
            "Batch 4320 done: batch loss 0.0013699035625904799\n",
            "Batch 4330 done: batch loss 0.001325629884377122\n",
            "Batch 4340 done: batch loss 0.0014558780239894986\n",
            "Batch 4350 done: batch loss 0.0012235030299052596\n",
            "Batch 4360 done: batch loss 0.0012547897640615702\n",
            "Batch 4370 done: batch loss 0.0013257734244689345\n",
            "Batch 4380 done: batch loss 0.001339204260148108\n",
            "Batch 4390 done: batch loss 0.001401319052092731\n",
            "Batch 4400 done: batch loss 0.0013484901282936335\n",
            "Batch 4410 done: batch loss 0.0012813764624297619\n",
            "Batch 4420 done: batch loss 0.001320195966400206\n",
            "Batch 4430 done: batch loss 0.0012691383017227054\n",
            "Batch 4440 done: batch loss 0.0014169878559187055\n",
            "Batch 4450 done: batch loss 0.001277521369047463\n",
            "Batch 4460 done: batch loss 0.0012917851563543081\n",
            "Batch 4470 done: batch loss 0.0012675273464992642\n",
            "Batch 4480 done: batch loss 0.0014152162475511432\n",
            "Batch 4490 done: batch loss 0.0013306370237842202\n",
            "Batch 4500 done: batch loss 0.0012968588853254914\n",
            "Batch 4510 done: batch loss 0.0012563177151605487\n",
            "Batch 4520 done: batch loss 0.0013504285598173738\n",
            "Batch 4530 done: batch loss 0.0012998274760320783\n",
            "Batch 4540 done: batch loss 0.001218069694004953\n",
            "Batch 4550 done: batch loss 0.001272966037504375\n",
            "Batch 4560 done: batch loss 0.0013305472675710917\n",
            "Batch 4570 done: batch loss 0.0012155550066381693\n",
            "Batch 4580 done: batch loss 0.0012725836131721735\n",
            "Batch 4590 done: batch loss 0.0012596598826348782\n",
            "Batch 4600 done: batch loss 0.0012667439877986908\n",
            "Batch 4610 done: batch loss 0.001288459636271\n",
            "Batch 4620 done: batch loss 0.0012459661811590195\n",
            "Batch 4630 done: batch loss 0.0013225090224295855\n",
            "Batch 4640 done: batch loss 0.0012893390376120806\n",
            "Batch 4650 done: batch loss 0.0012539118761196733\n",
            "Batch 4660 done: batch loss 0.0012801802949979901\n",
            "Batch 4670 done: batch loss 0.00134151556994766\n",
            "Batch 4680 done: batch loss 0.001288596075028181\n",
            "Batch 4690 done: batch loss 0.0012529999949038029\n",
            "Batch 4700 done: batch loss 0.0012566152727231383\n",
            "Batch 4710 done: batch loss 0.0013097504852339625\n",
            "Batch 4720 done: batch loss 0.0013544777175411582\n",
            "Batch 4730 done: batch loss 0.0013282082509249449\n",
            "Batch 4740 done: batch loss 0.0012399418046697974\n",
            "Batch 4750 done: batch loss 0.0012431867653504014\n",
            "Batch 4760 done: batch loss 0.0013312353985384107\n",
            "Batch 4770 done: batch loss 0.001190235372632742\n",
            "Batch 4780 done: batch loss 0.0013080965727567673\n",
            "Batch 4790 done: batch loss 0.0012735308846458793\n",
            "Batch 4800 done: batch loss 0.0013235246296972036\n",
            "Batch 4810 done: batch loss 0.0013611746253445745\n",
            "Batch 4820 done: batch loss 0.0012499045114964247\n",
            "Batch 4830 done: batch loss 0.0013370153028517962\n",
            "Batch 4840 done: batch loss 0.0012556450674310327\n",
            "Batch 4850 done: batch loss 0.001257654745131731\n",
            "Batch 4860 done: batch loss 0.0012360330438241363\n",
            "Batch 4870 done: batch loss 0.0012298887595534325\n",
            "Batch 4880 done: batch loss 0.0013057957403361797\n",
            "Batch 4890 done: batch loss 0.0012088021030649543\n",
            "Batch 4900 done: batch loss 0.0011991618666797876\n",
            "Batch 4910 done: batch loss 0.0013388574589043856\n",
            "Batch 4920 done: batch loss 0.0012143086642026901\n",
            "Batch 4930 done: batch loss 0.0012289398582652211\n",
            "Batch 4940 done: batch loss 0.0012907962081953883\n",
            "Batch 4950 done: batch loss 0.0012004440650343895\n",
            "Batch 4960 done: batch loss 0.0012481246376410127\n",
            "Batch 4970 done: batch loss 0.0013022602070122957\n",
            "Batch 4980 done: batch loss 0.001247431500814855\n",
            "Batch 4990 done: batch loss 0.0012782672420144081\n",
            "Batch 5000 done: batch loss 0.0012774391798302531\n",
            "Batch 5010 done: batch loss 0.0012688904535025358\n",
            "Batch 5020 done: batch loss 0.0012948946096003056\n",
            "Batch 5030 done: batch loss 0.0012427096953615546\n",
            "Batch 5040 done: batch loss 0.001262972131371498\n",
            "Batch 5050 done: batch loss 0.0012215287424623966\n",
            "Batch 5060 done: batch loss 0.0012443962041288614\n",
            "Batch 5070 done: batch loss 0.001302284887060523\n",
            "Batch 5080 done: batch loss 0.001153884339146316\n",
            "Batch 5090 done: batch loss 0.001214888645336032\n",
            "Batch 5100 done: batch loss 0.0012779560638591647\n",
            "Batch 5110 done: batch loss 0.001220549107529223\n",
            "Batch 5120 done: batch loss 0.0012510152300819755\n",
            "Batch 5130 done: batch loss 0.0012262527598068118\n",
            "Batch 5140 done: batch loss 0.00124105263967067\n",
            "Batch 5150 done: batch loss 0.001243888633325696\n",
            "Batch 5160 done: batch loss 0.0012940028682351112\n",
            "Batch 5170 done: batch loss 0.0012157652527093887\n",
            "Batch 5180 done: batch loss 0.001271702698431909\n",
            "Batch 5190 done: batch loss 0.0012239546049386263\n",
            "Batch 5200 done: batch loss 0.0012279364746063948\n",
            "Batch 5210 done: batch loss 0.0012328899465501308\n",
            "Batch 5220 done: batch loss 0.0011599741410464048\n",
            "Batch 5230 done: batch loss 0.001285024918615818\n",
            "Batch 5240 done: batch loss 0.0012842967407777905\n",
            "Batch 5250 done: batch loss 0.0011721779592335224\n",
            "Batch 5260 done: batch loss 0.0012570936232805252\n",
            "Batch 5270 done: batch loss 0.0012256731279194355\n",
            "Batch 5280 done: batch loss 0.0011529376497492194\n",
            "Batch 5290 done: batch loss 0.0012020004214718938\n",
            "Batch 5300 done: batch loss 0.0012294092448428273\n",
            "Batch 5310 done: batch loss 0.0012277974747121334\n",
            "Batch 5320 done: batch loss 0.0011986710596829653\n",
            "Batch 5330 done: batch loss 0.0012565867509692907\n",
            "Batch 5340 done: batch loss 0.0011567476904019713\n",
            "Batch 5350 done: batch loss 0.0011872343020513654\n",
            "Batch 5360 done: batch loss 0.0012268852442502975\n",
            "Batch 5370 done: batch loss 0.0011791214346885681\n",
            "Batch 5380 done: batch loss 0.0011915932409465313\n",
            "Batch 5390 done: batch loss 0.001271047629415989\n",
            "Batch 5400 done: batch loss 0.001217292039655149\n",
            "Batch 5410 done: batch loss 0.001222125836648047\n",
            "Batch 5420 done: batch loss 0.0011804532259702682\n",
            "Batch 5430 done: batch loss 0.0012475642142817378\n",
            "Batch 5440 done: batch loss 0.0012531982501968741\n",
            "Batch 5450 done: batch loss 0.001199274673126638\n",
            "Batch 5460 done: batch loss 0.0012443028390407562\n",
            "Batch 5470 done: batch loss 0.0012193856528028846\n",
            "Batch 5480 done: batch loss 0.0012272984022274613\n",
            "Batch 5490 done: batch loss 0.0011848910944536328\n",
            "Batch 5500 done: batch loss 0.001234956900589168\n",
            "Batch 5510 done: batch loss 0.0012329338351264596\n",
            "Batch 5520 done: batch loss 0.0011857780627906322\n",
            "Batch 5530 done: batch loss 0.001269209897145629\n",
            "Batch 5540 done: batch loss 0.0011988631449639797\n",
            "Batch 5550 done: batch loss 0.0011392539599910378\n",
            "Batch 5560 done: batch loss 0.0012016200926154852\n",
            "Batch 5570 done: batch loss 0.0011618741555139422\n",
            "Batch 5580 done: batch loss 0.0011720893671736121\n",
            "Batch 5590 done: batch loss 0.0011638394789770246\n",
            "Batch 5600 done: batch loss 0.0011961839627474546\n",
            "Batch 5610 done: batch loss 0.0012262153904885054\n",
            "Batch 5620 done: batch loss 0.0012295317137613893\n",
            "Batch 5630 done: batch loss 0.0011451536556705832\n",
            "Batch 5640 done: batch loss 0.0012123899068683386\n",
            "Batch 5650 done: batch loss 0.001146610826253891\n",
            "Batch 5660 done: batch loss 0.0011956924572587013\n",
            "Batch 5670 done: batch loss 0.001139024505391717\n",
            "Batch 5680 done: batch loss 0.0012153369607403874\n",
            "Batch 5690 done: batch loss 0.0011681322939693928\n",
            "Batch 5700 done: batch loss 0.0012339574750512838\n",
            "Batch 5710 done: batch loss 0.0012429871130734682\n",
            "Batch 5720 done: batch loss 0.0012096344726160169\n",
            "Batch 5730 done: batch loss 0.001173721393570304\n",
            "Batch 5740 done: batch loss 0.001187408110126853\n",
            "Batch 5750 done: batch loss 0.0011801054934039712\n",
            "Batch 5760 done: batch loss 0.0011533132055774331\n",
            "Batch 5770 done: batch loss 0.0011023588012903929\n",
            "Batch 5780 done: batch loss 0.0011555679375305772\n",
            "Batch 5790 done: batch loss 0.0011882218532264233\n",
            "Batch 5800 done: batch loss 0.0012030811049044132\n",
            "Batch 5810 done: batch loss 0.0012205977691337466\n",
            "Batch 5820 done: batch loss 0.001138295978307724\n",
            "Batch 5830 done: batch loss 0.0012789074098691344\n",
            "Batch 5840 done: batch loss 0.001151320873759687\n",
            "Batch 5850 done: batch loss 0.0011780986096709967\n",
            "Batch 5860 done: batch loss 0.0011319533223286271\n",
            "Batch 5870 done: batch loss 0.0011452460894361138\n",
            "Batch 5880 done: batch loss 0.0011071385815739632\n",
            "Batch 5890 done: batch loss 0.0012486038031056523\n",
            "Batch 5900 done: batch loss 0.0011783166555687785\n",
            "Batch 5910 done: batch loss 0.0011080570984631777\n",
            "Batch 5920 done: batch loss 0.0011035918723791838\n",
            "Batch 5930 done: batch loss 0.001205988461151719\n",
            "Batch 5940 done: batch loss 0.0011666967766359448\n",
            "Batch 5950 done: batch loss 0.0011453571496531367\n",
            "Batch 5960 done: batch loss 0.0011376256588846445\n",
            "Batch 5970 done: batch loss 0.0012381712440401316\n",
            "Batch 5980 done: batch loss 0.0011518954997882247\n",
            "Batch 5990 done: batch loss 0.0011393929598852992\n",
            "Batch 6000 done: batch loss 0.0011072935303673148\n",
            "Batch 6010 done: batch loss 0.001208041561767459\n",
            "Batch 6020 done: batch loss 0.001172948395833373\n",
            "Batch 6030 done: batch loss 0.0011330005945637822\n",
            "Batch 6040 done: batch loss 0.0011776461033150554\n",
            "Batch 6050 done: batch loss 0.0011866322020068765\n",
            "Batch 6060 done: batch loss 0.0011279296595603228\n",
            "Batch 6070 done: batch loss 0.0011630839435383677\n",
            "Batch 6080 done: batch loss 0.001112914877012372\n",
            "Batch 6090 done: batch loss 0.001144766341894865\n",
            "Batch 6100 done: batch loss 0.0011727327946573496\n",
            "Batch 6110 done: batch loss 0.0011592335067689419\n",
            "Batch 6120 done: batch loss 0.0011138281552121043\n",
            "Batch 6130 done: batch loss 0.0011969269253313541\n",
            "Batch 6140 done: batch loss 0.001114772167056799\n",
            "Batch 6150 done: batch loss 0.0011372902663424611\n",
            "Batch 6160 done: batch loss 0.0011582018341869116\n",
            "Batch 6170 done: batch loss 0.001148772775195539\n",
            "Batch 6180 done: batch loss 0.0010726371547207236\n",
            "Batch 6190 done: batch loss 0.0012393610086292028\n",
            "Batch 6200 done: batch loss 0.0010725895408540964\n",
            "Batch 6210 done: batch loss 0.0011154990643262863\n",
            "Batch 6220 done: batch loss 0.0011994416126981378\n",
            "Batch 6230 done: batch loss 0.0011166674084961414\n",
            "Batch 6240 done: batch loss 0.0011339554330334067\n",
            "Batch 6250 done: batch loss 0.0011177773121744394\n",
            "Batch 6260 done: batch loss 0.0011286395601928234\n",
            "Batch 6270 done: batch loss 0.0011464880080893636\n",
            "Batch 6280 done: batch loss 0.0011411956511437893\n",
            "Batch 6290 done: batch loss 0.0011394821340218186\n",
            "Batch 6300 done: batch loss 0.0011413604952394962\n",
            "Batch 6310 done: batch loss 0.0011440636590123177\n",
            "Batch 6320 done: batch loss 0.0011436262866482139\n",
            "Batch 6330 done: batch loss 0.0011418142821639776\n",
            "Batch 6340 done: batch loss 0.0011331351706758142\n",
            "Batch 6350 done: batch loss 0.001122192246839404\n",
            "Batch 6360 done: batch loss 0.0012237848713994026\n",
            "Batch 6370 done: batch loss 0.0011102557182312012\n",
            "Batch 6380 done: batch loss 0.0011153180385008454\n",
            "Batch 6390 done: batch loss 0.0011162525042891502\n",
            "Batch 6400 done: batch loss 0.0011207006173208356\n",
            "Batch 6410 done: batch loss 0.0011177238775417209\n",
            "Batch 6420 done: batch loss 0.001097952015697956\n",
            "Batch 6430 done: batch loss 0.0011068155290558934\n",
            "Batch 6440 done: batch loss 0.0011387746781110764\n",
            "Batch 6450 done: batch loss 0.0011264978675171733\n",
            "Batch 6460 done: batch loss 0.0011784604284912348\n",
            "Batch 6470 done: batch loss 0.0011233219411224127\n",
            "Batch 6480 done: batch loss 0.0011190391378477216\n",
            "Batch 6490 done: batch loss 0.0011214847909286618\n",
            "Batch 6500 done: batch loss 0.0010987812420353293\n",
            "Batch 6510 done: batch loss 0.001147991744801402\n",
            "Batch 6520 done: batch loss 0.0011408959981054068\n",
            "Batch 6530 done: batch loss 0.001118752988986671\n",
            "Batch 6540 done: batch loss 0.0010651139309629798\n",
            "Batch 6550 done: batch loss 0.001108742319047451\n",
            "Batch 6560 done: batch loss 0.0011880402453243732\n",
            "Batch 6570 done: batch loss 0.001188354566693306\n",
            "Batch 6580 done: batch loss 0.001127315335907042\n",
            "Batch 6590 done: batch loss 0.0010727134067565203\n",
            "Batch 6600 done: batch loss 0.0011123360600322485\n",
            "Batch 6610 done: batch loss 0.0010609335731714964\n",
            "Batch 6620 done: batch loss 0.0011560233542695642\n",
            "Batch 6630 done: batch loss 0.0012418357655405998\n",
            "Batch 6640 done: batch loss 0.0010953102027997375\n",
            "Batch 6650 done: batch loss 0.00111996044870466\n",
            "Batch 6660 done: batch loss 0.0010762987658381462\n",
            "Batch 6670 done: batch loss 0.001088866381905973\n",
            "Batch 6680 done: batch loss 0.001201425096951425\n",
            "Batch 6690 done: batch loss 0.0010902327485382557\n",
            "Batch 6700 done: batch loss 0.0011317653115838766\n",
            "Batch 6710 done: batch loss 0.0011967472964897752\n",
            "Batch 6720 done: batch loss 0.0011795596219599247\n",
            "Batch 6730 done: batch loss 0.0011472266633063555\n",
            "Batch 6740 done: batch loss 0.001183612970635295\n",
            "Batch 6750 done: batch loss 0.0011047793086618185\n",
            "Batch 6760 done: batch loss 0.0010959821520373225\n",
            "Batch 6770 done: batch loss 0.0011277837911620736\n",
            "Batch 6780 done: batch loss 0.0010756219271570444\n",
            "Batch 6790 done: batch loss 0.0011527481256052852\n",
            "Batch 6800 done: batch loss 0.0011506987502798438\n",
            "Batch 6810 done: batch loss 0.0010296194814145565\n",
            "Batch 6820 done: batch loss 0.0010941334767267108\n",
            "Batch 6830 done: batch loss 0.0010863883653655648\n",
            "Batch 6840 done: batch loss 0.0011127813486382365\n",
            "Batch 6850 done: batch loss 0.0011506940936669707\n",
            "Batch 6860 done: batch loss 0.0010849222308024764\n",
            "Batch 6870 done: batch loss 0.0011706476798281074\n",
            "Batch 6880 done: batch loss 0.0010605270508676767\n",
            "Batch 6890 done: batch loss 0.00108584004919976\n",
            "Batch 6900 done: batch loss 0.0011166965123265982\n",
            "Batch 6910 done: batch loss 0.0011080987751483917\n",
            "Batch 6920 done: batch loss 0.0010800064774230123\n",
            "Batch 6930 done: batch loss 0.0010767867788672447\n",
            "Batch 6940 done: batch loss 0.0010790120577439666\n",
            "Batch 6950 done: batch loss 0.001107920310460031\n",
            "Batch 6960 done: batch loss 0.001043545315042138\n",
            "Batch 6970 done: batch loss 0.0010702863801270723\n",
            "Batch 6980 done: batch loss 0.0011204869952052832\n",
            "Batch 6990 done: batch loss 0.0011678256560117006\n",
            "Batch 7000 done: batch loss 0.0011294942814856768\n",
            "Batch 7010 done: batch loss 0.0011221994645893574\n",
            "Batch 7020 done: batch loss 0.0011015716008841991\n",
            "Batch 7030 done: batch loss 0.0011093083303421736\n",
            "Batch 7040 done: batch loss 0.0010515189496800303\n",
            "Batch 7050 done: batch loss 0.001049207174219191\n",
            "Batch 7060 done: batch loss 0.0010692242067307234\n",
            "Batch 7070 done: batch loss 0.0011317998869344592\n",
            "Batch 7080 done: batch loss 0.0011339804623275995\n",
            "Batch 7090 done: batch loss 0.0011018338846042752\n",
            "Batch 7100 done: batch loss 0.0010727442568168044\n",
            "Batch 7110 done: batch loss 0.001057776971720159\n",
            "Batch 7120 done: batch loss 0.0010180415119975805\n",
            "Batch 7130 done: batch loss 0.001062524737790227\n",
            "Batch 7140 done: batch loss 0.001089105848222971\n",
            "Batch 7150 done: batch loss 0.0011669460218399763\n",
            "Batch 7160 done: batch loss 0.0011361240176483989\n",
            "Batch 7170 done: batch loss 0.001075083389878273\n",
            "Batch 7180 done: batch loss 0.0010744408937171102\n",
            "Batch 7190 done: batch loss 0.0010351124219596386\n",
            "Batch 7200 done: batch loss 0.001094717881642282\n",
            "Batch 7210 done: batch loss 0.001090515055693686\n",
            "Batch 7220 done: batch loss 0.0010628598975017667\n",
            "Batch 7230 done: batch loss 0.001077300519682467\n",
            "Batch 7240 done: batch loss 0.0010877068853005767\n",
            "Batch 7250 done: batch loss 0.001064657117240131\n",
            "Batch 7260 done: batch loss 0.0010591426398605108\n",
            "Batch 7270 done: batch loss 0.0010740113211795688\n",
            "Batch 7280 done: batch loss 0.0010756674455478787\n",
            "Batch 7290 done: batch loss 0.0010081419022753835\n",
            "Batch 7300 done: batch loss 0.0011247163638472557\n",
            "Batch 7310 done: batch loss 0.0010653900681063533\n",
            "Batch 7320 done: batch loss 0.0010560686932876706\n",
            "Batch 7330 done: batch loss 0.001038354355841875\n",
            "Batch 7340 done: batch loss 0.0010518700582906604\n",
            "Batch 7350 done: batch loss 0.001103141694329679\n",
            "Batch 7360 done: batch loss 0.0010904672089964151\n",
            "Batch 7370 done: batch loss 0.001064406824298203\n",
            "Batch 7380 done: batch loss 0.0011265028733760118\n",
            "Batch 7390 done: batch loss 0.0010942902881652117\n",
            "Batch 7400 done: batch loss 0.0010546038392931223\n",
            "Batch 7410 done: batch loss 0.0011123199947178364\n",
            "Batch 7420 done: batch loss 0.0010106945410370827\n",
            "Batch 7430 done: batch loss 0.0010276634711772203\n",
            "Batch 7440 done: batch loss 0.0010669847251847386\n",
            "Batch 7450 done: batch loss 0.0010952040320262313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1M8GSkh-GVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "0288c10e-778f-4ca1-9936-6de94efe5fa2"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=1001, batch_size=16)\n",
        "\n",
        "my_model.test(train_imgs[0:16])\n"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is False\n",
            "INFO:tensorflow:Restoring parameters from model/latest1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from model/latest1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryasiSMz-GdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmakTnm-GjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgTCmum-Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1B9Ick-GnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}