{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saliency_system.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/Saliency_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwVzzyrpD2R",
        "colab_type": "code",
        "outputId": "6d9560e6-d6a5-4874-ed64-7df4d4a870e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import zipfile\n",
        "from google.colab import files, drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK13Xnq_pG2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/CV2 exercies/Archive.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EayiS3g6pJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "from __future__ import division\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import KFold"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBWzAZLlpLcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT= '/tmp'\n",
        "NUM_IMAGES = 1200\n",
        "VAL_NUM_IMAGES = 400\n",
        "\n",
        "def load_train_data():\n",
        "  training_img_directory = ROOT+'/data/train/images'\n",
        "  training_fixation_directory = ROOT+'/data/train/fixations'\n",
        "  val_img_directory = ROOT+'/data/val/images'\n",
        "  val_fixation_directory = ROOT+'/data/val/fixations'\n",
        "  train_imgs = np.zeros((NUM_IMAGES + VAL_NUM_IMAGES, 180, 320, 3), dtype=np.uint8)\n",
        "  train_fixations = np.zeros((NUM_IMAGES + VAL_NUM_IMAGES, 180, 320, 1), dtype=np.uint8)\n",
        "  \n",
        "  for i in range(1, NUM_IMAGES + 1):\n",
        "    img_file = os.path.join(training_img_directory, '{:04d}.jpg'.format(i))\n",
        "    fixation_file = os.path.join(training_fixation_directory, '{:04d}.jpg'.format(i))\n",
        "    train_imgs[i-1] = imageio.imread(img_file)\n",
        "    fixation = imageio.imread(fixation_file)\n",
        "    train_fixations[i-1] = np.expand_dims(fixation, -1) # adds singleton dimension so fixation size is (180,320,1)\n",
        "  \n",
        "  for j in range(i + 1, i + VAL_NUM_IMAGES + 1):\n",
        "    img_file = os.path.join(val_img_directory, '{:04d}.jpg'.format(j))\n",
        "    fixation_file = os.path.join(val_fixation_directory, '{:04d}.jpg'.format(j))\n",
        "    train_imgs[j-1] = imageio.imread(img_file)\n",
        "    fixation = imageio.imread(fixation_file)\n",
        "    train_fixations[j-1] = np.expand_dims(fixation, -1) # adds singleton dimension so fixation size is (180,320,1)\n",
        "    \n",
        "  return train_imgs, train_fixations\n",
        "\n",
        "# Generator function will output one (image, target) tuple at a time,\n",
        "# and shuffle the data for each new epoch\n",
        "def data_generator(imgs, targets):\n",
        "\twhile True: # produce new epochs forever\n",
        "\t\t# Shuffle the data for this epoch\n",
        "\t\tidx = np.arange(imgs.shape[0])\n",
        "\t\tnp.random.shuffle(idx)\n",
        "\n",
        "\t\timgs = imgs[idx]\n",
        "\t\ttargets = targets[idx]\n",
        "\t\tfor i in range(imgs.shape[0]):\n",
        "\t\t\tyield imgs[i], targets[i]\n",
        "\n",
        "def get_batch_from_generator(gen, batchsize):\n",
        "\tbatch_imgs = []\n",
        "\tbatch_fixations = []\n",
        "\tfor i in range(batchsize):\n",
        "\t\timg, target = gen.__next__()\n",
        "\t\tbatch_imgs.append(img)\n",
        "\t\tbatch_fixations.append(target)\n",
        "\treturn np.array(batch_imgs), np.array(batch_fixations)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFi73W_n0rEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tf.reset_default_graph()\n",
        "\n",
        "'''\n",
        "Model class\n",
        "'''\n",
        "\n",
        "class saliencyModel:\n",
        "  def __init__(self, model_weights=None, learning_rate=1e-4, batch_size=32, num_epochs=100, prior_downsampling_factor=10):\n",
        "    self.dir_path = 'drive/My Drive/Colab Notebooks/CV2 exercies/'\n",
        "    self.model_path = self.dir_path+'model_cptk/my-model1'\n",
        "    self.lr_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_epochs = num_epochs\n",
        "    self.prior_downsampling_factor = prior_downsampling_factor\n",
        "    self.input_h = 180\n",
        "    self.input_w = 320\n",
        "    self.prior_h = self.input_h /(2 * prior_downsampling_factor)\n",
        "    self.prior_w = self.input_w / (2 * prior_downsampling_factor)\n",
        "    self.reg_lambda = 1 / (self.prior_h * self.prior_w)\n",
        "    self.input_images_placeholder = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 3])\n",
        "    self.target_images_placeholder = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 1])\n",
        "    \n",
        "    if model_weights is not None:\n",
        "      self.load_weights(model_weights)\n",
        "    \n",
        "  def load_weights(self, model_weights):\n",
        "    vgg_weight_file = model_weights\n",
        "    self.weights = np.load(vgg_weight_file)\n",
        "  \n",
        "  def setup(self, mode= 'Train'):\n",
        "    \n",
        "    with tf.name_scope('preprocessing') as scope:\n",
        "      input_imgs = tf.image.convert_image_dtype(self.input_images_placeholder, tf.float32) * 255\n",
        "      fixations_normalized = tf.image.convert_image_dtype(self.target_images_placeholder, tf.float32)\n",
        "      mean = tf.constant([123.68 , 116.779 , 103.939], dtype = tf.float32, shape =[1,1,1,3], name ='img_mean')\n",
        "      imgs_normalized = input_imgs - mean\n",
        "\n",
        "    with tf.name_scope('conv1_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(imgs_normalized, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv1_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool1') as scope:\n",
        "      pool = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv2_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv2_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool2') as scope:\n",
        "      pool2 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv3_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool3') as scope:\n",
        "      pool3 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv4_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv4_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv4_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('pool4') as scope:\n",
        "      pool4 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "    \n",
        "    with tf.name_scope('conv5_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv5_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv5_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      conv5_3 = act\n",
        "      \n",
        "    with tf.name_scope('concat_featuremaps') as scope:\n",
        "      concatenated_feature_maps = tf.concat([pool2, pool3, pool4, conv5_3], axis=3)\n",
        "      print('IN_TRAINING_MODE is',mode =='Train')\n",
        "      regularized_feature_maps = tf.layers.dropout(concatenated_feature_maps, rate=0.5, training= mode =='Train')\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv1') as scope:\n",
        "      my_regularizer = tf.contrib.layers.l2_regularizer(1e-5)\n",
        "      act_featuremaps_conv1 = tf.layers.conv2d(regularized_feature_maps, filters=64, kernel_size=(3,3), padding='SAME', activation=tf.nn.relu, name='featuremaps_conv1', kernel_regularizer = my_regularizer, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv2') as scope:\n",
        "      act_featuremaps_conv2 = tf.layers.conv2d(act_featuremaps_conv1, filters=1, kernel_size=(1,1), activation=tf.nn.relu, name='featuremaps_conv2', kernel_regularizer = my_regularizer, reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('Learned_prior') as scope:\n",
        "      prior_shape = (1, self.prior_h, self.prior_w,1)\n",
        "      prior = tf.Variable(tf.ones(prior_shape), name=\"prior\", trainable=True)\n",
        "      upsampled_prior = tf.image.resize_bilinear(prior, size=act_featuremaps_conv2.shape[1:3])\n",
        "      saliency_mixed = tf.multiply(act_featuremaps_conv2, upsampled_prior)\n",
        "      saliency_raw = tf.nn.relu(saliency_mixed)\n",
        "      \n",
        "    with tf.name_scope('loss') as scope:\n",
        "      upsampled_saliency = tf.image.resize_bilinear(saliency_raw, size=fixations_normalized.shape[1:3])\n",
        "      # normalize saliency\n",
        "      max_value_per_image = tf.reduce_max(upsampled_saliency, axis=[1,2,3], keepdims=True)\n",
        "      predicted_saliency = (upsampled_saliency / max_value_per_image)\n",
        "      \n",
        "      # Loss function from Cornia et al. (2016) [with higher weight for salient pixels]\n",
        "      alpha = 1.01\n",
        "      weight = 1.0 / (alpha - fixations_normalized)\n",
        "      loss = tf.losses.mean_squared_error(labels=fixations_normalized, \n",
        "                        predictions=predicted_saliency, \n",
        "                        weights=weight)\n",
        "      regularizer = tf.nn.l2_loss(1 - prior)\n",
        "      loss += tf.reduce_mean(self.reg_lambda * regularizer)\n",
        "      l2_loss = tf.losses.get_regularization_loss() \n",
        "      loss += l2_loss\n",
        "      \n",
        "    return predicted_saliency, loss\n",
        "    \n",
        "  def train(self, t_imgs, t_fixations):\n",
        "    \n",
        "    pred_val ,loss_op_val = self.setup(mode='Test')\n",
        "    pred ,loss_op = self.setup(mode='Train')\n",
        "    training_loss = []\n",
        "    val_loss = []\n",
        "    \n",
        "    # Optimizer settings from Cornia et al. (2016) [except for decay]\n",
        "    optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr_rate, momentum=0.4, use_nesterov=True)\n",
        "    minimize_op = optimizer.minimize(loss_op)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      saver.restore(sess, \"/model/latest1-900\")\n",
        "      for epoch_ind in range(0, self.num_epochs, 5):\n",
        "        kf = KFold(n_splits=5)\n",
        "        for train_index, val_index in kf.split(t_imgs):\n",
        "          train_gen = data_generator(t_imgs[train_index], t_fixations[train_index])\n",
        "          val_gen = data_generator(t_imgs[val_index], t_fixations[val_index])\n",
        "          \n",
        "          num_batches = len(train_index) // self.batch_size\n",
        "          for b in range(num_batches):\n",
        "            batch_imgs, batch_fixations = get_batch_from_generator(train_gen, self.batch_size)\n",
        "            predication, batch_loss,_ = sess.run([pred, loss_op, minimize_op], feed_dict={self.input_images_placeholder: batch_imgs, self.target_images_placeholder: batch_fixations})\n",
        "          training_loss.append(batch_loss)\n",
        "          \n",
        "        if epoch_ind % 5 == 0:\n",
        "          #writer.add_summary(l_summary, global_step=b)\n",
        "          print('epoch {0} done: TRAINING batch loss {1}'.format(epoch_ind, batch_loss))\n",
        "            \n",
        "        if epoch_ind % 100 == 0:\n",
        "          val_batch_imgs, val_batch_fixations = get_batch_from_generator(val_gen, self.batch_size)\n",
        "          prediction_val, batch_loss_val = sess.run([pred_val, loss_op_val], feed_dict={self.input_images_placeholder: val_batch_imgs, self.target_images_placeholder: val_batch_fixations})\n",
        "          print('epoch {0} VALIDATION batch loss {1}'.format(epoch_ind, batch_loss_val))\n",
        "          val_loss.append(batch_loss_val)\n",
        "          \n",
        "        if epoch_ind % 100 == 0 and epoch_ind != 0:\n",
        "          save_path = saver.save(sess, 'model/latest1', global_step=epoch_ind)\n",
        "      save_path = saver.save(sess, 'model/latest1')\n",
        "      \n",
        "      \n",
        "  def test(self, test_imgs):\n",
        "    pred_saliency, l = self.setup(mode='Test')\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      saver.restore(sess, 'model/latest1')\n",
        "      saliency = sess.run(pred_saliency, feed_dict={self.input_images_placeholder: test_imgs})\n",
        "      for i in range(len(test_imgs)):\n",
        "        print('saving images')\n",
        "        saliency_img = sess.run(tf.image.convert_image_dtype(saliency[i], tf.uint8))\n",
        "        imageio.imwrite(str(i)+'.jpg', saliency_img)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhmQ0g-1-GKV",
        "colab_type": "code",
        "outputId": "5f9c188c-238c-47c2-bdf0-dc410834eecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3386
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-9, num_epochs=601, batch_size=32)\n",
        "\n",
        "my_model.train(train_imgs, train_fixations)\n",
        "#my_model.train(train_imgs, train_fixations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is False\n",
            "IN_TRAINING_MODE is True\n",
            "epoch 0 done: TRAINING batch loss 0.020769167691469193\n",
            "epoch 0 VALIDATION batch loss 0.01909538358449936\n",
            "epoch 5 done: TRAINING batch loss 0.01939050294458866\n",
            "epoch 10 done: TRAINING batch loss 0.019853781908750534\n",
            "epoch 15 done: TRAINING batch loss 0.021648284047842026\n",
            "epoch 20 done: TRAINING batch loss 0.023059943690896034\n",
            "epoch 25 done: TRAINING batch loss 0.021401621401309967\n",
            "epoch 30 done: TRAINING batch loss 0.01946244202554226\n",
            "epoch 35 done: TRAINING batch loss 0.01944793201982975\n",
            "epoch 40 done: TRAINING batch loss 0.021813172847032547\n",
            "epoch 45 done: TRAINING batch loss 0.021766619756817818\n",
            "epoch 50 done: TRAINING batch loss 0.019501768052577972\n",
            "epoch 55 done: TRAINING batch loss 0.020478906109929085\n",
            "epoch 60 done: TRAINING batch loss 0.019124040380120277\n",
            "epoch 65 done: TRAINING batch loss 0.02035260573029518\n",
            "epoch 70 done: TRAINING batch loss 0.019317541271448135\n",
            "epoch 75 done: TRAINING batch loss 0.01885630376636982\n",
            "epoch 80 done: TRAINING batch loss 0.02056903950870037\n",
            "epoch 85 done: TRAINING batch loss 0.017341600731015205\n",
            "epoch 90 done: TRAINING batch loss 0.018989713862538338\n",
            "epoch 95 done: TRAINING batch loss 0.020914189517498016\n",
            "epoch 100 done: TRAINING batch loss 0.02070208080112934\n",
            "epoch 100 VALIDATION batch loss 0.020006947219371796\n",
            "epoch 105 done: TRAINING batch loss 0.018682152032852173\n",
            "epoch 110 done: TRAINING batch loss 0.020505569875240326\n",
            "epoch 115 done: TRAINING batch loss 0.019934650510549545\n",
            "epoch 120 done: TRAINING batch loss 0.019964417442679405\n",
            "epoch 125 done: TRAINING batch loss 0.01960049755871296\n",
            "epoch 130 done: TRAINING batch loss 0.021295709535479546\n",
            "epoch 135 done: TRAINING batch loss 0.020251739770174026\n",
            "epoch 140 done: TRAINING batch loss 0.01871589571237564\n",
            "epoch 145 done: TRAINING batch loss 0.01881754770874977\n",
            "epoch 150 done: TRAINING batch loss 0.01718377135694027\n",
            "epoch 155 done: TRAINING batch loss 0.02005893737077713\n",
            "epoch 160 done: TRAINING batch loss 0.01860414817929268\n",
            "epoch 165 done: TRAINING batch loss 0.01723005622625351\n",
            "epoch 170 done: TRAINING batch loss 0.019625641405582428\n",
            "epoch 175 done: TRAINING batch loss 0.01895136386156082\n",
            "epoch 180 done: TRAINING batch loss 0.019679775461554527\n",
            "epoch 185 done: TRAINING batch loss 0.02233358472585678\n",
            "epoch 190 done: TRAINING batch loss 0.018301619216799736\n",
            "epoch 195 done: TRAINING batch loss 0.01639648899435997\n",
            "epoch 200 done: TRAINING batch loss 0.02066652849316597\n",
            "epoch 200 VALIDATION batch loss 0.016541659832000732\n",
            "epoch 205 done: TRAINING batch loss 0.01822911761701107\n",
            "epoch 210 done: TRAINING batch loss 0.01830812729895115\n",
            "epoch 215 done: TRAINING batch loss 0.01932580955326557\n",
            "epoch 220 done: TRAINING batch loss 0.018528148531913757\n",
            "epoch 225 done: TRAINING batch loss 0.019429577514529228\n",
            "epoch 230 done: TRAINING batch loss 0.02098504826426506\n",
            "epoch 235 done: TRAINING batch loss 0.017282402142882347\n",
            "epoch 240 done: TRAINING batch loss 0.01820322684943676\n",
            "epoch 245 done: TRAINING batch loss 0.01907748356461525\n",
            "epoch 250 done: TRAINING batch loss 0.019035514444112778\n",
            "epoch 255 done: TRAINING batch loss 0.017488688230514526\n",
            "epoch 260 done: TRAINING batch loss 0.019735729321837425\n",
            "epoch 265 done: TRAINING batch loss 0.0182795450091362\n",
            "epoch 270 done: TRAINING batch loss 0.017820723354816437\n",
            "epoch 275 done: TRAINING batch loss 0.019685734063386917\n",
            "epoch 280 done: TRAINING batch loss 0.019217390567064285\n",
            "epoch 285 done: TRAINING batch loss 0.01765390858054161\n",
            "epoch 290 done: TRAINING batch loss 0.019349785521626472\n",
            "epoch 295 done: TRAINING batch loss 0.016942713409662247\n",
            "epoch 300 done: TRAINING batch loss 0.02066882699728012\n",
            "epoch 300 VALIDATION batch loss 0.018378473818302155\n",
            "epoch 305 done: TRAINING batch loss 0.020701264962553978\n",
            "epoch 310 done: TRAINING batch loss 0.019993465393781662\n",
            "epoch 315 done: TRAINING batch loss 0.018651233986020088\n",
            "epoch 320 done: TRAINING batch loss 0.020895283669233322\n",
            "epoch 325 done: TRAINING batch loss 0.016018282622098923\n",
            "epoch 330 done: TRAINING batch loss 0.01964454911649227\n",
            "epoch 335 done: TRAINING batch loss 0.019022421911358833\n",
            "epoch 340 done: TRAINING batch loss 0.01776827499270439\n",
            "epoch 345 done: TRAINING batch loss 0.01614890620112419\n",
            "epoch 350 done: TRAINING batch loss 0.016624409705400467\n",
            "epoch 355 done: TRAINING batch loss 0.018397634848952293\n",
            "epoch 360 done: TRAINING batch loss 0.01949850283563137\n",
            "epoch 365 done: TRAINING batch loss 0.01920466497540474\n",
            "epoch 370 done: TRAINING batch loss 0.020082954317331314\n",
            "epoch 375 done: TRAINING batch loss 0.02065897546708584\n",
            "epoch 380 done: TRAINING batch loss 0.02042417600750923\n",
            "epoch 385 done: TRAINING batch loss 0.01705458201467991\n",
            "epoch 390 done: TRAINING batch loss 0.017386140301823616\n",
            "epoch 395 done: TRAINING batch loss 0.01884211041033268\n",
            "epoch 400 done: TRAINING batch loss 0.016927961260080338\n",
            "epoch 400 VALIDATION batch loss 0.016502229496836662\n",
            "epoch 405 done: TRAINING batch loss 0.018677987158298492\n",
            "epoch 410 done: TRAINING batch loss 0.01899152435362339\n",
            "epoch 415 done: TRAINING batch loss 0.019785357639193535\n",
            "epoch 420 done: TRAINING batch loss 0.019084403291344643\n",
            "epoch 425 done: TRAINING batch loss 0.01918788067996502\n",
            "epoch 430 done: TRAINING batch loss 0.019385050982236862\n",
            "epoch 435 done: TRAINING batch loss 0.019928785040974617\n",
            "epoch 440 done: TRAINING batch loss 0.01919042505323887\n",
            "epoch 445 done: TRAINING batch loss 0.019211992621421814\n",
            "epoch 450 done: TRAINING batch loss 0.018386078998446465\n",
            "epoch 455 done: TRAINING batch loss 0.017503172159194946\n",
            "epoch 460 done: TRAINING batch loss 0.01781490631401539\n",
            "epoch 465 done: TRAINING batch loss 0.01794038712978363\n",
            "epoch 470 done: TRAINING batch loss 0.01683245785534382\n",
            "epoch 475 done: TRAINING batch loss 0.018610956147313118\n",
            "epoch 480 done: TRAINING batch loss 0.018748078495264053\n",
            "epoch 485 done: TRAINING batch loss 0.019185485318303108\n",
            "epoch 490 done: TRAINING batch loss 0.01935637928545475\n",
            "epoch 495 done: TRAINING batch loss 0.0191396065056324\n",
            "epoch 500 done: TRAINING batch loss 0.01841244101524353\n",
            "epoch 500 VALIDATION batch loss 0.014865586534142494\n",
            "epoch 505 done: TRAINING batch loss 0.019030123949050903\n",
            "epoch 510 done: TRAINING batch loss 0.019807282835245132\n",
            "epoch 515 done: TRAINING batch loss 0.015894733369350433\n",
            "epoch 520 done: TRAINING batch loss 0.018742229789495468\n",
            "epoch 525 done: TRAINING batch loss 0.017012907192111015\n",
            "epoch 530 done: TRAINING batch loss 0.018548697233200073\n",
            "epoch 535 done: TRAINING batch loss 0.019163114950060844\n",
            "epoch 540 done: TRAINING batch loss 0.01911744847893715\n",
            "epoch 545 done: TRAINING batch loss 0.02126186341047287\n",
            "epoch 550 done: TRAINING batch loss 0.019310669973492622\n",
            "epoch 555 done: TRAINING batch loss 0.019613677635788918\n",
            "epoch 560 done: TRAINING batch loss 0.020337557420134544\n",
            "epoch 565 done: TRAINING batch loss 0.017521772533655167\n",
            "epoch 570 done: TRAINING batch loss 0.017924431711435318\n",
            "epoch 575 done: TRAINING batch loss 0.01632702723145485\n",
            "epoch 580 done: TRAINING batch loss 0.01906311698257923\n",
            "epoch 585 done: TRAINING batch loss 0.017044052481651306\n",
            "epoch 590 done: TRAINING batch loss 0.018800320103764534\n",
            "epoch 595 done: TRAINING batch loss 0.01625312678515911\n",
            "epoch 600 done: TRAINING batch loss 0.017541345208883286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0614 12:50:54.475792 139650139862912 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 600 VALIDATION batch loss 0.01582648605108261\n",
            "epoch 605 done: TRAINING batch loss 0.017552973702549934\n",
            "epoch 610 done: TRAINING batch loss 0.016805170103907585\n",
            "epoch 615 done: TRAINING batch loss 0.018477067351341248\n",
            "epoch 620 done: TRAINING batch loss 0.01615270972251892\n",
            "epoch 625 done: TRAINING batch loss 0.019445078447461128\n",
            "epoch 630 done: TRAINING batch loss 0.0173631701618433\n",
            "epoch 635 done: TRAINING batch loss 0.016551600769162178\n",
            "epoch 640 done: TRAINING batch loss 0.01913217268884182\n",
            "epoch 645 done: TRAINING batch loss 0.018713420256972313\n",
            "epoch 650 done: TRAINING batch loss 0.01713373512029648\n",
            "epoch 655 done: TRAINING batch loss 0.01899234764277935\n",
            "epoch 660 done: TRAINING batch loss 0.01686106249690056\n",
            "epoch 665 done: TRAINING batch loss 0.019459355622529984\n",
            "epoch 670 done: TRAINING batch loss 0.019476836547255516\n",
            "epoch 675 done: TRAINING batch loss 0.016698164865374565\n",
            "epoch 680 done: TRAINING batch loss 0.016630826517939568\n",
            "epoch 685 done: TRAINING batch loss 0.01864081434905529\n",
            "epoch 690 done: TRAINING batch loss 0.017111487686634064\n",
            "epoch 695 done: TRAINING batch loss 0.018300417810678482\n",
            "epoch 700 done: TRAINING batch loss 0.016793809831142426\n",
            "epoch 700 VALIDATION batch loss 0.018489373847842216\n",
            "epoch 705 done: TRAINING batch loss 0.017690742388367653\n",
            "epoch 710 done: TRAINING batch loss 0.01572468690574169\n",
            "epoch 715 done: TRAINING batch loss 0.017719553783535957\n",
            "epoch 720 done: TRAINING batch loss 0.017481349408626556\n",
            "epoch 725 done: TRAINING batch loss 0.017654554918408394\n",
            "epoch 730 done: TRAINING batch loss 0.017269158735871315\n",
            "epoch 735 done: TRAINING batch loss 0.016678016632795334\n",
            "epoch 740 done: TRAINING batch loss 0.017455529421567917\n",
            "epoch 745 done: TRAINING batch loss 0.019552264362573624\n",
            "epoch 750 done: TRAINING batch loss 0.019537057727575302\n",
            "epoch 755 done: TRAINING batch loss 0.017344387248158455\n",
            "epoch 760 done: TRAINING batch loss 0.015605525113642216\n",
            "epoch 765 done: TRAINING batch loss 0.017636658623814583\n",
            "epoch 770 done: TRAINING batch loss 0.02041683904826641\n",
            "epoch 775 done: TRAINING batch loss 0.020149752497673035\n",
            "epoch 780 done: TRAINING batch loss 0.017107093706727028\n",
            "epoch 785 done: TRAINING batch loss 0.017440583556890488\n",
            "epoch 790 done: TRAINING batch loss 0.018539728596806526\n",
            "epoch 795 done: TRAINING batch loss 0.016520541161298752\n",
            "epoch 800 done: TRAINING batch loss 0.01770239695906639\n",
            "epoch 800 VALIDATION batch loss 0.015143085271120071\n",
            "epoch 805 done: TRAINING batch loss 0.01636657491326332\n",
            "epoch 810 done: TRAINING batch loss 0.0173491220921278\n",
            "epoch 815 done: TRAINING batch loss 0.016820505261421204\n",
            "epoch 820 done: TRAINING batch loss 0.017503920942544937\n",
            "epoch 825 done: TRAINING batch loss 0.01617887243628502\n",
            "epoch 830 done: TRAINING batch loss 0.019018473103642464\n",
            "epoch 835 done: TRAINING batch loss 0.01608220487833023\n",
            "epoch 840 done: TRAINING batch loss 0.016251595690846443\n",
            "epoch 845 done: TRAINING batch loss 0.017054017633199692\n",
            "epoch 850 done: TRAINING batch loss 0.015798406675457954\n",
            "epoch 855 done: TRAINING batch loss 0.01580350659787655\n",
            "epoch 860 done: TRAINING batch loss 0.018662303686141968\n",
            "epoch 865 done: TRAINING batch loss 0.01843228191137314\n",
            "epoch 870 done: TRAINING batch loss 0.018304402008652687\n",
            "epoch 875 done: TRAINING batch loss 0.01753227598965168\n",
            "epoch 880 done: TRAINING batch loss 0.01737569458782673\n",
            "epoch 885 done: TRAINING batch loss 0.0174013189971447\n",
            "epoch 890 done: TRAINING batch loss 0.01660207472741604\n",
            "epoch 895 done: TRAINING batch loss 0.018606744706630707\n",
            "epoch 900 done: TRAINING batch loss 0.019301816821098328\n",
            "epoch 900 VALIDATION batch loss 0.019051076844334602\n",
            "epoch 905 done: TRAINING batch loss 0.018445409834384918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1M8GSkh-GVC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=1001, batch_size=16)\n",
        "\n",
        "my_model.test(train_imgs[0:16])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmakTnm-GjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}