{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Saliency_system.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nilesh0109/CV2_SoSe_19/blob/master/Saliency_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxwVzzyrpD2R",
        "colab_type": "code",
        "outputId": "d0e85e98-2948-4d25-abb3-2c92275296f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK13Xnq_pG2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/Colab Notebooks/CV2 exercies/Archive.zip\", 'r')\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EayiS3g6pJ8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import imageio\n",
        "from __future__ import division\n",
        "from skimage.transform import resize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBWzAZLlpLcj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ROOT= '/tmp'\n",
        "NUM_IMAGES = 1200\n",
        "def load_train_data():\n",
        "\ttraining_img_directory = ROOT+'/data/train/images'\n",
        "\ttraining_fixation_directory = ROOT+'/data/train/fixations'\n",
        "\n",
        "\ttrain_imgs = np.zeros((NUM_IMAGES, 180, 320, 3), dtype=np.uint8)\n",
        "\ttrain_fixations = np.zeros((NUM_IMAGES, 180, 320, 1), dtype=np.uint8)\n",
        "\tfor i in range(1, NUM_IMAGES + 1):\n",
        "\t\timg_file = os.path.join(training_img_directory, '{:04d}.jpg'.format(i))\n",
        "\t\tfixation_file = os.path.join(training_fixation_directory, '{:04d}.jpg'.format(i))\n",
        "\t\ttrain_imgs[i-1] = imageio.imread(img_file)\n",
        "\t\tfixation = imageio.imread(fixation_file)\n",
        "\t\ttrain_fixations[i-1] = np.expand_dims(fixation, -1) # adds singleton dimension so fixation size is (180,320,1)\n",
        "\t\n",
        "\treturn train_imgs, train_fixations\n",
        "\n",
        "# Generator function will output one (image, target) tuple at a time,\n",
        "# and shuffle the data for each new epoch\n",
        "def data_generator(imgs, targets):\n",
        "\twhile True: # produce new epochs forever\n",
        "\t\t# Shuffle the data for this epoch\n",
        "\t\tidx = np.arange(imgs.shape[0])\n",
        "\t\tnp.random.shuffle(idx)\n",
        "\n",
        "\t\timgs = imgs[idx]\n",
        "\t\ttargets = targets[idx]\n",
        "\t\tfor i in range(imgs.shape[0]):\n",
        "\t\t\tyield imgs[i], targets[i]\n",
        "\n",
        "def get_batch_from_generator(gen, batchsize):\n",
        "\tbatch_imgs = []\n",
        "\tbatch_fixations = []\n",
        "\tfor i in range(batchsize):\n",
        "\t\timg, target = gen.__next__()\n",
        "\t\tbatch_imgs.append(img)\n",
        "\t\tbatch_fixations.append(target)\n",
        "\treturn np.array(batch_imgs), np.array(batch_fixations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFi73W_n0rEm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "'''\n",
        "Model class\n",
        "'''\n",
        "\n",
        "class saliencyModel:\n",
        "  def __init__(self, model_weights=None, learning_rate=1e-4, batch_size=32, num_batches=100, prior_downsampling_factor=10):\n",
        "    self.dir_path = 'drive/My Drive/Colab Notebooks/CV2 exercies/'\n",
        "    self.model_path = self.dir_path+'model_cptk/my-model1'\n",
        "    self.lr_rate = learning_rate\n",
        "    self.batch_size = batch_size\n",
        "    self.num_batches = num_batches\n",
        "    self.prior_downsampling_factor = prior_downsampling_factor\n",
        "    self.input_h = 180\n",
        "    self.input_w = 320\n",
        "    self.prior_h = self.input_h /(2 * prior_downsampling_factor)\n",
        "    self.prior_w = self.input_w / (2 * prior_downsampling_factor)\n",
        "    self.reg_lambda = 1 / (self.prior_h * self.prior_w)\n",
        "    \n",
        "    if model_weights is not None:\n",
        "      self.load_weights(model_weights)\n",
        "    \n",
        "  def load_weights(self, model_weights):\n",
        "    vgg_weight_file = model_weights\n",
        "    self.weights = np.load(vgg_weight_file)\n",
        "  \n",
        "  def setup(self, mode= 'Train'):\n",
        "    self.input_images_placeholder = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 3])\n",
        "    self.target_images_placehodler = tf.placeholder(tf.uint8, [None, self.input_h, self.input_w, 1])\n",
        "    \n",
        "    with tf.name_scope('preprocessing') as scope:\n",
        "      input_imgs = tf.image.convert_image_dtype(self.input_images_placeholder, tf.float32)\n",
        "      fixations_normalized = tf.image.convert_image_dtype(self.target_images_placehodler, tf.float32)\n",
        "\n",
        "    with tf.name_scope('conv1_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(input_imgs, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv1_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv1_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv1_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool1') as scope:\n",
        "      pool = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv2_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv2_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv2_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv2_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool2') as scope:\n",
        "      pool2 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(2,2), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv3_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('conv3_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv3_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv3_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "\n",
        "    with tf.name_scope('pool3') as scope:\n",
        "      pool3 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "\n",
        "    with tf.name_scope('conv4_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv4_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv4_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv4_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv4_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('pool4') as scope:\n",
        "      pool4 = tf.layers.max_pooling2d(act, pool_size=(2,2), strides=(1,1), padding='same')\n",
        "    \n",
        "    with tf.name_scope('conv5_1') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_1_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_1_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      \n",
        "    with tf.name_scope('conv5_2') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_2_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_2_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "    \n",
        "    with tf.name_scope('conv5_3') as scope:\n",
        "      kernel = tf.Variable(initial_value=self.weights['conv5_3_W'], trainable=False, name=\"weights\")\n",
        "      biases = tf.Variable(initial_value=self.weights['conv5_3_b'], trainable=False, name=\"biases\")\n",
        "      conv = tf.nn.conv2d(act, kernel, [1, 1, 1, 1], padding='SAME')\n",
        "      out = tf.nn.bias_add(conv, biases)\n",
        "      act = tf.nn.relu(out, name=scope)\n",
        "      conv5_3 = act\n",
        "      \n",
        "    with tf.name_scope('concat_featuremaps') as scope:\n",
        "      concatenated_feature_maps = tf.concat([pool2, pool3, pool4, conv5_3], axis=3)\n",
        "      print('IN_TRAINING_MODE is',mode =='Train')\n",
        "      regularized_feature_maps = tf.layers.dropout(concatenated_feature_maps, rate=0.5, training= mode =='Train')\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv1') as scope:\n",
        "      act_featuremaps_conv1 = tf.layers.conv2d(regularized_feature_maps, filters=64, kernel_size=(3,3), activation=tf.nn.relu, name='featuremaps_conv1', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('featuremaps_conv2') as scope:\n",
        "      act_featuremaps_conv2 = tf.layers.conv2d(act_featuremaps_conv1, filters=1, kernel_size=(1,1), activation=tf.nn.relu, name='featuremaps_conv2', reuse=tf.AUTO_REUSE)\n",
        "\n",
        "    with tf.name_scope('Learned_prior') as scope:\n",
        "      #weight_initer = tf.random_normal_initializer(mean=0.0, stddev=0.1)\n",
        "      #prior_shape = tf.ceil(tf.divide(act_featuremaps_conv2.shape[1:3], self.prior_downsampling_factor))\n",
        "      prior_shape = (1, self.prior_h, self.prior_w,1)\n",
        "      #init = tf.ones(prior_shape, dtype=tf.dtypes.float32, name='prior')\n",
        "      #prior = tf.get_variable(name=\"prior\", dtype=tf.float32, shape=prior_shape, init=weight_initer)\n",
        "      prior = tf.Variable(tf.ones(prior_shape), name=\"prior\", trainable=True)\n",
        "      upsampled_prior = tf.image.resize_bilinear(prior, size=act_featuremaps_conv2.shape[1:3])\n",
        "      saliency_mixed = tf.multiply(act_featuremaps_conv2, upsampled_prior)\n",
        "      saliency_raw = tf.nn.relu(saliency_mixed)\n",
        "      \n",
        "    with tf.name_scope('loss') as scope:\n",
        "      # normalize saliency\n",
        "      max_value_per_image = tf.reduce_max(saliency_raw, axis=[1,2,3], keepdims=True)\n",
        "      predicted_saliency = (saliency_raw / max_value_per_image)\n",
        "      target_shape = predicted_saliency.shape[1:3]\n",
        "      target_downscaled = tf.image.resize_images(fixations_normalized, target_shape)\n",
        "      \n",
        "      # Loss function from Cornia et al. (2016) [with higher weight for salient pixels]\n",
        "      alpha = 1.01\n",
        "      weight = 1.0 / (alpha - target_downscaled)\n",
        "      loss = tf.losses.mean_squared_error(labels=target_downscaled, \n",
        "                        predictions=predicted_saliency, \n",
        "                        weights=weight)\n",
        "      regularizer = tf.nn.l2_loss(1 - prior)\n",
        "      loss += tf.reduce_mean(self.reg_lambda * regularizer)\n",
        "      \n",
        "    return predicted_saliency, loss\n",
        "    \n",
        "  def train(self, train_imgs, train_fixations):\n",
        "    \n",
        "    pred ,loss_op = self.setup(mode='Train')\n",
        "    # Optimizer settings from Cornia et al. (2016) [except for decay]\n",
        "    #optimizer = tf.train.MomentumOptimizer(learning_rate=self.lr_rate, momentum=0.9, use_nesterov=True)\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=self.lr_rate)\n",
        "    minimize_op = optimizer.minimize(loss_op)\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      \n",
        "      sess.run(tf.global_variables_initializer())\n",
        "      #saver.restore(sess, self.model_path)\n",
        "      gen = data_generator(train_imgs, train_fixations)\n",
        "      for b in range(self.num_batches):\n",
        "        batch_imgs, batch_fixations = get_batch_from_generator(gen, self.batch_size)\n",
        "        predication, batch_loss,_ = sess.run([pred, loss_op, minimize_op], feed_dict={self.input_images_placeholder: batch_imgs, self.target_images_placehodler: batch_fixations})\n",
        "        #print(predication.shape)\n",
        "        if b % 10 == 0:\n",
        "          #writer.add_summary(l_summary, global_step=b)\n",
        "          print('Batch {0} done: batch loss {1}'.format(b, batch_loss))\n",
        "          save_path = saver.save(sess, self.model_path, global_step=b)\n",
        "      save_path = saver.save(sess, self.model_path)\n",
        "      \n",
        "  def test(self, test_imgs):\n",
        "    pred_saliency, l = self.setup(mode='Test')\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "      #writer = tf.summary.FileWriter(logdir=\"./\", graph=sess.graph)\n",
        "      #saver.restore(sess, self.model_path)\n",
        "      saliency = sess.run(pred_saliency, feed_dict={self.input_images_placeholder: test_imgs})\n",
        "      for i in range(len(test_imgs)):\n",
        "        print('saving images')\n",
        "        upsampled_saliency = resize(saliency[i], output_shape=(180, 320, 1))\n",
        "        saliency_img = sess.run(tf.image.convert_image_dtype(upsampled_saliency, tf.uint8))\n",
        "        imageio.imwrite(self.dir_path+'/saliency/'+str(i)+'.jpg', upsampled_saliency)\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhmQ0g-1-GKV",
        "colab_type": "code",
        "outputId": "a33cb435-3e76-4cd8-d159-001d0fa33a15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_imgs, train_fixations = load_train_data()\n",
        "\n",
        "my_model = saliencyModel(model_weights='/tmp/Model/VGG16/vgg16-conv-weights.npz', learning_rate=1e-1, num_batches=101, batch_size=64)\n",
        "\n",
        "my_model.train(train_imgs, train_fixations)\n",
        "#my_model.train(train_imgs, train_fixations)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is True\n",
            "Batch 0 done: batch loss 0.09543723613023758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1M8GSkh-GVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "outputId": "0e8496b3-efba-40c5-8027-659adee33ad1"
      },
      "source": [
        "my_model.test(train_imgs[0:20])\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IN_TRAINING_MODE is False\n",
            "INFO:tensorflow:Restoring parameters from drive/My Drive/Colab Notebooks/CV2 exercies/model_cptk/my-model1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from drive/My Drive/Colab Notebooks/CV2 exercies/model_cptk/my-model1\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
            "WARNING:root:Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "saving images\n",
            "saving images\n",
            "saving images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryasiSMz-GdA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixmakTnm-GjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rgTCmum-Gk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-O1B9Ick-GnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}